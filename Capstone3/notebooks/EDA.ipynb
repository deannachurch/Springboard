{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Read in and explore dataset:\n",
    "https://www.kaggle.com/datasets/sumitm004/arxiv-scientific-research-papers-dataset\n",
    "\n",
    "To review:\n",
    "\n",
    "## Data Cleaning\n",
    "- [X] Data structure (columns, data types, missing values)\n",
    "- [ ] Identify duplicates\n",
    "- [ ] Data distribution (histograms, box plots)\n",
    "- [ ] Identify temporal coverage (date ranges)\n",
    "\n",
    "## Data Exploration\n",
    "- [ ] Look for class imbalance in target variable\n",
    "- [ ] Generate derived features as needed and characterize\n",
    "- [ ] Identify statistical relationships between features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_col_info_df(df):\n",
    "    \"\"\"\n",
    "        Take a dataframe and create a new dataframe containing column information. \n",
    "    \"\"\"\n",
    "    missing_values=df.isnull().sum()\n",
    "    pct_missing = (missing_values/len(df)) * 100\n",
    "    datatypes = df.dtypes\n",
    "    unique_values = df.nunique()\n",
    "\n",
    "    col_info_df = pd.DataFrame({\n",
    "        \"Column name\": df.columns,\n",
    "        \"Number of Missing Values\": missing_values,\n",
    "        \"Percent Missing Values\": pct_missing,\n",
    "        \"Datatype\": datatypes,\n",
    "        \"Number of Unique Values\": unique_values\n",
    "    })\n",
    "    # reset the index\n",
    "    col_info_df = col_info_df.reset_index(drop=True)\n",
    "    return (col_info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category_code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "published_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "updated_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "first_author",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary_word_count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "4e2db700-f59c-44ea-8f84-ae027bb1bc89",
       "rows": [
        [
         "0",
         "cs-9308101v1",
         "Dynamic Backtracking",
         "Artificial Intelligence",
         "cs.AI",
         "8/1/93",
         "8/1/93",
         "['M. L. Ginsberg']",
         "'M. L. Ginsberg'",
         "Because of their occasional need to return to shallow points in a search\ntree, existing backtracking methods can sometimes erase meaningful progress\ntoward solving a search problem. In this paper, we present a method by which\nbacktrack points can be moved deeper in the search space, thereby avoiding this\ndifficulty. The technique developed is a variant of dependency-directed\nbacktracking that uses only polynomial space while still providing useful\ncontrol information and retaining the completeness guarantees provided by\nearlier approaches.",
         "79"
        ],
        [
         "1",
         "cs-9308102v1",
         "A Market-Oriented Programming Environment and its Application to\n  Distributed Multicommodity Flow Problems",
         "Artificial Intelligence",
         "cs.AI",
         "8/1/93",
         "8/1/93",
         "['M. P. Wellman']",
         "'M. P. Wellman'",
         "Market price systems constitute a well-understood class of mechanisms that\nunder certain conditions provide effective decentralization of decision making\nwith minimal communication overhead. In a market-oriented programming approach\nto distributed problem solving, we derive the activities and resource\nallocations for a set of computational agents by computing the competitive\nequilibrium of an artificial economy. WALRAS provides basic constructs for\ndefining computational market structures, and protocols for deriving their\ncorresponding price equilibria. In a particular realization of this approach\nfor a form of multicommodity flow problem, we see that careful construction of\nthe decision process according to economic principles can lead to efficient\ndistributed resource allocation, and that the behavior of the system can be\nmeaningfully analyzed in economic terms.",
         "119"
        ],
        [
         "2",
         "cs-9309101v1",
         "An Empirical Analysis of Search in GSAT",
         "Artificial Intelligence",
         "cs.AI",
         "9/1/93",
         "9/1/93",
         "['I. P. Gent', 'T. Walsh']",
         "'I. P. Gent'",
         "We describe an extensive study of search in GSAT, an approximation procedure\nfor propositional satisfiability. GSAT performs greedy hill-climbing on the\nnumber of satisfied clauses in a truth assignment. Our experiments provide a\nmore complete picture of GSAT's search than previous accounts. We describe in\ndetail the two phases of search: rapid hill-climbing followed by a long plateau\nsearch. We demonstrate that when applied to randomly generated 3SAT problems,\nthere is a very simple scaling with problem size for both the mean number of\nsatisfied clauses and the mean branching rate. Our results allow us to make\ndetailed numerical conjectures about the length of the hill-climbing phase, the\naverage gradient of this phase, and to conjecture that both the average score\nand average branching rate decay exponentially during plateau search. We end by\nshowing how these results can be used to direct future theoretical analysis.\nThis work provides a case study of how computer experiments can be used to\nimprove understanding of the theoretical properties of algorithms.",
         "167"
        ],
        [
         "3",
         "cs-9311101v1",
         "The Difficulties of Learning Logic Programs with Cut",
         "Artificial Intelligence",
         "cs.AI",
         "11/1/93",
         "11/1/93",
         "['F. Bergadano', 'D. Gunetti', 'U. Trinchero']",
         "'F. Bergadano'",
         "As real logic programmers normally use cut (!), an effective learning\nprocedure for logic programs should be able to deal with it. Because the cut\npredicate has only a procedural meaning, clauses containing cut cannot be\nlearned using an extensional evaluation method, as is done in most learning\nsystems. On the other hand, searching a space of possible programs (instead of\na space of independent clauses) is unfeasible. An alternative solution is to\ngenerate first a candidate base program which covers the positive examples, and\nthen make it consistent by inserting cut where appropriate. The problem of\nlearning programs with cut has not been investigated before and this seems to\nbe a natural and reasonable approach. We generalize this scheme and investigate\nthe difficulties that arise. Some of the major shortcomings are actually\ncaused, in general, by the need for intensional evaluation. As a conclusion,\nthe analysis of this paper suggests, on precise and technical grounds, that\nlearning cut is difficult, and current induction techniques should probably be\nrestricted to purely declarative logic languages.",
         "174"
        ],
        [
         "4",
         "cs-9311102v1",
         "Software Agents: Completing Patterns and Constructing User Interfaces",
         "Artificial Intelligence",
         "cs.AI",
         "11/1/93",
         "11/1/93",
         "['J. C. Schlimmer', 'L. A. Hermens']",
         "'J. C. Schlimmer'",
         "To support the goal of allowing users to record and retrieve information,\nthis paper describes an interactive note-taking system for pen-based computers\nwith two distinctive features. First, it actively predicts what the user is\ngoing to write. Second, it automatically constructs a custom, button-box user\ninterface on request. The system is an example of a learning-apprentice\nsoftware- agent. A machine learning component characterizes the syntax and\nsemantics of the user's information. A performance system uses this learned\ninformation to generate completion strings and construct a user interface.\nDescription of Online Appendix: People like to record information. Doing this\non paper is initially efficient, but lacks flexibility. Recording information\non a computer is less efficient but more powerful. In our new note taking\nsoftwre, the user records information directly on a computer. Behind the\ninterface, an agent acts for the user. To help, it provides defaults and\nconstructs a custom user interface. The demonstration is a QuickTime movie of\nthe note taking agent in action. The file is a binhexed self-extracting\narchive. Macintosh utilities for binhex are available from\nmac.archive.umich.edu. QuickTime is available from ftp.apple.com in the\ndts/mac/sys.soft/quicktime.",
         "187"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>category_code</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>first_author</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cs-9308101v1</td>\n",
       "      <td>Dynamic Backtracking</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>8/1/93</td>\n",
       "      <td>8/1/93</td>\n",
       "      <td>['M. L. Ginsberg']</td>\n",
       "      <td>'M. L. Ginsberg'</td>\n",
       "      <td>Because of their occasional need to return to ...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cs-9308102v1</td>\n",
       "      <td>A Market-Oriented Programming Environment and ...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>8/1/93</td>\n",
       "      <td>8/1/93</td>\n",
       "      <td>['M. P. Wellman']</td>\n",
       "      <td>'M. P. Wellman'</td>\n",
       "      <td>Market price systems constitute a well-underst...</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cs-9309101v1</td>\n",
       "      <td>An Empirical Analysis of Search in GSAT</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>9/1/93</td>\n",
       "      <td>9/1/93</td>\n",
       "      <td>['I. P. Gent', 'T. Walsh']</td>\n",
       "      <td>'I. P. Gent'</td>\n",
       "      <td>We describe an extensive study of search in GS...</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cs-9311101v1</td>\n",
       "      <td>The Difficulties of Learning Logic Programs wi...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>11/1/93</td>\n",
       "      <td>11/1/93</td>\n",
       "      <td>['F. Bergadano', 'D. Gunetti', 'U. Trinchero']</td>\n",
       "      <td>'F. Bergadano'</td>\n",
       "      <td>As real logic programmers normally use cut (!)...</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cs-9311102v1</td>\n",
       "      <td>Software Agents: Completing Patterns and Const...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>11/1/93</td>\n",
       "      <td>11/1/93</td>\n",
       "      <td>['J. C. Schlimmer', 'L. A. Hermens']</td>\n",
       "      <td>'J. C. Schlimmer'</td>\n",
       "      <td>To support the goal of allowing users to recor...</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title  \\\n",
       "0  cs-9308101v1                               Dynamic Backtracking   \n",
       "1  cs-9308102v1  A Market-Oriented Programming Environment and ...   \n",
       "2  cs-9309101v1            An Empirical Analysis of Search in GSAT   \n",
       "3  cs-9311101v1  The Difficulties of Learning Logic Programs wi...   \n",
       "4  cs-9311102v1  Software Agents: Completing Patterns and Const...   \n",
       "\n",
       "                  category category_code published_date updated_date  \\\n",
       "0  Artificial Intelligence         cs.AI         8/1/93       8/1/93   \n",
       "1  Artificial Intelligence         cs.AI         8/1/93       8/1/93   \n",
       "2  Artificial Intelligence         cs.AI         9/1/93       9/1/93   \n",
       "3  Artificial Intelligence         cs.AI        11/1/93      11/1/93   \n",
       "4  Artificial Intelligence         cs.AI        11/1/93      11/1/93   \n",
       "\n",
       "                                          authors       first_author  \\\n",
       "0                              ['M. L. Ginsberg']   'M. L. Ginsberg'   \n",
       "1                               ['M. P. Wellman']    'M. P. Wellman'   \n",
       "2                      ['I. P. Gent', 'T. Walsh']       'I. P. Gent'   \n",
       "3  ['F. Bergadano', 'D. Gunetti', 'U. Trinchero']     'F. Bergadano'   \n",
       "4            ['J. C. Schlimmer', 'L. A. Hermens']  'J. C. Schlimmer'   \n",
       "\n",
       "                                             summary  summary_word_count  \n",
       "0  Because of their occasional need to return to ...                  79  \n",
       "1  Market price systems constitute a well-underst...                 119  \n",
       "2  We describe an extensive study of search in GS...                 167  \n",
       "3  As real logic programmers normally use cut (!)...                 174  \n",
       "4  To support the goal of allowing users to recor...                 187  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw/arXiv_scientific_dataset.csv')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Column name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Number of Missing Values",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Percent Missing Values",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Datatype",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Number of Unique Values",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f0d92d08-d37d-4368-a0a6-a758a891e608",
       "rows": [
        [
         "0",
         "id",
         "0",
         "0.0",
         "object",
         "136238"
        ],
        [
         "1",
         "title",
         "0",
         "0.0",
         "object",
         "136154"
        ],
        [
         "2",
         "category",
         "0",
         "0.0",
         "object",
         "138"
        ],
        [
         "3",
         "category_code",
         "0",
         "0.0",
         "object",
         "139"
        ],
        [
         "4",
         "published_date",
         "0",
         "0.0",
         "object",
         "7259"
        ],
        [
         "5",
         "updated_date",
         "0",
         "0.0",
         "object",
         "7196"
        ],
        [
         "6",
         "authors",
         "0",
         "0.0",
         "object",
         "125548"
        ],
        [
         "7",
         "first_author",
         "0",
         "0.0",
         "object",
         "77742"
        ],
        [
         "8",
         "summary",
         "0",
         "0.0",
         "object",
         "136193"
        ],
        [
         "9",
         "summary_word_count",
         "0",
         "0.0",
         "int64",
         "346"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column name</th>\n",
       "      <th>Number of Missing Values</th>\n",
       "      <th>Percent Missing Values</th>\n",
       "      <th>Datatype</th>\n",
       "      <th>Number of Unique Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "      <td>136238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "      <td>136154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>category</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>category_code</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>published_date</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "      <td>7259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>updated_date</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "      <td>7196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>authors</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "      <td>125548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>first_author</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "      <td>77742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>summary</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "      <td>136193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>summary_word_count</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>int64</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Column name  Number of Missing Values  Percent Missing Values  \\\n",
       "0                  id                         0                     0.0   \n",
       "1               title                         0                     0.0   \n",
       "2            category                         0                     0.0   \n",
       "3       category_code                         0                     0.0   \n",
       "4      published_date                         0                     0.0   \n",
       "5        updated_date                         0                     0.0   \n",
       "6             authors                         0                     0.0   \n",
       "7        first_author                         0                     0.0   \n",
       "8             summary                         0                     0.0   \n",
       "9  summary_word_count                         0                     0.0   \n",
       "\n",
       "  Datatype  Number of Unique Values  \n",
       "0   object                   136238  \n",
       "1   object                   136154  \n",
       "2   object                      138  \n",
       "3   object                      139  \n",
       "4   object                     7259  \n",
       "5   object                     7196  \n",
       "6   object                   125548  \n",
       "7   object                    77742  \n",
       "8   object                   136193  \n",
       "9    int64                      346  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_inf_df=create_col_info_df(df)\n",
    "display(col_inf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column information\n",
    "* There are 10 columns.\n",
    "* There is no missing data!\n",
    "* Interestingly, there are 138 categories, and 139 catagories codes- we should explore this discrepancy.\n",
    "* I would have expected the 'id', 'title' and 'summary' columns to have the same number of unique values, but they do not. \n",
    "* Date columns need to be converted to datetime format.\n",
    "* May be useful to create an 'author_count' column and 'title_word_count' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n",
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# update date columns to date types\n",
    "df['published_date'] = pd.to_datetime(df['published_date'], format='%Y-%m-%d')\n",
    "df['updated_date'] = pd.to_datetime(df['updated_date'], format='%Y-%m-%d')\n",
    "print(df['published_date'].dtype)\n",
    "print(df['updated_date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "Number of rows that appear more than once: 0\n",
      "Empty DataFrame\n",
      "Columns: [id, title, category, category_code, published_date, updated_date, authors, first_author, summary, summary_word_count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Find all duplicate rows\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "\n",
    "# Count how many duplicate rows exist\n",
    "num_duplicates = len(duplicate_rows)\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "\n",
    "# You can also count duplicates including the first occurrence\n",
    "all_duplicates = df[df.duplicated(keep=False)]\n",
    "num_all_duplicates = len(all_duplicates)\n",
    "print(f\"Number of rows that appear more than once: {num_all_duplicates}\")\n",
    "\n",
    "# View the duplicates\n",
    "print(duplicate_rows.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no full duplicate rows, but there are some duplicate values in the 'title' and 'summary' columns that are worth exploring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories with multiple category codes: 1\n",
      "              category  category_code\n",
      "98  Numerical Analysis              2\n",
      "\n",
      "Category: Numerical Analysis\n",
      "                 category category_code\n",
      "39965  Numerical Analysis         cs.NA\n",
      "40889  Numerical Analysis       math.NA\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame that groups by category and counts distinct category_codes\n",
    "category_mapping = df.groupby('category')['category_code'].nunique().reset_index()\n",
    "\n",
    "# Find categories that map to more than one category_code\n",
    "multiple_mappings = category_mapping[category_mapping['category_code'] > 1]\n",
    "\n",
    "print(f\"Categories with multiple category codes: {len(multiple_mappings)}\")\n",
    "print(multiple_mappings)\n",
    "\n",
    "# For the categories with multiple mappings, show the actual combinations\n",
    "if len(multiple_mappings) > 0:\n",
    "    for category in multiple_mappings['category']:\n",
    "        print(f\"\\nCategory: {category}\")\n",
    "        print(df[df['category'] == category][['category', 'category_code']].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where category_code == 'cs.NA': 25\n",
      "Number of rows where category_code == 'math.NA': 56\n",
      "\n",
      "Categories for 'cs.NA':\n",
      "category\n",
      "Numerical Analysis    25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categories for 'math.NA':\n",
      "category\n",
      "Numerical Analysis    56\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count rows where category_code is 'cs.NA'\n",
    "cs_na_count = df[df['category_code'] == 'cs.NA'].shape[0]\n",
    "print(f\"Number of rows where category_code == 'cs.NA': {cs_na_count}\")\n",
    "\n",
    "# Count rows where category_code is 'math.NA'\n",
    "math_na_count = df[df['category_code'] == 'math.NA'].shape[0]\n",
    "print(f\"Number of rows where category_code == 'math.NA': {math_na_count}\")\n",
    "\n",
    "# You can also check what categories these map to\n",
    "if cs_na_count > 0:\n",
    "    print(\"\\nCategories for 'cs.NA':\")\n",
    "    print(df[df['category_code'] == 'cs.NA']['category'].value_counts())\n",
    "\n",
    "if math_na_count > 0:\n",
    "    print(\"\\nCategories for 'math.NA':\")\n",
    "    print(df[df['category_code'] == 'math.NA']['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category_code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "published_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "updated_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "first_author",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary_word_count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "7086c11d-6007-4779-bba0-114fc374c892",
       "rows": [
        [
         "39965",
         "abs-1312.6872v1",
         "Matrix recovery using Split Bregman",
         "Numerical Analysis",
         "cs.NA",
         "2013-12-17 00:00:00",
         "2013-12-17 00:00:00",
         "['Anupriya Gogna', 'Ankita Shukla', 'Angshul Majumdar']",
         "'Anupriya Gogna'",
         "In this paper we address the problem of recovering a matrix, with inherent\nlow rank structure, from its lower dimensional projections. This problem is\nfrequently encountered in wide range of areas including pattern recognition,\nwireless sensor networks, control systems, recommender systems, image/video\nreconstruction etc. Both in theory and practice, the most optimal way to solve\nthe low rank matrix recovery problem is via nuclear norm minimization. In this\npaper, we propose a Split Bregman algorithm for nuclear norm minimization. The\nuse of Bregman technique improves the convergence speed of our algorithm and\ngives a higher success rate. Also, the accuracy of reconstruction is much\nbetter even for cases where small number of linear measurements are available.\nOur claim is supported by empirical results obtained using our algorithm and\nits comparison to other existing methods for matrix recovery. The algorithms\nare compared on the basis of NMSE, execution time and success rate for varying\nranks and sampling ratios.",
         "157"
        ],
        [
         "39971",
         "abs-1401.0159v1",
         "Speeding-Up Convergence via Sequential Subspace Optimization: Current\n  State and Future Directions",
         "Numerical Analysis",
         "cs.NA",
         "2013-12-31 00:00:00",
         "2013-12-31 00:00:00",
         "['Michael Zibulevsky']",
         "'Michael Zibulevsky'",
         "This is an overview paper written in style of research proposal. In recent\nyears we introduced a general framework for large-scale unconstrained\noptimization -- Sequential Subspace Optimization (SESOP) and demonstrated its\nusefulness for sparsity-based signal/image denoising, deconvolution,\ncompressive sensing, computed tomography, diffraction imaging, support vector\nmachines. We explored its combination with Parallel Coordinate Descent and\nSeparable Surrogate Function methods, obtaining state of the art results in\nabove-mentioned areas. There are several methods, that are faster than plain\nSESOP under specific conditions: Trust region Newton method - for problems with\neasily invertible Hessian matrix; Truncated Newton method - when fast\nmultiplication by Hessian is available; Stochastic optimization methods - for\nproblems with large stochastic-type data; Multigrid methods - for problems with\nnested multilevel structure. Each of these methods can be further improved by\nmerge with SESOP. One can also accelerate Augmented Lagrangian method for\nconstrained optimization problems and Alternating Direction Method of\nMultipliers for problems with separable objective function and non-separable\nconstraints.",
         "161"
        ],
        [
         "39985",
         "abs-1407.1399v1",
         "Generalized Higher-Order Tensor Decomposition via Parallel ADMM",
         "Numerical Analysis",
         "cs.NA",
         "2014-07-05 00:00:00",
         "2014-07-05 00:00:00",
         "['Fanhua Shang', 'Yuanyuan Liu', 'James Cheng']",
         "'Fanhua Shang'",
         "Higher-order tensors are becoming prevalent in many scientific areas such as\ncomputer vision, social network analysis, data mining and neuroscience.\nTraditional tensor decomposition approaches face three major challenges: model\nselecting, gross corruptions and computational efficiency. To address these\nproblems, we first propose a parallel trace norm regularized tensor\ndecomposition method, and formulate it as a convex optimization problem. This\nmethod does not require the rank of each mode to be specified beforehand, and\ncan automatically determine the number of factors in each mode through our\noptimization scheme. By considering the low-rank structure of the observed\ntensor, we analyze the equivalent relationship of the trace norm between a\nlow-rank tensor and its core tensor. Then, we cast a non-convex tensor\ndecomposition model into a weighted combination of multiple much smaller-scale\nmatrix trace norm minimization. Finally, we develop two parallel alternating\ndirection methods of multipliers (ADMM) to solve our problems. Experimental\nresults verify that our regularized formulation is effective, and our methods\nare robust to noise or outliers.",
         "166"
        ],
        [
         "40351",
         "abs-1601.07721v1",
         "Distributed Low Rank Approximation of Implicit Functions of a Matrix",
         "Numerical Analysis",
         "cs.NA",
         "2016-01-28 00:00:00",
         "2016-01-28 00:00:00",
         "['David P. Woodruff', 'Peilin Zhong']",
         "'David P. Woodruff'",
         "We study distributed low rank approximation in which the matrix to be\napproximated is only implicitly represented across the different servers. For\nexample, each of $s$ servers may have an $n \\times d$ matrix $A^t$, and we may\nbe interested in computing a low rank approximation to $A = f(\\sum_{t=1}^s\nA^t)$, where $f$ is a function which is applied entrywise to the matrix\n$\\sum_{t=1}^s A^t$. We show for a wide class of functions $f$ it is possible to\nefficiently compute a $d \\times d$ rank-$k$ projection matrix $P$ for which\n$\\|A - AP\\|_F^2 \\leq \\|A - [A]_k\\|_F^2 + \\varepsilon \\|A\\|_F^2$, where $AP$\ndenotes the projection of $A$ onto the row span of $P$, and $[A]_k$ denotes the\nbest rank-$k$ approximation to $A$ given by the singular value decomposition.\nThe communication cost of our protocols is $d \\cdot (sk/\\varepsilon)^{O(1)}$,\nand they succeed with high probability. Our framework allows us to efficiently\ncompute a low rank approximation to an entry-wise softmax, to a Gaussian kernel\nexpansion, and to $M$-Estimators applied entrywise (i.e., forms of robust low\nrank approximation). We also show that our additive error approximation is best\npossible, in the sense that any protocol achieving relative error for these\nproblems requires significantly more communication. Finally, we experimentally\nvalidate our algorithms on real datasets.",
         "212"
        ],
        [
         "40889",
         "abs-1707.09428v1",
         "A unified method for super-resolution recovery and real exponential-sum\n  separation",
         "Numerical Analysis",
         "math.NA",
         "2017-07-26 00:00:00",
         "2017-07-26 00:00:00",
         "['Charles K. Chui', 'Hrushikesh N. Mhaskar']",
         "'Charles K. Chui'",
         "In this paper, motivated by diffraction of traveling light waves, a simple\nmathematical model is proposed, both for the multivariate super-resolution\nproblem and the problem of blind-source separation of real-valued exponential\nsums. This model facilitates the development of a unified theory and a unified\nsolution of both problems in this paper. Our consideration of the\nsuper-resolution problem is aimed at applications to fluorescence microscopy\nand observational astronomy, and the motivation for our consideration of the\nsecond problem is the current need of extracting multivariate exponential\nfeatures in magnetic resonance spectroscopy (MRS) for the neurologist and\nradiologist as well as for providing a mathematical tool for isotope separation\nin Nuclear Chemistry. The unified method introduced in this paper can be easily\nrealized by processing only finitely many data, sampled at locations that are\nnot necessarily prescribed in advance, with computational scheme consisting\nonly of matrix - vector multiplication, peak finding, and clustering.",
         "151"
        ],
        [
         "43186",
         "abs-1808.07452v2",
         "Generalized Canonical Polyadic Tensor Decomposition",
         "Numerical Analysis",
         "math.NA",
         "2018-08-22 00:00:00",
         "2019-01-22 00:00:00",
         "['David Hong', 'Tamara G. Kolda', 'Jed A. Duersch']",
         "'David Hong'",
         "Tensor decomposition is a fundamental unsupervised machine learning method in\ndata science, with applications including network analysis and sensor data\nprocessing. This work develops a generalized canonical polyadic (GCP) low-rank\ntensor decomposition that allows other loss functions besides squared error.\nFor instance, we can use logistic loss or Kullback-Leibler divergence, enabling\ntensor decomposition for binary or count data. We present a variety\nstatistically-motivated loss functions for various scenarios. We provide a\ngeneralized framework for computing gradients and handling missing data that\nenables the use of standard optimization methods for fitting the model. We\ndemonstrate the flexibility of GCP on several real-world examples including\ninteractions in a social network, neural activity in a mouse, and monthly\nrainfall measurements in India.",
         "119"
        ],
        [
         "44516",
         "abs-1511.04695v1",
         "An Iterative Reweighted Method for Tucker Decomposition of Incomplete\n  Multiway Tensors",
         "Numerical Analysis",
         "cs.NA",
         "2015-11-15 00:00:00",
         "2015-11-15 00:00:00",
         "['Linxiao Yang', 'Jun Fang', 'Hongbin Li', 'Bing Zeng']",
         "'Linxiao Yang'",
         "We consider the problem of low-rank decomposition of incomplete multiway\ntensors. Since many real-world data lie on an intrinsically low dimensional\nsubspace, tensor low-rank decomposition with missing entries has applications\nin many data analysis problems such as recommender systems and image\ninpainting. In this paper, we focus on Tucker decomposition which represents an\nNth-order tensor in terms of N factor matrices and a core tensor via\nmultilinear operations. To exploit the underlying multilinear low-rank\nstructure in high-dimensional datasets, we propose a group-based log-sum\npenalty functional to place structural sparsity over the core tensor, which\nleads to a compact representation with smallest core tensor. The method for\nTucker decomposition is developed by iteratively minimizing a surrogate\nfunction that majorizes the original objective function, which results in an\niterative reweighted process. In addition, to reduce the computational\ncomplexity, an over-relaxed monotone fast iterative shrinkage-thresholding\ntechnique is adapted and embedded in the iterative reweighted process. The\nproposed method is able to determine the model complexity (i.e. multilinear\nrank) in an automatic way. Simulation results show that the proposed algorithm\noffers competitive performance compared with other existing algorithms.",
         "184"
        ],
        [
         "48409",
         "abs-1604.01376v1",
         "Lipschitz Continuity of Mahalanobis Distances and Bilinear Forms",
         "Numerical Analysis",
         "cs.NA",
         "2016-04-04 00:00:00",
         "2016-04-04 00:00:00",
         "['Valentina Zantedeschi', 'RÃ©mi Emonet', 'Marc Sebban']",
         "'Valentina Zantedeschi'",
         "Many theoretical results in the machine learning domain stand only for\nfunctions that are Lipschitz continuous. Lipschitz continuity is a strong form\nof continuity that linearly bounds the variations of a function. In this paper,\nwe derive tight Lipschitz constants for two families of metrics: Mahalanobis\ndistances and bounded-space bilinear forms. To our knowledge, this is the first\ntime the Mahalanobis distance is formally proved to be Lipschitz continuous and\nthat such tight Lipschitz constants are derived.",
         "77"
        ],
        [
         "48499",
         "abs-1708.02276v2",
         "Parallelizing Over Artificial Neural Network Training Runs with\n  Multigrid",
         "Numerical Analysis",
         "cs.NA",
         "2017-08-07 00:00:00",
         "2017-10-01 00:00:00",
         "['Jacob B. Schroder']",
         "'Jacob B. Schroder'",
         "Artificial neural networks are a popular and effective machine learning\ntechnique. Great progress has been made parallelizing the expensive training\nphase of an individual network, leading to highly specialized pieces of\nhardware, many based on GPU-type architectures, and more concurrent algorithms\nsuch as synthetic gradients. However, the training phase continues to be a\nbottleneck, where the training data must be processed serially over thousands\nof individual training runs. This work considers a multigrid reduction in time\n(MGRIT) algorithm that is able to parallelize over the thousands of training\nruns and converge to the exact same solution as traditional training would\nprovide. MGRIT was originally developed to provide parallelism for time\nevolution problems that serially step through a finite number of time-steps.\nThis work recasts the training of a neural network similarly, treating neural\nnetwork training as an evolution equation that evolves the network weights from\none step to the next. Thus, this work concerns distributed computing approaches\nfor neural networks, but is distinct from other approaches which seek to\nparallelize only over individual training runs. The work concludes with\nsupporting numerical results for two model problems.",
         "186"
        ],
        [
         "48592",
         "abs-1708.09165v1",
         "Tensor Networks for Dimensionality Reduction and Large-Scale\n  Optimizations. Part 2 Applications and Future Perspectives",
         "Numerical Analysis",
         "cs.NA",
         "2017-08-30 00:00:00",
         "2017-08-30 00:00:00",
         "['A. Cichocki', 'A-H. Phan', 'Q. Zhao', 'N. Lee', 'I. V. Oseledets', 'M. Sugiyama', 'D. Mandic']",
         "'A. Cichocki'",
         "Part 2 of this monograph builds on the introduction to tensor networks and\ntheir operations presented in Part 1. It focuses on tensor network models for\nsuper-compressed higher-order representation of data/parameters and related\ncost functions, while providing an outline of their applications in machine\nlearning and data analytics. A particular emphasis is on the tensor train (TT)\nand Hierarchical Tucker (HT) decompositions, and their physically meaningful\ninterpretations which reflect the scalability of the tensor network approach.\nThrough a graphical approach, we also elucidate how, by virtue of the\nunderlying low-rank tensor approximations and sophisticated contractions of\ncore tensors, tensor networks have the ability to perform distributed\ncomputations on otherwise prohibitively large volumes of data/parameters,\nthereby alleviating or even eliminating the curse of dimensionality. The\nusefulness of this concept is illustrated over a number of applied areas,\nincluding generalized regression and classification (support tensor machines,\ncanonical correlation analysis, higher order partial least squares),\ngeneralized eigenvalue decomposition, Riemannian optimization, and in the\noptimization of deep neural networks. Part 1 and Part 2 of this work can be\nused either as stand-alone separate texts, or indeed as a conjoint\ncomprehensive review of the exciting field of low-rank tensor networks and\ntensor decompositions.",
         "199"
        ],
        [
         "48639",
         "abs-1711.02271v2",
         "High-order Tensor Completion for Data Recovery via Sparse Tensor-train\n  Optimization",
         "Numerical Analysis",
         "cs.NA",
         "2017-11-07 00:00:00",
         "2018-03-22 00:00:00",
         "['Longhao Yuan', 'Qibin Zhao', 'Jianting Cao']",
         "'Longhao Yuan'",
         "In this paper, we aim at the problem of tensor data completion. Tensor-train\ndecomposition is adopted because of its powerful representation ability and\nlinear scalability to tensor order. We propose an algorithm named Sparse\nTensor-train Optimization (STTO) which considers incomplete data as sparse\ntensor and uses first-order optimization method to find the factors of\ntensor-train decomposition. Our algorithm is shown to perform well in\nsimulation experiments at both low-order cases and high-order cases. We also\nemploy a tensorization method to transform data to a higher-order form to\nenhance the performance of our algorithm. The results of image recovery\nexperiments in various cases manifest that our method outperforms other\ncompletion algorithms. Especially when the missing rate is very high, e.g.,\n90\\% to 99\\%, our method is significantly better than the state-of-the-art\nmethods.",
         "131"
        ],
        [
         "48757",
         "abs-1711.07038v2",
         "A Coordinate-wise Optimization Algorithm for Sparse Inverse Covariance\n  Selection",
         "Numerical Analysis",
         "cs.NA",
         "2017-11-19 00:00:00",
         "2018-04-04 00:00:00",
         "['Ganzhao Yuan', 'Haoxian Tan', 'Wei-Shi Zheng']",
         "'Ganzhao Yuan'",
         "Sparse inverse covariance selection is a fundamental problem for analyzing\ndependencies in high dimensional data. However, such a problem is difficult to\nsolve since it is NP-hard. Existing solutions are primarily based on convex\napproximation and iterative hard thresholding, which only lead to sub-optimal\nsolutions. In this work, we propose a coordinate-wise optimization algorithm to\nsolve this problem which is guaranteed to converge to a coordinate-wise minimum\npoint. The algorithm iteratively and greedily selects one variable or swaps two\nvariables to identify the support set, and then solves a reduced convex\noptimization problem over the support set to achieve the greatest descent. As a\nside contribution of this paper, we propose a Newton-like algorithm to solve\nthe reduced convex sub-problem, which is proven to always converge to the\noptimal solution with global linear convergence rate and local quadratic\nconvergence rate. Finally, we demonstrate the efficacy of our method on\nsynthetic data and real-world data sets. As a result, the proposed method\nconsistently outperforms existing solutions in terms of accuracy.",
         "169"
        ],
        [
         "49440",
         "abs-1810.09675v1",
         "SwitchNet: a neural network model for forward and inverse scattering\n  problems",
         "Numerical Analysis",
         "math.NA",
         "2018-10-23 00:00:00",
         "2018-10-23 00:00:00",
         "['Yuehaw Khoo', 'Lexing Ying']",
         "'Yuehaw Khoo'",
         "We propose a novel neural network architecture, SwitchNet, for solving the\nwave equation based inverse scattering problems via providing maps between the\nscatterers and the scattered field (and vice versa). The main difficulty of\nusing a neural network for this problem is that a scatterer has a global impact\non the scattered wave field, rendering typical convolutional neural network\nwith local connections inapplicable. While it is possible to deal with such a\nproblem using a fully connected network, the number of parameters grows\nquadratically with the size of the input and output data. By leveraging the\ninherent low-rank structure of the scattering problems and introducing a novel\nswitching layer with sparse connections, the SwitchNet architecture uses much\nfewer parameters and facilitates the training process. Numerical experiments\nshow promising accuracy in learning the forward and inverse maps between the\nscatterers and the scattered wave field.",
         "144"
        ],
        [
         "76867",
         "abs-1301.0339v1",
         "A Geometric Blind Source Separation Method Based on Facet Component\n  Analysis",
         "Numerical Analysis",
         "math.NA",
         "2013-01-02 00:00:00",
         "2013-01-02 00:00:00",
         "['P. Yin', 'Y. Sun', 'J. Xin']",
         "'P. Yin'",
         "Given a set of mixtures, blind source separation attempts to retrieve the\nsource signals without or with very little information of the the mixing\nprocess. We present a geometric approach for blind separation of nonnegative\nlinear mixtures termed {\\em facet component analysis} (FCA). The approach is\nbased on facet identification of the underlying cone structure of the data.\nEarlier works focus on recovering the cone by locating its vertices (vertex\ncomponent analysis or VCA) based on a mutual sparsity condition which requires\neach source signal to possess a stand-alone peak in its spectrum. We formulate\nalternative conditions so that enough data points fall on the facets of a cone\ninstead of accumulating around the vertices. To find a regime of unique\nsolvability, we make use of both geometric and density properties of the data\npoints, and develop an efficient facet identification method by combining data\nclassification and linear regression. For noisy data, we show that denoising\nmethods may be employed, such as the total variation technique in imaging\nprocessing, and principle component analysis. We show computational results on\nnuclear magnetic resonance spectroscopic data to substantiate our method.",
         "187"
        ],
        [
         "77015",
         "abs-1305.0030v2",
         "A least-squares method for sparse low rank approximation of multivariate\n  functions",
         "Numerical Analysis",
         "math.NA",
         "2013-04-30 00:00:00",
         "2014-07-09 00:00:00",
         "['Mathilde Chevreuil', 'RÃ©gis Lebrun', 'Anthony Nouy', 'Prashant Rai']",
         "'Mathilde Chevreuil'",
         "In this paper, we propose a low-rank approximation method based on discrete\nleast-squares for the approximation of a multivariate function from random,\nnoisy-free observations. Sparsity inducing regularization techniques are used\nwithin classical algorithms for low-rank approximation in order to exploit the\npossible sparsity of low-rank approximations. Sparse low-rank approximations\nare constructed with a robust updated greedy algorithm which includes an\noptimal selection of regularization parameters and approximation ranks using\ncross validation techniques. Numerical examples demonstrate the capability of\napproximating functions of many variables even when very few function\nevaluations are available, thus proving the interest of the proposed algorithm\nfor the propagation of uncertainties through complex computational models.",
         "108"
        ],
        [
         "77195",
         "abs-1403.2073v1",
         "Generalized Canonical Correlation Analysis and Its Application to Blind\n  Source Separation Based on a Dual-Linear Predictor Structure",
         "Numerical Analysis",
         "math.NA",
         "2014-03-09 00:00:00",
         "2014-03-09 00:00:00",
         "['Wei Liu']",
         "'Wei Liu'",
         "Blind source separation (BSS) is one of the most important and established\nresearch topics in signal processing and many algorithms have been proposed\nbased on different statistical properties of the source signals. For\nsecond-order statistics (SOS) based methods, canonical correlation analysis\n(CCA) has been proved to be an effective solution to the problem. In this work,\nthe CCA approach is generalized to accommodate the case with added white noise\nand it is then applied to the BSS problem for noisy mixtures. In this approach,\nthe noise component is assumed to be spatially and temporally white, but the\nvariance information of noise is not required. An adaptive blind source\nextraction algorithm is derived based on this idea and a further extension is\nproposed by employing a dual-linear predictor structure for blind source\nextraction (BSE).",
         "133"
        ],
        [
         "77378",
         "abs-1502.05571v1",
         "Finding Dantzig selectors with a proximity operator based fixed-point\n  algorithm",
         "Numerical Analysis",
         "math.NA",
         "2015-02-19 00:00:00",
         "2015-02-19 00:00:00",
         "['Ashley Prater', 'Lixin Shen', 'Bruce W. Suter']",
         "'Ashley Prater'",
         "In this paper, we study a simple iterative method for finding the Dantzig\nselector, which was designed for linear regression problems. The method\nconsists of two main stages. The first stage is to approximate the Dantzig\nselector through a fixed-point formulation of solutions to the Dantzig selector\nproblem. The second stage is to construct a new estimator by regressing data\nonto the support of the approximated Dantzig selector. We compare our method to\nan alternating direction method, and present the results of numerical\nsimulations using both the proposed method and the alternating direction method\non synthetic and real data sets. The numerical simulations demonstrate that the\ntwo methods produce results of similar quality, however the proposed method\ntends to be significantly faster.",
         "122"
        ],
        [
         "77395",
         "abs-1503.00282v1",
         "Constructive sparse trigonometric approximation for functions with small\n  mixed smoothness",
         "Numerical Analysis",
         "math.NA",
         "2015-03-01 00:00:00",
         "2015-03-01 00:00:00",
         "['V. N. Temlyakov']",
         "'V. N. Temlyakov'",
         "The paper gives a constructive method, based on greedy algorithms, that\nprovides for the classes of functions with small mixed smoothness the best\npossible in the sense of order approximation error for the $m$-term\napproximation with respect to the trigonometric system.",
         "41"
        ],
        [
         "77433",
         "abs-1505.00526v1",
         "An Explicit Sampling Dependent Spectral Error Bound for Column Subset\n  Selection",
         "Numerical Analysis",
         "math.NA",
         "2015-05-04 00:00:00",
         "2015-05-04 00:00:00",
         "['Tianbao Yang', 'Lijun Zhang', 'Rong Jin', 'Shenghuo Zhu']",
         "'Tianbao Yang'",
         "In this paper, we consider the problem of column subset selection. We present\na novel analysis of the spectral norm reconstruction for a simple randomized\nalgorithm and establish a new bound that depends explicitly on the sampling\nprobabilities. The sampling dependent error bound (i) allows us to better\nunderstand the tradeoff in the reconstruction error due to sampling\nprobabilities, (ii) exhibits more insights than existing error bounds that\nexploit specific probability distributions, and (iii) implies better sampling\ndistributions. In particular, we show that a sampling distribution with\nprobabilities proportional to the square root of the statistical leverage\nscores is always better than uniform sampling and is better than leverage-based\nsampling when the statistical leverage scores are very nonuniform. And by\nsolving a constrained optimization problem related to the error bound with an\nefficient bisection search we are able to achieve better performance than using\neither the leverage-based distribution or that proportional to the square root\nof the statistical leverage scores. Numerical simulations demonstrate the\nbenefits of the new sampling distributions for low-rank matrix approximation\nand least square approximation compared to state-of-the art algorithms.",
         "183"
        ],
        [
         "77469",
         "abs-1506.00059v3",
         "Saddle-free Hessian-free Optimization",
         "Numerical Analysis",
         "cs.NA",
         "2015-05-30 00:00:00",
         "2016-11-05 00:00:00",
         "['Martin Arjovsky']",
         "'Martin Arjovsky'",
         "Nonconvex optimization problems such as the ones in training deep neural\nnetworks suffer from a phenomenon called saddle point proliferation. This means\nthat there are a vast number of high error saddle points present in the loss\nfunction. Second order methods have been tremendously successful and widely\nadopted in the convex optimization community, while their usefulness in deep\nlearning remains limited. This is due to two problems: computational complexity\nand the methods being driven towards the high error saddle points. We introduce\na novel algorithm specially designed to solve these two issues, providing a\ncrucial first step to take the widely known advantages of Newton's method to\nthe nonconvex optimization community, especially in high dimensional settings.",
         "116"
        ],
        [
         "77902",
         "abs-1606.07686v2",
         "Gamblets for opening the complexity-bottleneck of implicit schemes for\n  hyperbolic and parabolic ODEs/PDEs with rough coefficients",
         "Numerical Analysis",
         "math.NA",
         "2016-06-24 00:00:00",
         "2017-06-30 00:00:00",
         "['Houman Owhadi', 'Lei Zhang']",
         "'Houman Owhadi'",
         "Implicit schemes are popular methods for the integration of time dependent\nPDEs such as hyperbolic and parabolic PDEs. However the necessity to solve\ncorresponding linear systems at each time step constitutes a complexity\nbottleneck in their application to PDEs with rough coefficients. We present a\ngeneralization of gamblets introduced in \\cite{OwhadiMultigrid:2015} enabling\nthe resolution of these implicit systems in near-linear complexity and provide\nrigorous a-priori error bounds on the resulting numerical approximations of\nhyperbolic and parabolic PDEs. These generalized gamblets induce a\nmultiresolution decomposition of the solution space that is adapted to both the\nunderlying (hyperbolic and parabolic) PDE (and the system of ODEs resulting\nfrom space discretization) and to the time-steps of the numerical scheme.",
         "116"
        ],
        [
         "78277",
         "abs-1703.03722v2",
         "Recovery of Sparse and Low Rank Components of Matrices Using Iterative\n  Method with Adaptive Thresholding",
         "Numerical Analysis",
         "cs.NA",
         "2017-03-09 00:00:00",
         "2017-04-12 00:00:00",
         "['Nematollah Zarmehi', 'Farokh Marvasti']",
         "'Nematollah Zarmehi'",
         "In this letter, we propose an algorithm for recovery of sparse and low rank\ncomponents of matrices using an iterative method with adaptive thresholding. In\neach iteration, the low rank and sparse components are obtained using a\nthresholding operator. This algorithm is fast and can be implemented easily. We\ncompare it with one of the most common fast methods in which the rank and\nsparsity are approximated by $\\ell_1$ norm. We also apply it to some real\napplications where the noise is not so sparse. The simulation results show that\nit has a suitable performance with low run-time.",
         "98"
        ],
        [
         "79555",
         "abs-1607.05073v1",
         "Higher-Order Block Term Decomposition for Spatially Folded fMRI Data",
         "Numerical Analysis",
         "cs.NA",
         "2016-07-15 00:00:00",
         "2016-07-15 00:00:00",
         "['Christos Chatzichristos', 'Eleftherios Kofidis', 'Giannis Kopsinis', 'Sergios Theodoridis']",
         "'Christos Chatzichristos'",
         "The growing use of neuroimaging technologies generates a massive amount of\nbiomedical data that exhibit high dimensionality. Tensor-based analysis of\nbrain imaging data has been proved quite effective in exploiting their multiway\nnature. The advantages of tensorial methods over matrix-based approaches have\nalso been demonstrated in the characterization of functional magnetic resonance\nimaging (fMRI) data, where the spatial (voxel) dimensions are commonly grouped\n(unfolded) as a single way/mode of the 3-rd order array, the other two ways\ncorresponding to time and subjects. However, such methods are known to be\nineffective in more demanding scenarios, such as the ones with strong noise\nand/or significant overlapping of activated regions. This paper aims at\ninvestigating the possible gains from a better exploitation of the spatial\ndimension, through a higher- (4 or 5) order tensor modeling of the fMRI signal.\nIn this context, and in order to increase the degrees of freedom of the\nmodeling process, a higher-order Block Term Decomposition (BTD) is applied, for\nthe first time in fMRI analysis. Its effectiveness is demonstrated via\nextensive simulation results.",
         "175"
        ],
        [
         "80717",
         "abs-1501.04819v1",
         "Separation of undersampled composite signals using the Dantzig selector\n  with overcomplete dictionaries",
         "Numerical Analysis",
         "math.NA",
         "2015-01-20 00:00:00",
         "2015-01-20 00:00:00",
         "['Ashley Prater', 'Lixin Shen']",
         "'Ashley Prater'",
         "In many applications one may acquire a composition of several signals that\nmay be corrupted by noise, and it is a challenging problem to reliably separate\nthe components from one another without sacrificing significant details. Adding\nto the challenge, in a compressive sensing framework, one is given only an\nundersampled set of linear projections of the composite signal. In this paper,\nwe propose using the Dantzig selector model incorporating an overcomplete\ndictionary to separate a noisy undersampled collection of composite signals,\nand present an algorithm to efficiently solve the model.\n  The Dantzig selector is a statistical approach to finding a solution to a\nnoisy linear regression problem by minimizing the $\\ell_1$ norm of candidate\ncoefficient vectors while constraining the scope of the residuals. If the\nunderlying coefficient vector is sparse, then the Dantzig selector performs\nwell in the recovery and separation of the unknown composite signal. In the\nfollowing, we propose a proximity operator based algorithm to recover and\nseparate unknown noisy undersampled composite signals through the Dantzig\nselector. We present numerical simulations comparing the proposed algorithm\nwith the competing Alternating Direction Method, and the proposed algorithm is\nfound to be faster, while producing similar quality results. Additionally, we\ndemonstrate the utility of the proposed algorithm in several experiments by\napplying it in various domain applications including the recovery of\ncomplex-valued coefficient vectors, the removal of impulse noise from smooth\nsignals, and the separation and classification of a composition of handwritten\ndigits.",
         "242"
        ],
        [
         "80977",
         "abs-1709.10276v1",
         "Fast online low-rank tensor subspace tracking by CP decomposition using\n  recursive least squares from incomplete observations",
         "Numerical Analysis",
         "cs.NA",
         "2017-09-29 00:00:00",
         "2017-09-29 00:00:00",
         "['Hiroyuki Kasai']",
         "'Hiroyuki Kasai'",
         "We consider the problem of online subspace tracking of a partially observed\nhigh-dimensional data stream corrupted by noise, where we assume that the data\nlie in a low-dimensional linear subspace. This problem is cast as an online\nlow-rank tensor completion problem. We propose a novel online tensor subspace\ntracking algorithm based on the CANDECOMP/PARAFAC (CP) decomposition, dubbed\nOnLine Low-rank Subspace tracking by TEnsor CP Decomposition (OLSTEC). The\nproposed algorithm especially addresses the case in which the subspace of\ninterest is dynamically time-varying. To this end, we build up our proposed\nalgorithm exploiting the recursive least squares (RLS), which is the\nsecond-order gradient algorithm. Numerical evaluations on synthetic datasets\nand real-world datasets such as communication network traffic, environmental\ndata, and surveillance videos, show that the proposed OLSTEC algorithm\noutperforms state-of-the-art online algorithms in terms of the convergence rate\nper iteration.",
         "139"
        ],
        [
         "81809",
         "abs-1511.01846v1",
         "Sparse approximation by greedy algorithms",
         "Numerical Analysis",
         "math.NA",
         "2015-11-05 00:00:00",
         "2015-11-05 00:00:00",
         "['Vladimir Temlyakov']",
         "'Vladimir Temlyakov'",
         "It is a survey on recent results in constructive sparse approximation. Three\ndirections are discussed here: (1) Lebesgue-type inequalities for greedy\nalgorithms with respect to a special class of dictionaries, (2) constructive\nsparse approximation with respect to the trigonometric system, (3) sparse\napproximation with respect to dictionaries with tensor product structure. In\nall three cases constructive ways are provided for sparse approximation. The\ntechnique used is based on fundamental results from the theory of greedy\napproximation. In particular, results in the direction (1) are based on deep\nmethods developed recently in compressed sensing. We present some of these\nresults with detailed proofs.",
         "102"
        ],
        [
         "83846",
         "abs-1412.8464v2",
         "Alternating Minimization Algorithm with Automatic Relevance\n  Determination for Transmission Tomography under Poisson Noise",
         "Numerical Analysis",
         "math.NA",
         "2014-12-29 00:00:00",
         "2015-08-11 00:00:00",
         "['Yan Kaganovsky', 'Shaobo Han', 'Soysal Degirmenci', 'David G. Politte', 'David J. Brady', \"Joseph A. O'Sullivan\", 'Lawrence Carin']",
         "'Yan Kaganovsky'",
         "We propose a globally convergent alternating minimization (AM) algorithm for\nimage reconstruction in transmission tomography, which extends automatic\nrelevance determination (ARD) to Poisson noise models with Beer's law. The\nalgorithm promotes solutions that are sparse in the pixel/voxel-differences\ndomain by introducing additional latent variables, one for each pixel/voxel,\nand then learning these variables from the data using a hierarchical Bayesian\nmodel. Importantly, the proposed AM algorithm is free of any tuning parameters\nwith image quality comparable to standard penalized likelihood methods. Our\nalgorithm exploits optimization transfer principles which reduce the problem\ninto parallel 1D optimization tasks (one for each pixel/voxel), making the\nalgorithm feasible for large-scale problems. This approach considerably reduces\nthe computational bottleneck of ARD associated with the posterior variances.\nPositivity constraints inherent in transmission tomography problems are also\nenforced. We demonstrate the performance of the proposed algorithm for x-ray\ncomputed tomography using synthetic and real-world datasets. The algorithm is\nshown to have much better performance than prior ARD algorithms based on\napproximate Gaussian noise models, even for high photon flux.",
         "172"
        ],
        [
         "94033",
         "abs-1005.4006v2",
         "Temporal Link Prediction using Matrix and Tensor Factorizations",
         "Numerical Analysis",
         "math.NA",
         "2010-05-21 00:00:00",
         "2010-06-19 00:00:00",
         "['Daniel M. Dunlavy', 'Tamara G. Kolda', 'Evrim Acar']",
         "'Daniel M. Dunlavy'",
         "The data in many disciplines such as social networks, web analysis, etc. is\nlink-based, and the link structure can be exploited for many different data\nmining tasks. In this paper, we consider the problem of temporal link\nprediction: Given link data for times 1 through T, can we predict the links at\ntime T+1? If our data has underlying periodic structure, can we predict out\neven further in time, i.e., links at time T+2, T+3, etc.? In this paper, we\nconsider bipartite graphs that evolve over time and consider matrix- and\ntensor-based methods for predicting future links. We present a weight-based\nmethod for collapsing multi-year data into a single matrix. We show how the\nwell-known Katz method for link prediction can be extended to bipartite graphs\nand, moreover, approximated in a scalable way using a truncated singular value\ndecomposition. Using a CANDECOMP/PARAFAC tensor decomposition of the data, we\nillustrate the usefulness of exploiting the natural three-dimensional structure\nof temporal link data. Through several numerical experiments, we demonstrate\nthat both matrix- and tensor-based techniques are effective for temporal link\nprediction despite the inherent difficulty of the problem. Additionally, we\nshow that tensor-based techniques are particularly effective for temporal data\nwith varying periodic patterns.",
         "202"
        ],
        [
         "94058",
         "abs-1008.3043v2",
         "Learning Functions of Few Arbitrary Linear Parameters in High Dimensions",
         "Numerical Analysis",
         "math.NA",
         "2010-08-18 00:00:00",
         "2012-01-17 00:00:00",
         "['Massimo Fornasier', 'Karin Schnass', 'Jan Vybiral']",
         "'Massimo Fornasier'",
         "Let us assume that $f$ is a continuous function defined on the unit ball of\n$\\mathbb R^d$, of the form $f(x) = g (A x)$, where $A$ is a $k \\times d$ matrix\nand $g$ is a function of $k$ variables for $k \\ll d$. We are given a budget $m\n\\in \\mathbb N$ of possible point evaluations $f(x_i)$, $i=1,...,m$, of $f$,\nwhich we are allowed to query in order to construct a uniform approximating\nfunction. Under certain smoothness and variation assumptions on the function\n$g$, and an {\\it arbitrary} choice of the matrix $A$, we present in this paper\n  1. a sampling choice of the points $\\{x_i\\}$ drawn at random for each\nfunction approximation;\n  2. algorithms (Algorithm 1 and Algorithm 2) for computing the approximating\nfunction, whose complexity is at most polynomial in the dimension $d$ and in\nthe number $m$ of points.\n  Due to the arbitrariness of $A$, the choice of the sampling points will be\naccording to suitable random distributions and our results hold with\noverwhelming probability. Our approach uses tools taken from the {\\it\ncompressed sensing} framework, recent Chernoff bounds for sums of\npositive-semidefinite matrices, and classical stability bounds for invariant\nsubspaces of singular value decompositions.",
         "200"
        ],
        [
         "94159",
         "abs-1206.4602v1",
         "Quasi-Newton Methods: A New Direction",
         "Numerical Analysis",
         "cs.NA",
         "2012-06-18 00:00:00",
         "2012-06-18 00:00:00",
         "['Philipp Hennig', 'Martin Kiefel']",
         "'Philipp Hennig'",
         "Four decades after their invention, quasi-Newton methods are still state of\nthe art in unconstrained numerical optimization. Although not usually\ninterpreted thus, these are learning algorithms that fit a local quadratic\napproximation to the objective function. We show that many, including the most\npopular, quasi-Newton methods can be interpreted as approximations of Bayesian\nlinear regression under varying prior assumptions. This new notion elucidates\nsome shortcomings of classical algorithms, and lights the way to a novel\nnonparametric quasi-Newton method, which is able to make more efficient use of\navailable information at computational cost similar to its predecessors.",
         "96"
        ],
        [
         "94171",
         "abs-1206.4640v1",
         "Stability of matrix factorization for collaborative filtering",
         "Numerical Analysis",
         "cs.NA",
         "2012-06-18 00:00:00",
         "2012-06-18 00:00:00",
         "['Yu-Xiang Wang', 'Huan Xu']",
         "'Yu-Xiang Wang'",
         "We study the stability vis a vis adversarial noise of matrix factorization\nalgorithm for matrix completion. In particular, our results include: (I) we\nbound the gap between the solution matrix of the factorization method and the\nground truth in terms of root mean square error; (II) we treat the matrix\nfactorization as a subspace fitting problem and analyze the difference between\nthe solution subspace and the ground truth; (III) we analyze the prediction\nerror of individual users based on the subspace stability. We apply these\nresults to the problem of collaborative filtering under manipulator attack,\nwhich leads to useful insights and guidelines for collaborative filtering\nsystem design.",
         "107"
        ],
        [
         "94528",
         "abs-1509.02314v2",
         "A Scalable and Extensible Framework for Superposition-Structured Models",
         "Numerical Analysis",
         "cs.NA",
         "2015-09-08 00:00:00",
         "2016-03-08 00:00:00",
         "['Shenjian Zhao', 'Cong Xie', 'Zhihua Zhang']",
         "'Shenjian Zhao'",
         "In many learning tasks, structural models usually lead to better\ninterpretability and higher generalization performance. In recent years,\nhowever, the simple structural models such as lasso are frequently proved to be\ninsufficient. Accordingly, there has been a lot of work on\n\"superposition-structured\" models where multiple structural constraints are\nimposed. To efficiently solve these \"superposition-structured\" statistical\nmodels, we develop a framework based on a proximal Newton-type method.\nEmploying the smoothed conic dual approach with the LBFGS updating formula, we\npropose a scalable and extensible proximal quasi-Newton (SEP-QN) framework.\nEmpirical analysis on various datasets shows that our framework is potentially\npowerful, and achieves super-linear convergence rate for optimizing some\npopular \"superposition-structured\" statistical models such as the fused sparse\ngroup lasso.",
         "118"
        ],
        [
         "94676",
         "abs-1606.01245v1",
         "Scalable Algorithms for Tractable Schatten Quasi-Norm Minimization",
         "Numerical Analysis",
         "cs.NA",
         "2016-06-04 00:00:00",
         "2016-06-04 00:00:00",
         "['Fanhua Shang', 'Yuanyuan Liu', 'James Cheng']",
         "'Fanhua Shang'",
         "The Schatten-p quasi-norm $(0<p<1)$ is usually used to replace the standard\nnuclear norm in order to approximate the rank function more accurately.\nHowever, existing Schatten-p quasi-norm minimization algorithms involve\nsingular value decomposition (SVD) or eigenvalue decomposition (EVD) in each\niteration, and thus may become very slow and impractical for large-scale\nproblems. In this paper, we first define two tractable Schatten quasi-norms,\ni.e., the Frobenius/nuclear hybrid and bi-nuclear quasi-norms, and then prove\nthat they are in essence the Schatten-2/3 and 1/2 quasi-norms, respectively,\nwhich lead to the design of very efficient algorithms that only need to update\ntwo much smaller factor matrices. We also design two efficient proximal\nalternating linearized minimization algorithms for solving representative\nmatrix completion problems. Finally, we provide the global convergence and\nperformance guarantees for our algorithms, which have better convergence\nproperties than existing algorithms. Experimental results on synthetic and\nreal-world data show that our algorithms are more accurate than the\nstate-of-the-art methods, and are orders of magnitude faster.",
         "161"
        ],
        [
         "94942",
         "abs-1706.01108v4",
         "Stochastic Reformulations of Linear Systems: Algorithms and Convergence\n  Theory",
         "Numerical Analysis",
         "math.NA",
         "2017-06-04 00:00:00",
         "2020-01-24 00:00:00",
         "['Peter RichtÃ¡rik', 'Martin TakÃ¡Ä']",
         "'Peter RichtÃ¡rik'",
         "We develop a family of reformulations of an arbitrary consistent linear\nsystem into a stochastic problem. The reformulations are governed by two\nuser-defined parameters: a positive definite matrix defining a norm, and an\narbitrary discrete or continuous distribution over random matrices. Our\nreformulation has several equivalent interpretations, allowing for researchers\nfrom various communities to leverage their domain specific insights. In\nparticular, our reformulation can be equivalently seen as a stochastic\noptimization problem, stochastic linear system, stochastic fixed point problem\nand a probabilistic intersection problem. We prove sufficient, and necessary\nand sufficient conditions for the reformulation to be exact. Further, we\npropose and analyze three stochastic algorithms for solving the reformulated\nproblem---basic, parallel and accelerated methods---with global linear\nconvergence rates. The rates can be interpreted as condition numbers of a\nmatrix which depends on the system matrix and on the reformulation parameters.\nThis gives rise to a new phenomenon which we call stochastic preconditioning,\nand which refers to the problem of finding parameters (matrix and distribution)\nleading to a sufficiently small condition number. Our basic method can be\nequivalently interpreted as stochastic gradient descent, stochastic Newton\nmethod, stochastic proximal point method, stochastic fixed point method, and\nstochastic projection method, with fixed stepsize (relaxation parameter),\napplied to the reformulations.",
         "207"
        ],
        [
         "95019",
         "abs-1706.05736v1",
         "Fixed-Rank Approximation of a Positive-Semidefinite Matrix from\n  Streaming Data",
         "Numerical Analysis",
         "cs.NA",
         "2017-06-18 00:00:00",
         "2017-06-18 00:00:00",
         "['Joel A. Tropp', 'Alp Yurtsever', 'Madeleine Udell', 'Volkan Cevher']",
         "'Joel A. Tropp'",
         "Several important applications, such as streaming PCA and semidefinite\nprogramming, involve a large-scale positive-semidefinite (psd) matrix that is\npresented as a sequence of linear updates. Because of storage limitations, it\nmay only be possible to retain a sketch of the psd matrix. This paper develops\na new algorithm for fixed-rank psd approximation from a sketch. The approach\ncombines the Nystrom approximation with a novel mechanism for rank truncation.\nTheoretical analysis establishes that the proposed method can achieve any\nprescribed relative error in the Schatten 1-norm and that it exploits the\nspectral decay of the input matrix. Computer experiments show that the proposed\nmethod dominates alternative techniques for fixed-rank psd matrix approximation\nacross a wide range of examples.",
         "117"
        ],
        [
         "95111",
         "abs-1707.03340v1",
         "Deep Learning for Real Time Crime Forecasting",
         "Numerical Analysis",
         "math.NA",
         "2017-07-09 00:00:00",
         "2017-07-09 00:00:00",
         "['Bao Wang', 'Duo Zhang', 'Duanhao Zhang', 'P. Jeffery Brantingham', 'Andrea L. Bertozzi']",
         "'Bao Wang'",
         "Accurate real time crime prediction is a fundamental issue for public safety,\nbut remains a challenging problem for the scientific community. Crime\noccurrences depend on many complex factors. Compared to many predictable\nevents, crime is sparse. At different spatio-temporal scales, crime\ndistributions display dramatically different patterns. These distributions are\nof very low regularity in both space and time. In this work, we adapt the\nstate-of-the-art deep learning spatio-temporal predictor, ST-ResNet [Zhang et\nal, AAAI, 2017], to collectively predict crime distribution over the Los\nAngeles area. Our models are two staged. First, we preprocess the raw crime\ndata. This includes regularization in both space and time to enhance\npredictable signals. Second, we adapt hierarchical structures of residual\nconvolutional units to train multi-factor crime prediction models. Experiments\nover a half year period in Los Angeles reveal highly accurate predictive power\nof our models.",
         "141"
        ],
        [
         "95229",
         "abs-1710.03608v1",
         "CTD: Fast, Accurate, and Interpretable Method for Static and Dynamic\n  Tensor Decompositions",
         "Numerical Analysis",
         "cs.NA",
         "2017-10-09 00:00:00",
         "2017-10-09 00:00:00",
         "['Jungwoo Lee', 'Dongjin Choi', 'Lee Sael']",
         "'Jungwoo Lee'",
         "How can we find patterns and anomalies in a tensor, or multi-dimensional\narray, in an efficient and directly interpretable way? How can we do this in an\nonline environment, where a new tensor arrives each time step? Finding patterns\nand anomalies in a tensor is a crucial problem with many applications,\nincluding building safety monitoring, patient health monitoring, cyber\nsecurity, terrorist detection, and fake user detection in social networks.\nStandard PARAFAC and Tucker decomposition results are not directly\ninterpretable. Although a few sampling-based methods have previously been\nproposed towards better interpretability, they need to be made faster, more\nmemory efficient, and more accurate.\n  In this paper, we propose CTD, a fast, accurate, and directly interpretable\ntensor decomposition method based on sampling. CTD-S, the static version of\nCTD, provably guarantees a high accuracy that is 17 ~ 83x more accurate than\nthat of the state-of-the-art method. Also, CTD-S is made 5 ~ 86x faster, and 7\n~ 12x more memory-efficient than the state-of-the-art method by removing\nredundancy. CTD-D, the dynamic version of CTD, is the first interpretable\ndynamic tensor decomposition method ever proposed. Also, it is made 2 ~ 3x\nfaster than already fast CTD-S by exploiting factors at previous time step and\nby reordering operations. With CTD, we demonstrate how the results can be\neffectively interpreted in the online distributed denial of service (DDoS)\nattack detection.",
         "226"
        ],
        [
         "95331",
         "abs-1712.04667v5",
         "Empirical Variance Minimization with Applications in Variance Reduction\n  and Optimal Control",
         "Numerical Analysis",
         "math.NA",
         "2017-12-13 00:00:00",
         "2021-07-31 00:00:00",
         "['D. Belomestny', 'L. Iosipoi', 'Q. Paris', 'N. Zhivotovskiy']",
         "'D. Belomestny'",
         "We study the problem of empirical minimization for variance-type functionals\nover functional classes. Sharp non-asymptotic bounds for the excess variance\nare derived under mild conditions. In particular, it is shown that under some\nrestrictions imposed on the functional class fast convergence rates can be\nachieved including the optimal non-parametric rates for expressive classes in\nthe non-Donsker regime under some additional assumptions. Our main applications\ninclude variance reduction and optimal control.",
         "70"
        ],
        [
         "96190",
         "abs-1803.08600v1",
         "Lower error bounds for the stochastic gradient descent optimization\n  algorithm: Sharp convergence rates for slowly and fast decaying learning\n  rates",
         "Numerical Analysis",
         "math.NA",
         "2018-03-22 00:00:00",
         "2018-03-22 00:00:00",
         "['Arnulf Jentzen', 'Philippe von Wurstemberger']",
         "'Arnulf Jentzen'",
         "The stochastic gradient descent (SGD) optimization algorithm plays a central\nrole in a series of machine learning applications. The scientific literature\nprovides a vast amount of upper error bounds for the SGD method. Much less\nattention as been paid to proving lower error bounds for the SGD method. It is\nthe key contribution of this paper to make a step in this direction. More\nprecisely, in this article we establish for every $\\gamma, \\nu \\in (0,\\infty)$\nessentially matching lower and upper bounds for the mean square error of the\nSGD process with learning rates $(\\frac{\\gamma}{n^\\nu})_{n \\in \\mathbb{N}}$\nassociated to a simple quadratic stochastic optimization problem. This allows\nus to precisely quantify the mean square convergence rate of the SGD method in\ndependence on the asymptotic behavior of the learning rates.",
         "130"
        ],
        [
         "96261",
         "abs-1803.10986v3",
         "Error Analysis and Improving the Accuracy of Winograd Convolution for\n  Deep Neural Networks",
         "Numerical Analysis",
         "cs.NA",
         "2018-03-29 00:00:00",
         "2019-05-01 00:00:00",
         "['Barbara Barabasz', 'Andrew Anderson', 'Kirk M. Soodhalter', 'David Gregg']",
         "'Barbara Barabasz'",
         "Popular deep neural networks (DNNs) spend the majority of their execution\ntime computing convolutions. The Winograd family of algorithms can greatly\nreduce the number of arithmetic operations required and is present in many DNN\nsoftware frameworks. However, the performance gain is at the expense of a\nreduction in floating point (FP) numerical accuracy. In this paper, we analyse\nthe worst case FP error and prove the estimation of norm and conditioning of\nthe algorithm. We show that the bound grows exponentially with the size of the\nconvolution, but the error bound of the \\textit{modified} algorithm is smaller\nthan the original one. We propose several methods for reducing FP error. We\npropose a canonical evaluation ordering based on Huffman coding that reduces\nsummation error. We study the selection of sampling \"points\" experimentally and\nfind empirically good points for the most important sizes. We identify the main\nfactors associated with good points. In addition, we explore other methods to\nreduce FP error, including mixed-precision convolution, and pairwise summation\nacross DNN channels. Using our methods we can significantly reduce FP error for\na given block size, which allows larger block sizes and reduced computation.",
         "191"
        ],
        [
         "96418",
         "abs-1805.06604v2",
         "Accelerating Nonnegative Matrix Factorization Algorithms using\n  Extrapolation",
         "Numerical Analysis",
         "cs.NA",
         "2018-05-17 00:00:00",
         "2018-07-12 00:00:00",
         "['Andersen Man Shun Ang', 'Nicolas Gillis']",
         "'Andersen Man Shun Ang'",
         "In this paper, we propose a general framework to accelerate significantly the\nalgorithms for nonnegative matrix factorization (NMF). This framework is\ninspired from the extrapolation scheme used to accelerate gradient methods in\nconvex optimization and from the method of parallel tangents. However, the use\nof extrapolation in the context of the two-block exact coordinate descent\nalgorithms tackling the non-convex NMF problems is novel. We illustrate the\nperformance of this approach on two state-of-the-art NMF algorithms, namely,\naccelerated hierarchical alternating least squares (A-HALS) and alternating\nnonnegative least squares (ANLS), using synthetic, image and document data\nsets.",
         "95"
        ],
        [
         "96446",
         "abs-1805.07451v4",
         "Butterfly-Net: Optimal Function Representation Based on Convolutional\n  Neural Networks",
         "Numerical Analysis",
         "math.NA",
         "2018-05-18 00:00:00",
         "2020-04-30 00:00:00",
         "['Yingzhou Li', 'Xiuyuan Cheng', 'Jianfeng Lu']",
         "'Yingzhou Li'",
         "Deep networks, especially convolutional neural networks (CNNs), have been\nsuccessfully applied in various areas of machine learning as well as to\nchallenging problems in other scientific and engineering fields. This paper\nintroduces Butterfly-Net, a low-complexity CNN with structured and sparse\ncross-channel connections, together with a Butterfly initialization strategy\nfor a family of networks. Theoretical analysis of the approximation power of\nButterfly-Net to the Fourier representation of input data shows that the error\ndecays exponentially as the depth increases. Combining Butterfly-Net with a\nfully connected neural network, a large class of problems are proved to be well\napproximated with network complexity depending on the effective frequency\nbandwidth instead of the input dimension. Regular CNN is covered as a special\ncase in our analysis. Numerical experiments validate the analytical results on\nthe approximation of Fourier kernels and energy functionals of Poisson's\nequations. Moreover, all experiments support that training from Butterfly\ninitialization outperforms training from random initialization. Also, adding\nthe remaining cross-channel connections, although significantly increase the\nparameter number, does not much improve the post-training accuracy and is more\nsensitive to data distribution.",
         "180"
        ],
        [
         "96752",
         "abs-2001.05437v2",
         "Neural network representation of the probability density function of\n  diffusion processes",
         "Numerical Analysis",
         "math.NA",
         "2020-01-15 00:00:00",
         "2020-04-19 00:00:00",
         "['Wayne Isaac Tan Uy', 'Mircea Grigoriu']",
         "'Wayne Isaac Tan Uy'",
         "Physics-informed neural networks are developed to characterize the state of\ndynamical systems in a random environment. The neural network approximates the\nprobability density function (pdf) or the characteristic function (chf) of the\nstate of these systems which satisfy the Fokker-Planck equation or an\nintegro-differential equation under Gaussian and/or Poisson white noises. We\nexamine analytically and numerically the advantages and disadvantages of\nsolving each type of differential equation to characterize the state. It is\nalso demonstrated how prior information of the dynamical system can be\nexploited to design and simplify the neural network architecture. Numerical\nexamples show that: 1) the neural network solution can approximate the target\nsolution even for partial integro-differential equations and system of PDEs\ndescribing the time evolution of the pdf/chf, 2) solving either the\nFokker-Planck equation or the chf differential equation using neural networks\nyields similar pdfs of the state, and 3) the solution to these differential\nequations can be used to study the behavior of the state for different types of\nrandom forcings.",
         "167"
        ],
        [
         "97353",
         "abs-1705.10015v1",
         "Learning the Sparse and Low Rank PARAFAC Decomposition via the Elastic\n  Net",
         "Numerical Analysis",
         "math.NA",
         "2017-05-29 00:00:00",
         "2017-05-29 00:00:00",
         "['Songting Shi', 'Xiang Li', 'Arkadiusz Sitek', 'Quanzheng Li']",
         "'Songting Shi'",
         "In this article, we derive a Bayesian model to learning the sparse and low\nrank PARAFAC decomposition for the observed tensor with missing values via the\nelastic net, with property to find the true rank and sparse factor matrix which\nis robust to the noise. We formulate efficient block coordinate descent\nalgorithm and admax stochastic block coordinate descent algorithm to solve it,\nwhich can be used to solve the large scale problem. To choose the appropriate\nrank and sparsity in PARAFAC decomposition, we will give a solution path by\ngradually increasing the regularization to increase the sparsity and decrease\nthe rank. When we find the sparse structure of the factor matrix, we can fixed\nthe sparse structure, using a small to regularization to decreasing the\nrecovery error, and one can choose the proper decomposition from the solution\npath with sufficient sparse factor matrix with low recovery error. We test the\npower of our algorithm on the simulation data and real data, which show it is\npowerful.",
         "166"
        ],
        [
         "97375",
         "abs-1709.00147v2",
         "Convergence Analysis of Deterministic Kernel-Based Quadrature Rules in\n  Misspecified Settings",
         "Numerical Analysis",
         "math.NA",
         "2017-09-01 00:00:00",
         "2018-10-30 00:00:00",
         "['Motonobu Kanagawa', 'Bharath K. Sriperumbudur', 'Kenji Fukumizu']",
         "'Motonobu Kanagawa'",
         "This paper presents a convergence analysis of kernel-based quadrature rules\nin misspecified settings, focusing on deterministic quadrature in Sobolev\nspaces. In particular, we deal with misspecified settings where a test\nintegrand is less smooth than a Sobolev RKHS based on which a quadrature rule\nis constructed. We provide convergence guarantees based on two different\nassumptions on a quadrature rule: one on quadrature weights, and the other on\ndesign points. More precisely, we show that convergence rates can be derived\n(i) if the sum of absolute weights remains constant (or does not increase\nquickly), or (ii) if the minimum distance between design points does not\ndecrease very quickly. As a consequence of the latter result, we derive a rate\nof convergence for Bayesian quadrature in misspecified settings. We reveal a\ncondition on design points to make Bayesian quadrature robust to\nmisspecification, and show that, under this condition, it may adaptively\nachieve the optimal rate of convergence in the Sobolev space of a lesser order\n(i.e., of the unknown smoothness of a test integrand), under a slightly\nstronger regularity condition on the integrand.",
         "181"
        ],
        [
         "97446",
         "abs-1904.09848v2",
         "Bayesian inversion for nanowire field-effect sensors",
         "Numerical Analysis",
         "math.NA",
         "2019-04-12 00:00:00",
         "2019-10-26 00:00:00",
         "['Amirreza Khodadadian', 'Benjamin Stadlbauer', 'Clemens Heitzinger']",
         "'Amirreza Khodadadian'",
         "Nanowire field-effect sensors have recently been developed for label-free\ndetection of biomolecules. In this work, we introduce a computational technique\nbased on Bayesian estimation to determine the physical parameters of the sensor\nand, more importantly, the properties of the analyte molecules. To that end, we\nfirst propose a PDE based model to simulate the device charge transport and\nelectrochemical behavior. Then, the adaptive Metropolis algorithm with delayed\nrejection (DRAM) is applied to estimate the posterior distribution of unknown\nparameters, namely molecule charge density, molecule density, doping\nconcentration, and electron and hole mobilities. We determine the device and\nmolecules properties simultaneously, and we also calculate the molecule density\nas the only parameter after having determined the device parameters. This\napproach makes it possible not only to determine unknown parameters, but it\nalso shows how well each parameter can be determined by yielding the\nprobability density function (pdf).",
         "146"
        ],
        [
         "97849",
         "abs-2010.11708v2",
         "Context-aware surrogate modeling for balancing approximation and\n  sampling costs in multi-fidelity importance sampling and Bayesian inverse\n  problems",
         "Numerical Analysis",
         "math.NA",
         "2020-10-22 00:00:00",
         "2021-09-12 00:00:00",
         "['Terrence Alsup', 'Benjamin Peherstorfer']",
         "'Terrence Alsup'",
         "Multi-fidelity methods leverage low-cost surrogate models to speed up\ncomputations and make occasional recourse to expensive high-fidelity models to\nestablish accuracy guarantees. Because surrogate and high-fidelity models are\nused together, poor predictions by surrogate models can be compensated with\nfrequent recourse to high-fidelity models. Thus, there is a trade-off between\ninvesting computational resources to improve the accuracy of surrogate models\nversus simply making more frequent recourse to expensive high-fidelity models;\nhowever, this trade-off is ignored by traditional modeling methods that\nconstruct surrogate models that are meant to replace high-fidelity models\nrather than being used together with high-fidelity models. This work considers\nmulti-fidelity importance sampling and theoretically and computationally trades\noff increasing the fidelity of surrogate models for constructing more accurate\nbiasing densities and the numbers of samples that are required from the\nhigh-fidelity models to compensate poor biasing densities. Numerical examples\ndemonstrate that such context-aware surrogate models for multi-fidelity\nimportance sampling have lower fidelity than what typically is set as tolerance\nin traditional model reduction, leading to runtime speedups of up to one order\nof magnitude in the presented examples.",
         "180"
        ],
        [
         "97945",
         "abs-2010.15959v1",
         "Over-parametrized neural networks as under-determined linear systems",
         "Numerical Analysis",
         "math.NA",
         "2020-10-29 00:00:00",
         "2020-10-29 00:00:00",
         "['Austin R. Benson', 'Anil Damle', 'Alex Townsend']",
         "'Austin R. Benson'",
         "We draw connections between simple neural networks and under-determined\nlinear systems to comprehensively explore several interesting theoretical\nquestions in the study of neural networks. First, we emphatically show that it\nis unsurprising such networks can achieve zero training loss. More\nspecifically, we provide lower bounds on the width of a single hidden layer\nneural network such that only training the last linear layer suffices to reach\nzero training loss. Our lower bounds grow more slowly with data set size than\nexisting work that trains the hidden layer weights. Second, we show that\nkernels typically associated with the ReLU activation function have fundamental\nflaws -- there are simple data sets where it is impossible for widely studied\nbias-free models to achieve zero training loss irrespective of how the\nparameters are chosen or trained. Lastly, our analysis of gradient descent\nclearly illustrates how spectral properties of certain matrices impact both the\nearly iteration and long-term training behavior. We propose new activation\nfunctions that avoid the pitfalls of ReLU in that they admit zero training loss\nsolutions for any set of distinct data points and experimentally exhibit\nfavorable spectral properties.",
         "187"
        ],
        [
         "98930",
         "abs-2006.16376v1",
         "Bayesian Sparse learning with preconditioned stochastic gradient MCMC\n  and its applications",
         "Numerical Analysis",
         "math.NA",
         "2020-06-29 00:00:00",
         "2020-06-29 00:00:00",
         "['Yating Wang', 'Wei Deng', 'Lin Guang']",
         "'Yating Wang'",
         "In this work, we propose a Bayesian type sparse deep learning algorithm. The\nalgorithm utilizes a set of spike-and-slab priors for the parameters in the\ndeep neural network. The hierarchical Bayesian mixture will be trained using an\nadaptive empirical method. That is, one will alternatively sample from the\nposterior using preconditioned stochastic gradient Langevin Dynamics (PSGLD),\nand optimize the latent variables via stochastic approximation. The sparsity of\nthe network is achieved while optimizing the hyperparameters with adaptive\nsearching and penalizing. A popular SG-MCMC approach is Stochastic gradient\nLangevin dynamics (SGLD). However, considering the complex geometry in the\nmodel parameter space in non-convex learning, updating parameters using a\nuniversal step size in each component as in SGLD may cause slow mixing. To\naddress this issue, we apply a computationally manageable preconditioner in the\nupdating rule, which provides a step-size parameter to adapt to local geometric\nproperties. Moreover, by smoothly optimizing the hyperparameter in the\npreconditioning matrix, our proposed algorithm ensures a decreasing bias, which\nis introduced by ignoring the correction term in preconditioned SGLD. According\nto the existing theoretical framework, we show that the proposed algorithm can\nasymptotically converge to the correct distribution with a controllable bias\nunder mild conditions. Numerical tests are performed on both synthetic\nregression problems and learning the solutions of elliptic PDE, which\ndemonstrate the accuracy and efficiency of present work.",
         "224"
        ],
        [
         "99062",
         "abs-2103.05744v1",
         "Deep neural network approximation for high-dimensional parabolic\n  Hamilton-Jacobi-Bellman equations",
         "Numerical Analysis",
         "math.NA",
         "2021-03-09 00:00:00",
         "2021-03-09 00:00:00",
         "['Philipp Grohs', 'Lukas Herrmann']",
         "'Philipp Grohs'",
         "The approximation of solutions to second order Hamilton--Jacobi--Bellman\n(HJB) equations by deep neural networks is investigated. It is shown that for\nHJB equations that arise in the context of the optimal control of certain\nMarkov processes the solution can be approximated by deep neural networks\nwithout incurring the curse of dimension. The dynamics is assumed to depend\naffinely on the controls and the cost depends quadratically on the controls.\nThe admissible controls take values in a bounded set.",
         "78"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 81
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>category_code</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>first_author</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39965</th>\n",
       "      <td>abs-1312.6872v1</td>\n",
       "      <td>Matrix recovery using Split Bregman</td>\n",
       "      <td>Numerical Analysis</td>\n",
       "      <td>cs.NA</td>\n",
       "      <td>2013-12-17</td>\n",
       "      <td>2013-12-17</td>\n",
       "      <td>['Anupriya Gogna', 'Ankita Shukla', 'Angshul M...</td>\n",
       "      <td>'Anupriya Gogna'</td>\n",
       "      <td>In this paper we address the problem of recove...</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39971</th>\n",
       "      <td>abs-1401.0159v1</td>\n",
       "      <td>Speeding-Up Convergence via Sequential Subspac...</td>\n",
       "      <td>Numerical Analysis</td>\n",
       "      <td>cs.NA</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>['Michael Zibulevsky']</td>\n",
       "      <td>'Michael Zibulevsky'</td>\n",
       "      <td>This is an overview paper written in style of ...</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39985</th>\n",
       "      <td>abs-1407.1399v1</td>\n",
       "      <td>Generalized Higher-Order Tensor Decomposition ...</td>\n",
       "      <td>Numerical Analysis</td>\n",
       "      <td>cs.NA</td>\n",
       "      <td>2014-07-05</td>\n",
       "      <td>2014-07-05</td>\n",
       "      <td>['Fanhua Shang', 'Yuanyuan Liu', 'James Cheng']</td>\n",
       "      <td>'Fanhua Shang'</td>\n",
       "      <td>Higher-order tensors are becoming prevalent in...</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40351</th>\n",
       "      <td>abs-1601.07721v1</td>\n",
       "      <td>Distributed Low Rank Approximation of Implicit...</td>\n",
       "      <td>Numerical Analysis</td>\n",
       "      <td>cs.NA</td>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>['David P. Woodruff', 'Peilin Zhong']</td>\n",
       "      <td>'David P. Woodruff'</td>\n",
       "      <td>We study distributed low rank approximation in...</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40889</th>\n",
       "      <td>abs-1707.09428v1</td>\n",
       "      <td>A unified method for super-resolution recovery...</td>\n",
       "      <td>Numerical Analysis</td>\n",
       "      <td>math.NA</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>['Charles K. Chui', 'Hrushikesh N. Mhaskar']</td>\n",
       "      <td>'Charles K. Chui'</td>\n",
       "      <td>In this paper, motivated by diffraction of tra...</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111897</th>\n",
       "      <td>abs-2309.05947v3</td>\n",
       "      <td>Tumoral Angiogenic Optimizer: A new bio-inspir...</td>\n",
       "      <td>Numerical Analysis</td>\n",
       "      <td>math.NA</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>['HernÃ¡ndez RodrÃ­guez', 'MatÃ­as Ezequiel']</td>\n",
       "      <td>'HernÃ¡ndez RodrÃ­guez'</td>\n",
       "      <td>In this article, we propose a new metaheuristi...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112591</th>\n",
       "      <td>abs-1605.09232v3</td>\n",
       "      <td>Tradeoffs between Convergence Speed and Recons...</td>\n",
       "      <td>Numerical Analysis</td>\n",
       "      <td>cs.NA</td>\n",
       "      <td>2016-05-30</td>\n",
       "      <td>2018-02-15</td>\n",
       "      <td>['Raja Giryes', 'Yonina C. Eldar', 'Alex M. Br...</td>\n",
       "      <td>'Raja Giryes'</td>\n",
       "      <td>Solving inverse problems with iterative algori...</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112632</th>\n",
       "      <td>abs-1706.04702v1</td>\n",
       "      <td>Deep learning-based numerical methods for high...</td>\n",
       "      <td>Numerical Analysis</td>\n",
       "      <td>math.NA</td>\n",
       "      <td>2017-06-15</td>\n",
       "      <td>2017-06-15</td>\n",
       "      <td>['Weinan E', 'Jiequn Han', 'Arnulf Jentzen']</td>\n",
       "      <td>'Weinan E'</td>\n",
       "      <td>We propose a new algorithm for solving parabol...</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112975</th>\n",
       "      <td>abs-2408.14057v1</td>\n",
       "      <td>Revisiting time-variant complex conjugate matr...</td>\n",
       "      <td>Numerical Analysis</td>\n",
       "      <td>math.NA</td>\n",
       "      <td>2024-08-26</td>\n",
       "      <td>2024-08-26</td>\n",
       "      <td>['Jiakuang He', 'Dongqing Wu']</td>\n",
       "      <td>'Jiakuang He'</td>\n",
       "      <td>Large-scale linear equations and high dimensio...</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112986</th>\n",
       "      <td>abs-2411.02333v1</td>\n",
       "      <td>Discrete the solving model of time-variant sta...</td>\n",
       "      <td>Numerical Analysis</td>\n",
       "      <td>math.NA</td>\n",
       "      <td>2024-11-04</td>\n",
       "      <td>2024-11-04</td>\n",
       "      <td>['Jiakuang He', 'Dongqing Wu']</td>\n",
       "      <td>'Jiakuang He'</td>\n",
       "      <td>Time-variant standard Sylvester-conjugate matr...</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows Ã 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                              title  \\\n",
       "39965    abs-1312.6872v1                Matrix recovery using Split Bregman   \n",
       "39971    abs-1401.0159v1  Speeding-Up Convergence via Sequential Subspac...   \n",
       "39985    abs-1407.1399v1  Generalized Higher-Order Tensor Decomposition ...   \n",
       "40351   abs-1601.07721v1  Distributed Low Rank Approximation of Implicit...   \n",
       "40889   abs-1707.09428v1  A unified method for super-resolution recovery...   \n",
       "...                  ...                                                ...   \n",
       "111897  abs-2309.05947v3  Tumoral Angiogenic Optimizer: A new bio-inspir...   \n",
       "112591  abs-1605.09232v3  Tradeoffs between Convergence Speed and Recons...   \n",
       "112632  abs-1706.04702v1  Deep learning-based numerical methods for high...   \n",
       "112975  abs-2408.14057v1  Revisiting time-variant complex conjugate matr...   \n",
       "112986  abs-2411.02333v1  Discrete the solving model of time-variant sta...   \n",
       "\n",
       "                  category category_code published_date updated_date  \\\n",
       "39965   Numerical Analysis         cs.NA     2013-12-17   2013-12-17   \n",
       "39971   Numerical Analysis         cs.NA     2013-12-31   2013-12-31   \n",
       "39985   Numerical Analysis         cs.NA     2014-07-05   2014-07-05   \n",
       "40351   Numerical Analysis         cs.NA     2016-01-28   2016-01-28   \n",
       "40889   Numerical Analysis       math.NA     2017-07-26   2017-07-26   \n",
       "...                    ...           ...            ...          ...   \n",
       "111897  Numerical Analysis       math.NA     2023-09-12   2023-09-20   \n",
       "112591  Numerical Analysis         cs.NA     2016-05-30   2018-02-15   \n",
       "112632  Numerical Analysis       math.NA     2017-06-15   2017-06-15   \n",
       "112975  Numerical Analysis       math.NA     2024-08-26   2024-08-26   \n",
       "112986  Numerical Analysis       math.NA     2024-11-04   2024-11-04   \n",
       "\n",
       "                                                  authors  \\\n",
       "39965   ['Anupriya Gogna', 'Ankita Shukla', 'Angshul M...   \n",
       "39971                              ['Michael Zibulevsky']   \n",
       "39985     ['Fanhua Shang', 'Yuanyuan Liu', 'James Cheng']   \n",
       "40351               ['David P. Woodruff', 'Peilin Zhong']   \n",
       "40889        ['Charles K. Chui', 'Hrushikesh N. Mhaskar']   \n",
       "...                                                   ...   \n",
       "111897         ['HernÃ¡ndez RodrÃ­guez', 'MatÃ­as Ezequiel']   \n",
       "112591  ['Raja Giryes', 'Yonina C. Eldar', 'Alex M. Br...   \n",
       "112632       ['Weinan E', 'Jiequn Han', 'Arnulf Jentzen']   \n",
       "112975                     ['Jiakuang He', 'Dongqing Wu']   \n",
       "112986                     ['Jiakuang He', 'Dongqing Wu']   \n",
       "\n",
       "                 first_author  \\\n",
       "39965        'Anupriya Gogna'   \n",
       "39971    'Michael Zibulevsky'   \n",
       "39985          'Fanhua Shang'   \n",
       "40351     'David P. Woodruff'   \n",
       "40889       'Charles K. Chui'   \n",
       "...                       ...   \n",
       "111897  'HernÃ¡ndez RodrÃ­guez'   \n",
       "112591          'Raja Giryes'   \n",
       "112632             'Weinan E'   \n",
       "112975          'Jiakuang He'   \n",
       "112986          'Jiakuang He'   \n",
       "\n",
       "                                                  summary  summary_word_count  \n",
       "39965   In this paper we address the problem of recove...                 157  \n",
       "39971   This is an overview paper written in style of ...                 161  \n",
       "39985   Higher-order tensors are becoming prevalent in...                 166  \n",
       "40351   We study distributed low rank approximation in...                 212  \n",
       "40889   In this paper, motivated by diffraction of tra...                 151  \n",
       "...                                                   ...                 ...  \n",
       "111897  In this article, we propose a new metaheuristi...                 185  \n",
       "112591  Solving inverse problems with iterative algori...                 164  \n",
       "112632  We propose a new algorithm for solving parabol...                 118  \n",
       "112975  Large-scale linear equations and high dimensio...                 134  \n",
       "112986  Time-variant standard Sylvester-conjugate matr...                 121  \n",
       "\n",
       "[81 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter for rows where category is 'Numerical Analysis'\n",
    "na_rows = df[df['category'] == 'Numerical Analysis']\n",
    "display(na_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Numerical Analysis' category is the only one that seems to map to two codes. And, this happens more than once. It is unclear if this is a data entry error or if the category is actually meant to be two codes. I will leave this for now, but it may be worth exploring more later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 84 duplicate titles\n",
      "title\n",
      "Fairness in Reinforcement Learning                                                                             2\n",
      "SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis\\n  Using Artificial Neural Networks    2\n",
      "Bridging the Gap Between Target Networks and Functional Regularization                                         2\n",
      "Conditional Plausibility Measures and Bayesian Networks                                                        2\n",
      "Interpretable Two-level Boolean Rule Learning for Classification                                               2\n",
      "                                                                                                              ..\n",
      "Standards for Language Resources                                                                               2\n",
      "Fair Active Learning                                                                                           2\n",
      "Hypertree Decompositions Revisited for PGMs                                                                    2\n",
      "Variational Laplace for Bayesian neural networks                                                               2\n",
      "Iteratively Training Look-Up Tables for Network Quantization                                                   2\n",
      "Name: count, Length: 84, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Investigate why the 'title' unique values do not match the id unique values\n",
    "title_cts= df['title'].value_counts()\n",
    "non_unique_titles = title_cts[title_cts > 1]\n",
    "print(f\"There are {len(non_unique_titles)} duplicate titles\")\n",
    "print(non_unique_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category_code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "published_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "updated_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "first_author",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary_word_count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "496113fa-ab73-4cdc-bffe-b746bb217f04",
       "rows": [
        [
         "130124",
         "cmp-lg-9407014v1",
         "Abstract Machine for Typed Feature Structures",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-07-17 00:00:00",
         "1994-07-17 00:00:00",
         "['Shuly Wintner', 'Nissim Francez']",
         "'Shuly Wintner'",
         "This paper describes a first step towards the definition of an abstract\nmachine for linguistic formalisms that are based on typed feature structures,\nsuch as HPSG. The core design of the abstract machine is given in detail,\nincluding the compilation process from a high-level specification language to\nthe abstract machine language and the implementation of the abstract\ninstructions. We thus apply methods that were proved useful in computer science\nto the study of natural languages: a grammar specified using the formalism is\nendowed with an operational semantics. Currently, our machine supports the\nunification of simple feature structures, unification of sequences of such\nstructures, cyclic structures and disjunction.",
         "107"
        ],
        [
         "130324",
         "cmp-lg-9504009v1",
         "Abstract Machine for Typed Feature Structures",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1995-04-13 00:00:00",
         "1995-04-13 00:00:00",
         "['Shuly Wintner', 'Nissim Francez']",
         "'Shuly Wintner'",
         "This paper describes an abstract machine for linguistic formalisms that are\nbased on typed feature structures, such as HPSG. The core design of the\nabstract machine is given in detail, including the compilation process from a\nhigh-level language to the abstract machine language and the implementation of\nthe abstract instructions. The machine's engine supports the unification of\ntyped, possibly cyclic, feature structures. A separate module deals with\ncontrol structures and instructions to accommodate parsing for phrase structure\ngrammars. We treat the linguistic formalism as a high-level declarative\nprogramming language, applying methods that were proved useful in computer\nscience to the study of natural languages: a grammar specified using the\nformalism is endowed with an operational semantics.",
         "116"
        ],
        [
         "2472",
         "abs-1408.2056v1",
         "Active Sensing as Bayes-Optimal Sequential Decision Making",
         "Artificial Intelligence",
         "cs.AI",
         "2014-08-09 00:00:00",
         "2014-08-09 00:00:00",
         "['Sheeraz Ahmad', 'Angela Yu']",
         "'Sheeraz Ahmad'",
         "Sensory inference under conditions of uncertainty is a major problem in both\nmachine learning and computational neuroscience. An important but poorly\nunderstood aspect of sensory processing is the role of active sensing. Here, we\npresent a Bayes-optimal inference and control framework for active sensing,\nC-DAC (Context-Dependent Active Controller). Unlike previously proposed\nalgorithms that optimize abstract statistical objectives such as information\nmaximization (Infomax) [Butko & Movellan, 2010] or one-step look-ahead accuracy\n[Najemnik & Geisler, 2005], our active sensing model directly minimizes a\ncombination of behavioral costs, such as temporal delay, response error, and\neffort. We simulate these algorithms on a simple visual search task to\nillustrate scenarios in which context-sensitivity is particularly beneficial\nand optimization with respect to generic statistical objectives particularly\ninadequate. Motivated by the geometric properties of the C-DAC policy, we\npresent both parametric and non-parametric approximations, which retain\ncontext-sensitivity while significantly reducing computational complexity.\nThese approximations enable us to investigate the more complex problem\ninvolving peripheral vision, and we notice that the difference between C-DAC\nand statistical policies becomes even more evident in this scenario.",
         "177"
        ],
        [
         "10523",
         "abs-1305.6650v1",
         "Active Sensing as Bayes-Optimal Sequential Decision Making",
         "Artificial Intelligence",
         "cs.AI",
         "2013-05-28 00:00:00",
         "2013-05-28 00:00:00",
         "['Sheeraz Ahmad', 'Angela J. Yu']",
         "'Sheeraz Ahmad'",
         "Sensory inference under conditions of uncertainty is a major problem in both\nmachine learning and computational neuroscience. An important but poorly\nunderstood aspect of sensory processing is the role of active sensing. Here, we\npresent a Bayes-optimal inference and control framework for active sensing,\nC-DAC (Context-Dependent Active Controller). Unlike previously proposed\nalgorithms that optimize abstract statistical objectives such as information\nmaximization (Infomax) [Butko & Movellan, 2010] or one-step look-ahead accuracy\n[Najemnik & Geisler, 2005], our active sensing model directly minimizes a\ncombination of behavioral costs, such as temporal delay, response error, and\neffort. We simulate these algorithms on a simple visual search task to\nillustrate scenarios in which context-sensitivity is particularly beneficial\nand optimization with respect to generic statistical objectives particularly\ninadequate. Motivated by the geometric properties of the C-DAC policy, we\npresent both parametric and non-parametric approximations, which retain\ncontext-sensitivity while significantly reducing computational complexity.\nThese approximations enable us to investigate the more complex problem\ninvolving peripheral vision, and we notice that the difference between C-DAC\nand statistical policies becomes even more evident in this scenario.",
         "177"
        ],
        [
         "2467",
         "abs-1408.2034v1",
         "Approximate inference on planar graphs using Loop Calculus and Belief\n  Propagation",
         "Artificial Intelligence",
         "cs.AI",
         "2014-08-09 00:00:00",
         "2014-08-09 00:00:00",
         "['Vicenc Gomez', 'Hilbert Kappen', 'Michael Chertkov']",
         "'Vicenc Gomez'",
         "We introduce novel results for approximate inference on planar graphical\nmodels using the loop calculus framework. The loop calculus (Chertkov and\nChernyak, 2006b) allows to express the exact partition function Z of a\ngraphical model as a finite sum of terms that can be evaluated once the belief\npropagation (BP) solution is known. In general, full summation over all\ncorrection terms is intractable. We develop an algorithm for the approach\npresented in Chertkov et al. (2008) which represents an efficient truncation\nscheme on planar graphs and a new representation of the series in terms of\nPfaffians of matrices. We analyze in detail both the loop series and the\nPfaffian series for models with binary variables and pairwise interactions, and\nshow that the first term of the Pfaffian series can provide very accurate\napproximations. The algorithm outperforms previous truncation schemes of the\nloop series and is competitive with other state-of-the-art methods for\napproximate inference.",
         "153"
        ],
        [
         "464",
         "abs-0901.0786v3",
         "Approximate inference on planar graphs using Loop Calculus and Belief\n  Propagation",
         "Artificial Intelligence",
         "cs.AI",
         "2009-01-07 00:00:00",
         "2009-05-25 00:00:00",
         "['V. GÃ³mez', 'H. J. Kappen', 'M. Chertkov']",
         "'V. GÃ³mez'",
         "We introduce novel results for approximate inference on planar graphical\nmodels using the loop calculus framework. The loop calculus (Chertkov and\nChernyak, 2006) allows to express the exact partition function of a graphical\nmodel as a finite sum of terms that can be evaluated once the belief\npropagation (BP) solution is known. In general, full summation over all\ncorrection terms is intractable. We develop an algorithm for the approach\npresented in (Certkov et al., 2008) which represents an efficient truncation\nscheme on planar graphs and a new representation of the series in terms of\nPfaffians of matrices. We analyze the performance of the algorithm for the\npartition function approximation for models with binary variables and pairwise\ninteractions on grids and other planar graphs. We study in detail both the loop\nseries and the equivalent Pfaffian series and show that the first term of the\nPfaffian series for the general, intractable planar model, can provide very\naccurate approximations. The algorithm outperforms previous truncation schemes\nof the loop series and is competitive with other state-of-the-art methods for\napproximate inference.",
         "177"
        ],
        [
         "10876",
         "abs-1408.1482v1",
         "Axiomatizing Causal Reasoning",
         "Artificial Intelligence",
         "cs.AI",
         "2014-08-07 00:00:00",
         "2014-08-07 00:00:00",
         "['Joseph Y. Halpern']",
         "'Joseph Y. Halpern'",
         "Causal models defined in terms of a collection of equations, as defined by\nPearl, are axiomatized here. Axiomatizations are provided for three\nsuccessively more general classes of causal models: (1) the class of recursive\ntheories (those without feedback), (2) the class of theories where the\nsolutions to the equations are unique, (3) arbitrary theories (where the\nequations may not have solutions and, if they do, they are not necessarily\nunique). It is shown that to reason about causality in the most general third\nclass, we must extend the language used by Galles and Pearl. In addition, the\ncomplexity of the decision procedures is examined for all the languages and\nclasses of models considered.",
         "113"
        ],
        [
         "9374",
         "cs-0005030v1",
         "Axiomatizing Causal Reasoning",
         "Artificial Intelligence",
         "cs.AI",
         "2000-05-30 00:00:00",
         "2000-05-30 00:00:00",
         "['Joseph Y. Halpern']",
         "'Joseph Y. Halpern'",
         "Causal models defined in terms of a collection of equations, as defined by\nPearl, are axiomatized here. Axiomatizations are provided for three\nsuccessively more general classes of causal models: (1) the class of recursive\ntheories (those without feedback), (2) the class of theories where the\nsolutions to the equations are unique, (3) arbitrary theories (where the\nequations may not have solutions and, if they do, they are not necessarily\nunique). It is shown that to reason about causality in the most general third\nclass, we must extend the language used by Galles and Pearl. In addition, the\ncomplexity of the decision procedures is characterized for all the languages\nand classes of models considered.",
         "113"
        ],
        [
         "24601",
         "cs-0703062v1",
         "Bandit Algorithms for Tree Search",
         "Machine Learning",
         "cs.LG",
         "2007-03-13 00:00:00",
         "2007-03-13 00:00:00",
         "['Pierre-Arnaud Coquelin', 'RÃ©mi Munos']",
         "'Pierre-Arnaud Coquelin'",
         "Bandit based methods for tree search have recently gained popularity when\napplied to huge trees, e.g. in the game of go (Gelly et al., 2006). The UCT\nalgorithm (Kocsis and Szepesvari, 2006), a tree search method based on Upper\nConfidence Bounds (UCB) (Auer et al., 2002), is believed to adapt locally to\nthe effective smoothness of the tree. However, we show that UCT is too\n``optimistic'' in some cases, leading to a regret O(exp(exp(D))) where D is the\ndepth of the tree. We propose alternative bandit algorithms for tree search.\nFirst, a modification of UCT using a confidence sequence that scales\nexponentially with the horizon depth is proven to have a regret O(2^D\n\\sqrt{n}), but does not adapt to possible smoothness in the tree. We then\nanalyze Flat-UCB performed on the leaves and provide a finite regret bound with\nhigh probability. Then, we introduce a UCB-based Bandit Algorithm for Smooth\nTrees which takes into account actual smoothness of the rewards for performing\nefficient ``cuts'' of sub-optimal branches with high confidence. Finally, we\npresent an incremental tree search version which applies when the full tree is\ntoo big (possibly infinite) to be entirely represented and show that with high\nprobability, essentially only the optimal branches is indefinitely developed.\nWe illustrate these methods on a global optimization problem of a Lipschitz\nfunction, given noisy data.",
         "223"
        ],
        [
         "2465",
         "abs-1408.2028v1",
         "Bandit Algorithms for Tree Search",
         "Artificial Intelligence",
         "cs.AI",
         "2014-08-09 00:00:00",
         "2014-08-09 00:00:00",
         "['Pierre-Arnuad Coquelin', 'Remi Munos']",
         "'Pierre-Arnuad Coquelin'",
         "Bandit based methods for tree search have recently gained popularity when\napplied to huge trees, e.g. in the game of go [6]. Their efficient exploration\nof the tree enables to re- turn rapidly a good value, and improve preci- sion\nif more time is provided. The UCT algo- rithm [8], a tree search method based\non Up- per Confidence Bounds (UCB) [2], is believed to adapt locally to the\neffective smoothness of the tree. However, we show that UCT is\n\"over-optimistic\" in some sense, leading to a worst-case regret that may be\nvery poor. We propose alternative bandit algorithms for tree search. First, a\nmodification of UCT us- ing a confidence sequence that scales expo- nentially\nin the horizon depth is analyzed. We then consider Flat-UCB performed on the\nleaves and provide a finite regret bound with high probability. Then, we\nintroduce and analyze a Bandit Algorithm for Smooth Trees (BAST) which takes\ninto account ac- tual smoothness of the rewards for perform- ing efficient\n\"cuts\" of sub-optimal branches with high confidence. Finally, we present an\nincremental tree expansion which applies when the full tree is too big\n(possibly in- finite) to be entirely represented and show that with high\nprobability, only the optimal branches are indefinitely developed. We illus-\ntrate these methods on a global optimization problem of a continuous function,\ngiven noisy values.",
         "225"
        ],
        [
         "40034",
         "abs-1408.2032v1",
         "Bayesian Multitask Learning with Latent Hierarchies",
         "Machine Learning",
         "cs.LG",
         "2014-08-09 00:00:00",
         "2014-08-09 00:00:00",
         "['Hal Daume III']",
         "'Hal Daume III'",
         "We learn multiple hypotheses for related tasks under a latent hierarchical\nrelationship between tasks. We exploit the intuition that for domain\nadaptation, we wish to share classifier structure, but for multitask learning,\nwe wish to share covariance structure. Our hierarchical model is seen to\nsubsume several previously proposed multitask learning models and performs well\non three distinct real-world data sets.",
         "60"
        ],
        [
         "24680",
         "abs-0907.0783v1",
         "Bayesian Multitask Learning with Latent Hierarchies",
         "Machine Learning",
         "cs.LG",
         "2009-07-04 00:00:00",
         "2009-07-04 00:00:00",
         "['Hal DaumÃ© III']",
         "'Hal DaumÃ© III'",
         "We learn multiple hypotheses for related tasks under a latent hierarchical\nrelationship between tasks. We exploit the intuition that for domain\nadaptation, we wish to share classifier structure, but for multitask learning,\nwe wish to share covariance structure. Our hierarchical model is seen to\nsubsume several previously proposed multitask learning models and performs well\non three distinct real-world data sets.",
         "60"
        ],
        [
         "98307",
         "abs-2006.01490v2",
         "Bayesian Neural Networks",
         "Machine Learning (Statistics)",
         "stat.ML",
         "2020-06-02 00:00:00",
         "2020-11-06 00:00:00",
         "['Tom Charnock', 'Laurence Perreault-Levasseur', 'FranÃ§ois Lanusse']",
         "'Tom Charnock'",
         "In recent times, neural networks have become a powerful tool for the analysis\nof complex and abstract data models. However, their introduction intrinsically\nincreases our uncertainty about which features of the analysis are\nmodel-related and which are due to the neural network. This means that\npredictions by neural networks have biases which cannot be trivially\ndistinguished from being due to the true nature of the creation and observation\nof data or not. In order to attempt to address such issues we discuss Bayesian\nneural networks: neural networks where the uncertainty due to the network can\nbe characterised. In particular, we present the Bayesian statistical framework\nwhich allows us to categorise uncertainty in terms of the ingrained randomness\nof observing certain data and the uncertainty from our lack of knowledge about\nhow data can be created and observed. In presenting such techniques we show how\nerrors in prediction by neural networks can be obtained in principle, and\nprovide the two favoured methods for characterising these errors. We will also\ndescribe how both of these methods have substantial pitfalls when put into\npractice, highlighting the need for other statistical techniques to truly be\nable to do inference when using neural networks.",
         "199"
        ],
        [
         "98124",
         "abs-1801.07710v2",
         "Bayesian Neural Networks",
         "Machine Learning",
         "cs.LG",
         "2018-01-23 00:00:00",
         "2018-01-30 00:00:00",
         "['Vikram Mullachery', 'Aniruddh Khera', 'Amir Husain']",
         "'Vikram Mullachery'",
         "This paper describes and discusses Bayesian Neural Network (BNN). The paper\nshowcases a few different applications of them for classification and\nregression problems. BNNs are comprised of a Probabilistic Model and a Neural\nNetwork. The intent of such a design is to combine the strengths of Neural\nNetworks and Stochastic modeling. Neural Networks exhibit continuous function\napproximator capabilities. Stochastic models allow direct specification of a\nmodel with known interaction between parameters to generate data. During the\nprediction phase, stochastic models generate a complete posterior distribution\nand produce probabilistic guarantees on the predictions. Thus BNNs are a unique\ncombination of neural network and stochastic models with the stochastic model\nforming the core of this integration. BNNs can then produce probabilistic\nguarantees on it's predictions and also generate the distribution of parameters\nthat it has learnt from the observations. That means, in the parameter space,\none can deduce the nature and shape of the neural network's learnt parameters.\nThese two characteristics makes them highly attractive to theoreticians as well\nas practitioners. Recently there has been a lot of activity in this area, with\nthe advent of numerous probabilistic programming libraries such as: PyMC3,\nEdward, Stan etc. Further this area is rapidly gaining ground as a standard\nmachine learning approach for numerous problems",
         "210"
        ],
        [
         "76639",
         "abs-1206.1088v2",
         "Bayesian Structure Learning for Markov Random Fields with a Spike and\n  Slab Prior",
         "Machine Learning (Statistics)",
         "stat.ML",
         "2012-06-05 00:00:00",
         "2012-06-23 00:00:00",
         "['Yutian Chen', 'Max Welling']",
         "'Yutian Chen'",
         "In recent years a number of methods have been developed for automatically\nlearning the (sparse) connectivity structure of Markov Random Fields. These\nmethods are mostly based on L1-regularized optimization which has a number of\ndisadvantages such as the inability to assess model uncertainty and expensive\ncross-validation to find the optimal regularization parameter. Moreover, the\nmodel's predictive performance may degrade dramatically with a suboptimal value\nof the regularization parameter (which is sometimes desirable to induce\nsparseness). We propose a fully Bayesian approach based on a \"spike and slab\"\nprior (similar to L0 regularization) that does not suffer from these\nshortcomings. We develop an approximate MCMC method combining Langevin dynamics\nand reversible jump MCMC to conduct inference in this model. Experiments show\nthat the proposed model learns a good combination of the structure and\nparameter values without the need for separate hyper-parameter tuning.\nMoreover, the model's predictive performance is much more robust than L1-based\nmethods with hyper-parameter settings that induce highly sparse model\nstructures.",
         "162"
        ],
        [
         "40044",
         "abs-1408.2047v1",
         "Bayesian Structure Learning for Markov Random Fields with a Spike and\n  Slab Prior",
         "Machine Learning",
         "cs.LG",
         "2014-08-09 00:00:00",
         "2014-08-09 00:00:00",
         "['Yutian Chen', 'Max Welling']",
         "'Yutian Chen'",
         "In recent years a number of methods have been developed for automatically\nlearning the (sparse) connectivity structure of Markov Random Fields. These\nmethods are mostly based on L1-regularized optimization which has a number of\ndisadvantages such as the inability to assess model uncertainty and expensive\ncrossvalidation to find the optimal regularization parameter. Moreover, the\nmodel's predictive performance may degrade dramatically with a suboptimal value\nof the regularization parameter (which is sometimes desirable to induce\nsparseness). We propose a fully Bayesian approach based on a \"spike and slab\"\nprior (similar to L0 regularization) that does not suffer from these\nshortcomings. We develop an approximate MCMC method combining Langevin dynamics\nand reversible jump MCMC to conduct inference in this model. Experiments show\nthat the proposed model learns a good combination of the structure and\nparameter values without the need for separate hyper-parameter tuning.\nMoreover, the model's predictive performance is much more robust than L1-based\nmethods with hyper-parameter settings that induce highly sparse model\nstructures.",
         "162"
        ],
        [
         "84438",
         "abs-1909.11720v1",
         "Benefit of Interpolation in Nearest Neighbor Algorithms",
         "Machine Learning (Statistics)",
         "stat.ML",
         "2019-09-25 00:00:00",
         "2019-09-25 00:00:00",
         "['Yue Xing', 'Qifan Song', 'Guang Cheng']",
         "'Yue Xing'",
         "The over-parameterized models attract much attention in the era of data\nscience and deep learning. It is empirically observed that although these\nmodels, e.g. deep neural networks, over-fit the training data, they can still\nachieve small testing error, and sometimes even {\\em outperform} traditional\nalgorithms which are designed to avoid over-fitting. The major goal of this\nwork is to sharply quantify the benefit of data interpolation in the context of\nnearest neighbors (NN) algorithm. Specifically, we consider a class of\ninterpolated weighting schemes and then carefully characterize their asymptotic\nperformances. Our analysis reveals a U-shaped performance curve with respect to\nthe level of data interpolation, and proves that a mild degree of data\ninterpolation {\\em strictly} improves the prediction accuracy and statistical\nstability over those of the (un-interpolated) optimal $k$NN algorithm. This\ntheoretically justifies (predicts) the existence of the second U-shaped curve\nin the recently discovered double descent phenomenon. Note that our goal in\nthis study is not to promote the use of interpolated-NN method, but to obtain\ntheoretical insights on data interpolation inspired by the aforementioned\nphenomenon.",
         "178"
        ],
        [
         "86256",
         "abs-2202.11817v1",
         "Benefit of Interpolation in Nearest Neighbor Algorithms",
         "Machine Learning (Statistics)",
         "stat.ML",
         "2022-02-23 00:00:00",
         "2022-02-23 00:00:00",
         "['Yue Xing', 'Qifan Song', 'Guang Cheng']",
         "'Yue Xing'",
         "In some studies \\citep[e.g.,][]{zhang2016understanding} of deep learning, it\nis observed that over-parametrized deep neural networks achieve a small testing\nerror even when the training error is almost zero. Despite numerous works\ntowards understanding this so-called \"double descent\" phenomenon\n\\citep[e.g.,][]{belkin2018reconciling,belkin2019two}, in this paper, we turn\ninto another way to enforce zero training error (without over-parametrization)\nthrough a data interpolation mechanism. Specifically, we consider a class of\ninterpolated weighting schemes in the nearest neighbors (NN) algorithms. By\ncarefully characterizing the multiplicative constant in the statistical risk,\nwe reveal a U-shaped performance curve for the level of data interpolation in\nboth classification and regression setups. This sharpens the existing result\n\\citep{belkin2018does} that zero training error does not necessarily jeopardize\npredictive performances and claims a counter-intuitive result that a mild\ndegree of data interpolation actually {\\em strictly} improve the prediction\nperformance and statistical stability over those of the (un-interpolated)\n$k$-NN algorithm. In the end, the universality of our results, such as change\nof distance measure and corrupted testing data, will also be discussed.",
         "169"
        ],
        [
         "28106",
         "abs-2011.01168v1",
         "Bilevel Continual Learning",
         "Machine Learning",
         "cs.LG",
         "2020-11-02 00:00:00",
         "2020-11-02 00:00:00",
         "['Ammar Shaker', 'Francesco Alesiani', 'Shujian Yu', 'Wenzhe Yin']",
         "'Ammar Shaker'",
         "Continual learning (CL) studies the problem of learning a sequence of tasks,\none at a time, such that the learning of each new task does not lead to the\ndeterioration in performance on the previously seen ones while exploiting\npreviously learned features. This paper presents Bilevel Continual Learning\n(BiCL), a general framework for continual learning that fuses bilevel\noptimization and recent advances in meta-learning for deep neural networks.\nBiCL is able to train both deep discriminative and generative models under the\nconservative setting of the online continual learning. Experimental results\nshow that BiCL provides competitive performance in terms of accuracy for the\ncurrent task while reducing the effect of catastrophic forgetting. This is a\nconcurrent work with [1]. We submitted it to AAAI 2020 and IJCAI 2020. Now we\nput it on the arxiv for record. Different from [1], we also consider continual\ngenerative model as well. At the same time, the authors are aware of a recent\nproposal on bilevel optimization based coreset construction for continual\nlearning [2].\n  [1] Q. Pham, D. Sahoo, C. Liu, and S. C. Hoi. Bilevel continual learning.\narXiv preprint arXiv:2007.15553, 2020.\n  [2] Z. Borsos, M. Mutny, and A. Krause. Coresets via bilevel optimization for\ncontinual learning and streaming. arXiv preprint arXiv:2006.03875, 2020",
         "208"
        ],
        [
         "88417",
         "abs-2007.15553v1",
         "Bilevel Continual Learning",
         "Machine Learning",
         "cs.LG",
         "2020-07-30 00:00:00",
         "2020-07-30 00:00:00",
         "['Quang Pham', 'Doyen Sahoo', 'Chenghao Liu', 'Steven C. H Hoi']",
         "'Quang Pham'",
         "Continual learning aims to learn continuously from a stream of tasks and data\nin an online-learning fashion, being capable of exploiting what was learned\npreviously to improve current and future tasks while still being able to\nperform well on the previous tasks. One common limitation of many existing\ncontinual learning methods is that they often train a model directly on all\navailable training data without validation due to the nature of continual\nlearning, thus suffering poor generalization at test time. In this work, we\npresent a novel framework of continual learning named \"Bilevel Continual\nLearning\" (BCL) by unifying a {\\it bilevel optimization} objective and a {\\it\ndual memory management} strategy comprising both episodic memory and\ngeneralization memory to achieve effective knowledge transfer to future tasks\nand alleviate catastrophic forgetting on old tasks simultaneously. Our\nextensive experiments on continual learning benchmarks demonstrate the efficacy\nof the proposed BCL compared to many state-of-the-art methods. Our\nimplementation is available at\nhttps://github.com/phquang/bilevel-continual-learning.",
         "158"
        ],
        [
         "51092",
         "abs-1008.1695v1",
         "Biometric Authentication using Nonparametric Methods",
         "Computer Vision and Pattern Recognition",
         "cs.CV",
         "2010-08-10 00:00:00",
         "2010-08-10 00:00:00",
         "['S. V. Sheela', 'K. R. Radhika']",
         "'S. V. Sheela'",
         "The physiological and behavioral trait is employed to develop biometric\nauthentication systems. The proposed work deals with the authentication of iris\nand signature based on minimum variance criteria. The iris patterns are\npreprocessed based on area of the connected components. The segmented image\nused for authentication consists of the region with large variations in the\ngray level values. The image region is split into quadtree components. The\ncomponents with minimum variance are determined from the training samples. Hu\nmoments are applied on the components. The summation of moment values\ncorresponding to minimum variance components are provided as input vector to\nk-means and fuzzy k-means classifiers. The best performance was obtained for\nMMU database consisting of 45 subjects. The number of subjects with zero False\nRejection Rate [FRR] was 44 and number of subjects with zero False Acceptance\nRate [FAR] was 45. This paper addresses the computational load reduction in\noff-line signature verification based on minimal features using k-means, fuzzy\nk-means, k-nn, fuzzy k-nn and novel average-max approaches. FRR of 8.13% and\nFAR of 10% was achieved using k-nn classifier. The signature is a biometric,\nwhere variations in a genuine case, is a natural expectation. In the genuine\nsignature, certain parts of signature vary from one instance to another. The\nsystem aims to provide simple, fast and robust system using less number of\nfeatures when compared to state of art works.",
         "230"
        ],
        [
         "51052",
         "abs-1006.1187v1",
         "Biometric Authentication using Nonparametric Methods",
         "Computer Vision and Pattern Recognition",
         "cs.CV",
         "2010-06-07 00:00:00",
         "2010-06-07 00:00:00",
         "['S. V. Sheela', 'K. R. Radhika']",
         "'S. V. Sheela'",
         "The physiological and behavioral trait is employed to develop biometric\nauthentication systems. The proposed work deals with the authentication of iris\nand signature based on minimum variance criteria. The iris patterns are\npreprocessed based on area of the connected components. The segmented image\nused for authentication consists of the region with large variations in the\ngray level values. The image region is split into quadtree components. The\ncomponents with minimum variance are determined from the training samples. Hu\nmoments are applied on the components. The summation of moment values\ncorresponding to minimum variance components are provided as input vector to\nk-means and fuzzy kmeans classifiers. The best performance was obtained for MMU\ndatabase consisting of 45 subjects. The number of subjects with zero False\nRejection Rate [FRR] was 44 and number of subjects with zero False Acceptance\nRate [FAR] was 45. This paper addresses the computational load reduction in\noff-line signature verification based on minimal features using k-means, fuzzy\nk-means, k-nn, fuzzy k-nn and novel average-max approaches. FRR of 8.13% and\nFAR of 10% was achieved using k-nn classifier. The signature is a biometric,\nwhere variations in a genuine case, is a natural expectation. In the genuine\nsignature, certain parts of signature vary from one instance to another. The\nsystem aims to provide simple, fast and robust system using less number of\nfeatures when compared to state of art works.",
         "230"
        ],
        [
         "9902",
         "abs-0903.1146v1",
         "Breaking Value Symmetry",
         "Artificial Intelligence",
         "cs.AI",
         "2009-03-06 00:00:00",
         "2009-03-06 00:00:00",
         "['Toby Walsh']",
         "'Toby Walsh'",
         "One common type of symmetry is when values are symmetric. For example, if we\nare assigning colours (values) to nodes (variables) in a graph colouring\nproblem then we can uniformly interchange the colours throughout a colouring.\nFor a problem with value symmetries, all symmetric solutions can be eliminated\nin polynomial time. However, as we show here, both static and dynamic methods\nto deal with symmetry have computational limitations. With static methods,\npruning all symmetric values is NP-hard in general. With dynamic methods, we\ncan take exponential time on problems which static methods solve without\nsearch.",
         "95"
        ],
        [
         "480",
         "abs-0903.0465v1",
         "Breaking Value Symmetry",
         "Artificial Intelligence",
         "cs.AI",
         "2009-03-03 00:00:00",
         "2009-03-03 00:00:00",
         "['Toby Walsh']",
         "'Toby Walsh'",
         "Symmetry is an important factor in solving many constraint satisfaction\nproblems. One common type of symmetry is when we have symmetric values. In a\nrecent series of papers, we have studied methods to break value symmetries. Our\nresults identify computational limits on eliminating value symmetry. For\ninstance, we prove that pruning all symmetric values is NP-hard in general.\nNevertheless, experiments show that much value symmetry can be broken in\npractice. These results may be useful to researchers in planning, scheduling\nand other areas as value symmetry occurs in many different domains.",
         "91"
        ],
        [
         "88894",
         "abs-2106.02613v4",
         "Bridging the Gap Between Target Networks and Functional Regularization",
         "Machine Learning (Statistics)",
         "stat.ML",
         "2021-06-04 00:00:00",
         "2023-09-07 00:00:00",
         "['Alexandre PichÃ©', 'Valentin Thomas', 'Rafael Pardinas', 'Joseph Marino', 'Gian Maria Marconi', 'Christopher Pal', 'Mohammad Emtiyaz Khan']",
         "'Alexandre PichÃ©'",
         "Bootstrapping is behind much of the successes of deep Reinforcement Learning.\nHowever, learning the value function via bootstrapping often leads to unstable\ntraining due to fast-changing target values. Target Networks are employed to\nstabilize training by using an additional set of lagging parameters to estimate\nthe target values. Despite the popularity of Target Networks, their effect on\nthe optimization is still misunderstood. In this work, we show that they act as\nan implicit regularizer which can be beneficial in some cases, but also have\ndisadvantages such as being inflexible and can result in instabilities, even\nwhen vanilla TD(0) converges. To overcome these issues, we propose an explicit\nFunctional Regularization alternative that is flexible and a convex regularizer\nin function space and we theoretically study its convergence. We conduct an\nexperimental study across a range of environments, discount factors, and\noff-policiness data collections to investigate the effectiveness of the\nregularization induced by Target Networks and Functional Regularization in\nterms of performance, accuracy, and stability. Our findings emphasize that\nFunctional Regularization can be used as a drop-in replacement for Target\nNetworks and result in performance improvement. Furthermore, adjusting both the\nregularization weight and the network update period in Functional\nRegularization can result in further performance improvements compared to\nsolely adjusting the network update period as typically done with Target\nNetworks. Our approach also enhances the ability to networks to recover\naccurate $Q$-values.",
         "230"
        ],
        [
         "33142",
         "abs-2210.12282v2",
         "Bridging the Gap Between Target Networks and Functional Regularization",
         "Machine Learning",
         "cs.LG",
         "2022-10-21 00:00:00",
         "2024-01-03 00:00:00",
         "['Alexandre Piche', 'Valentin Thomas', 'Joseph Marino', 'Rafael Pardinas', 'Gian Maria Marconi', 'Christopher Pal', 'Mohammad Emtiyaz Khan']",
         "'Alexandre Piche'",
         "Bootstrapping is behind much of the successes of Deep Reinforcement Learning.\nHowever, learning the value function via bootstrapping often leads to unstable\ntraining due to fast-changing target values. Target Networks are employed to\nstabilize training by using an additional set of lagging parameters to estimate\nthe target values. Despite the popularity of Target Networks, their effect on\nthe optimization is still misunderstood. In this work, we show that they act as\nan implicit regularizer. This regularizer has disadvantages such as being\ninflexible and non convex. To overcome these issues, we propose an explicit\nFunctional Regularization that is a convex regularizer in function space and\ncan easily be tuned. We analyze the convergence of our method theoretically and\nempirically demonstrate that replacing Target Networks with the more\ntheoretically grounded Functional Regularization approach leads to better\nsample efficiency and performance improvements.",
         "139"
        ],
        [
         "107047",
         "abs-1412.3949v1",
         "CITlab ARGUS for historical handwritten documents",
         "Computer Vision and Pattern Recognition",
         "cs.CV",
         "2014-12-12 00:00:00",
         "2014-12-12 00:00:00",
         "['Tobias StrauÃ', 'Tobias GrÃ¼ning', 'Gundram Leifert', 'Roger Labahn']",
         "'Tobias StrauÃ'",
         "We describe CITlab's recognition system for the HTRtS competition attached to\nthe 14. International Conference on Frontiers in Handwriting Recognition, ICFHR\n2014. The task comprises the recognition of historical handwritten documents.\nThe core algorithms of our system are based on multi-dimensional recurrent\nneural networks (MDRNN) and connectionist temporal classification (CTC). The\nsoftware modules behind that as well as the basic utility technologies are\nessentially powered by PLANET's ARGUS framework for intelligent text\nrecognition and image processing.",
         "76"
        ],
        [
         "109260",
         "abs-1605.08412v1",
         "CITlab ARGUS for historical handwritten documents",
         "Computer Vision and Pattern Recognition",
         "cs.CV",
         "2016-05-26 00:00:00",
         "2016-05-26 00:00:00",
         "['Gundram Leifert', 'Tobias StrauÃ', 'Tobias GrÃ¼ning', 'Roger Labahn']",
         "'Gundram Leifert'",
         "We describe CITlab's recognition system for the HTRtS competition attached to\nthe 13. International Conference on Document Analysis and Recognition, ICDAR\n2015. The task comprises the recognition of historical handwritten documents.\nThe core algorithms of our system are based on multi-dimensional recurrent\nneural networks (MDRNN) and connectionist temporal classification (CTC). The\nsoftware modules behind that as well as the basic utility technologies are\nessentially powered by PLANET's ARGUS framework for intelligent text\nrecognition and image processing.",
         "76"
        ],
        [
         "117403",
         "abs-1511.04586v1",
         "Character-based Neural Machine Translation",
         "Computation and Language (Natural Language Processing)",
         "cs.CL",
         "2015-11-14 00:00:00",
         "2015-11-14 00:00:00",
         "['Wang Ling', 'Isabel Trancoso', 'Chris Dyer', 'Alan W Black']",
         "'Wang Ling'",
         "We introduce a neural machine translation model that views the input and\noutput sentences as sequences of characters rather than words. Since word-level\ninformation provides a crucial source of bias, our input model composes\nrepresentations of character sequences into representations of words (as\ndetermined by whitespace boundaries), and then these are translated using a\njoint attention/translation model. In the target language, the translation is\nmodeled as a sequence of word vectors, but each word is generated one character\nat a time, conditional on the previous character generations in each word. As\nthe representation and generation of words is performed at the character level,\nour model is capable of interpreting and generating unseen word forms. A\nsecondary benefit of this approach is that it alleviates much of the challenges\nassociated with preprocessing/tokenization of the source and target languages.\nWe show that our model can achieve translation results that are on par with\nconventional word-based models.",
         "154"
        ],
        [
         "109183",
         "abs-1603.00810v3",
         "Character-based Neural Machine Translation",
         "Computation and Language (Natural Language Processing)",
         "cs.CL",
         "2016-03-02 00:00:00",
         "2016-06-30 00:00:00",
         "['Marta R. Costa-JussÃ ', 'JosÃ© A. R. Fonollosa']",
         "'Marta R. Costa-JussÃ '",
         "Neural Machine Translation (MT) has reached state-of-the-art results.\nHowever, one of the main challenges that neural MT still faces is dealing with\nvery large vocabularies and morphologically rich languages. In this paper, we\npropose a neural MT system using character-based embeddings in combination with\nconvolutional and highway layers to replace the standard lookup-based word\nrepresentations. The resulting unlimited-vocabulary and affix-aware source word\nembeddings are tested in a state-of-the-art neural MT based on an\nattention-based bidirectional recurrent neural network. The proposed MT scheme\nprovides improved results even when the source language is not morphologically\nrich. Improvements up to 3 BLEU points are obtained in the German-English WMT\ntask.",
         "107"
        ],
        [
         "94020",
         "abs-1004.5194v1",
         "Clustering processes",
         "Machine Learning",
         "cs.LG",
         "2010-04-29 00:00:00",
         "2010-04-29 00:00:00",
         "['Daniil Ryabko']",
         "'Daniil Ryabko'",
         "The problem of clustering is considered, for the case when each data point is\na sample generated by a stationary ergodic process. We propose a very natural\nasymptotic notion of consistency, and show that simple consistent algorithms\nexist, under most general non-parametric assumptions. The notion of consistency\nis as follows: two samples should be put into the same cluster if and only if\nthey were generated by the same distribution. With this notion of consistency,\nclustering generalizes such classical statistical problems as homogeneity\ntesting and process classification. We show that, for the case of a known\nnumber of clusters, consistency can be achieved under the only assumption that\nthe joint distribution of the data is stationary ergodic (no parametric or\nMarkovian assumptions, no assumptions of independence, neither between nor\nwithin the samples). If the number of clusters is unknown, consistency can be\nachieved under appropriate assumptions on the mixing rates of the processes.\n(again, no parametric or independence assumptions). In both cases we give\nexamples of simple (at most quadratic in each argument) algorithms which are\nconsistent.",
         "177"
        ],
        [
         "94026",
         "abs-1005.0826v2",
         "Clustering processes",
         "Machine Learning",
         "cs.LG",
         "2010-05-05 00:00:00",
         "2013-04-30 00:00:00",
         "['Daniil Ryabko']",
         "'Daniil Ryabko'",
         "The problem of clustering is considered, for the case when each data point is\na sample generated by a stationary ergodic process. We propose a very natural\nasymptotic notion of consistency, and show that simple consistent algorithms\nexist, under most general non-parametric assumptions. The notion of consistency\nis as follows: two samples should be put into the same cluster if and only if\nthey were generated by the same distribution. With this notion of consistency,\nclustering generalizes such classical statistical problems as homogeneity\ntesting and process classification. We show that, for the case of a known\nnumber of clusters, consistency can be achieved under the only assumption that\nthe joint distribution of the data is stationary ergodic (no parametric or\nMarkovian assumptions, no assumptions of independence, neither between nor\nwithin the samples). If the number of clusters is unknown, consistency can be\nachieved under appropriate assumptions on the mixing rates of the processes.\n(again, no parametric or independence assumptions). In both cases we give\nexamples of simple (at most quadratic in each argument) algorithms which are\nconsistent.",
         "177"
        ],
        [
         "77882",
         "abs-1606.05896v1",
         "Clustering with a Reject Option: Interactive Clustering as Bayesian\n  Prior Elicitation",
         "Machine Learning (Statistics)",
         "stat.ML",
         "2016-06-19 00:00:00",
         "2016-06-19 00:00:00",
         "['Akash Srivastava', 'James Zou', 'Ryan P. Adams', 'Charles Sutton']",
         "'Akash Srivastava'",
         "A good clustering can help a data analyst to explore and understand a data\nset, but what constitutes a good clustering may depend on domain-specific and\napplication-specific criteria. These criteria can be difficult to formalize,\neven when it is easy for an analyst to know a good clustering when they see\none. We present a new approach to interactive clustering for data exploration\ncalled TINDER, based on a particularly simple feedback mechanism, in which an\nanalyst can reject a given clustering and request a new one, which is chosen to\nbe different from the previous clustering while fitting the data well. We\nformalize this interaction in a Bayesian framework as a method for prior\nelicitation, in which each different clustering is produced by a prior\ndistribution that is modified to discourage previously rejected clusterings. We\nshow that TINDER successfully produces a diverse set of clusterings, each of\nequivalent quality, that are much more diverse than would be obtained by\nrandomized restarts.",
         "161"
        ],
        [
         "40441",
         "abs-1602.06886v2",
         "Clustering with a Reject Option: Interactive Clustering as Bayesian\n  Prior Elicitation",
         "Machine Learning (Statistics)",
         "stat.ML",
         "2016-02-22 00:00:00",
         "2016-06-08 00:00:00",
         "['Akash Srivastava', 'James Zou', 'Charles Sutton']",
         "'Akash Srivastava'",
         "A good clustering can help a data analyst to explore and understand a data\nset, but what constitutes a good clustering may depend on domain-specific and\napplication-specific criteria. These criteria can be difficult to formalize,\neven when it is easy for an analyst to know a good clustering when she sees\none. We present a new approach to interactive clustering for data exploration,\ncalled \\ciif, based on a particularly simple feedback mechanism, in which an\nanalyst can choose to reject individual clusters and request new ones. The new\nclusters should be different from previously rejected clusters while still\nfitting the data well. We formalize this interaction in a novel Bayesian prior\nelicitation framework. In each iteration, the prior is adapted to account for\nall the previous feedback, and a new clustering is then produced from the\nposterior distribution. To achieve the computational efficiency necessary for\nan interactive setting, we propose an incremental optimization method over data\nminibatches using Lagrangian relaxation. Experiments demonstrate that \\ciif can\nproduce accurate and diverse clusterings.",
         "170"
        ],
        [
         "141",
         "cs-0005031v3",
         "Conditional Plausibility Measures and Bayesian Networks",
         "Artificial Intelligence",
         "cs.AI",
         "2000-05-30 00:00:00",
         "2011-06-15 00:00:00",
         "['Joseph Y. Halpern']",
         "'Joseph Y. Halpern'",
         "A general notion of algebraic conditional plausibility measures is defined.\nProbability measures, ranking functions, possibility measures, and (under the\nappropriate definitions) sets of probability measures can all be viewed as\ndefining algebraic conditional plausibility measures. It is shown that\nalgebraic conditional plausibility measures can be represented using Bayesian\nnetworks.",
         "49"
        ],
        [
         "2443",
         "abs-1407.7182v1",
         "Conditional Plausibility Measures and Bayesian Networks",
         "Artificial Intelligence",
         "cs.AI",
         "2014-07-27 00:00:00",
         "2014-07-27 00:00:00",
         "['Joseph Y. Halpern']",
         "'Joseph Y. Halpern'",
         "A general notion of algebraic conditional plausibility measures is defined.\nProbability measures, ranking functions, possibility measures, and (under the\nappropriate definitions) sets of probability measures can all be viewed as\ndefining algebraic conditional plausibility measures. It is shown that the\ntechnology of Bayesian networks can be applied to algebraic conditional\nplausibility measures.",
         "52"
        ],
        [
         "40033",
         "abs-1408.2031v1",
         "Conditional Probability Tree Estimation Analysis and Algorithms",
         "Machine Learning",
         "cs.LG",
         "2014-08-09 00:00:00",
         "2014-08-09 00:00:00",
         "['Alina Beygelzimer', 'John Langford', 'Yuri Lifshits', 'Gregory Sorkin', 'Alexander L. Strehl']",
         "'Alina Beygelzimer'",
         "We consider the problem of estimating the conditional probability of a label\nin time O(log n), where n is the number of possible labels. We analyze a\nnatural reduction of this problem to a set of binary regression problems\norganized in a tree structure, proving a regret bound that scales with the\ndepth of the tree. Motivated by this analysis, we propose the first online\nalgorithm which provably constructs a logarithmic depth tree on the set of\nlabels to solve this problem. We test the algorithm empirically, showing that\nit works succesfully on a dataset with roughly 106 labels.",
         "99"
        ],
        [
         "9908",
         "abs-0903.4217v2",
         "Conditional Probability Tree Estimation Analysis and Algorithms",
         "Machine Learning",
         "cs.LG",
         "2009-03-25 00:00:00",
         "2009-06-03 00:00:00",
         "['Alina Beygelzimer', 'John Langford', 'Yuri Lifshits', 'Gregory Sorkin', 'Alex Strehl']",
         "'Alina Beygelzimer'",
         "We consider the problem of estimating the conditional probability of a label\nin time $O(\\log n)$, where $n$ is the number of possible labels. We analyze a\nnatural reduction of this problem to a set of binary regression problems\norganized in a tree structure, proving a regret bound that scales with the\ndepth of the tree. Motivated by this analysis, we propose the first online\nalgorithm which provably constructs a logarithmic depth tree on the set of\nlabels to solve this problem. We test the algorithm empirically, showing that\nit works succesfully on a dataset with roughly $10^6$ labels.",
         "99"
        ],
        [
         "7517",
         "abs-2307.12289v4",
         "Controller Synthesis for Timeline-based Games",
         "Artificial Intelligence",
         "cs.AI",
         "2023-07-23 00:00:00",
         "2024-08-26 00:00:00",
         "['Renato Acampora', 'Luca Geatti', 'Nicola Gigante', 'Angelo Montanari', 'Valentino Picotti']",
         "'Renato Acampora'",
         "In the timeline-based approach to planning, the evolution over time of a set\nof state variables (the timelines) is governed by a set of temporal\nconstraints. Traditional timeline-based planning systems excel at the\nintegration of planning with execution by handling temporal uncertainty. In\norder to handle general nondeterminism as well, the concept of timeline-based\ngames has been recently introduced. It has been proved that finding whether a\nwinning strategy exists for such games is 2EXPTIME-complete. However, a\nconcrete approach to synthesize controllers implementing such strategies is\nmissing. This paper fills this gap, by providing an effective and\ncomputationally optimal approach to controller synthesis for timeline-based\ngames.",
         "106"
        ],
        [
         "6739",
         "abs-2209.10319v1",
         "Controller Synthesis for Timeline-based Games",
         "Artificial Intelligence",
         "cs.AI",
         "2022-09-21 00:00:00",
         "2022-09-21 00:00:00",
         "['Renato Acampora', 'Luca Geatti', 'Nicola Gigante', 'Angelo Montanari', 'Valentino Picotti']",
         "'Renato Acampora'",
         "In the timeline-based approach to planning, originally born in the space\nsector, the evolution over time of a set of state variables (the timelines) is\ngoverned by a set of temporal constraints. Traditional timeline-based planning\nsystems excel at the integration of planning with execution by handling\ntemporal uncertainty. In order to handle general nondeterminism as well, the\nconcept of timeline-based games has been recently introduced. It has been\nproved that finding whether a winning strategy exists for such games is\n2EXPTIME-complete. However, a concrete approach to synthesize controllers\nimplementing such strategies is missing. This paper fills this gap, outlining\nan approach to controller synthesis for timeline-based games.",
         "107"
        ],
        [
         "27929",
         "abs-1610.04900v2",
         "Convergence rate of stochastic k-means",
         "Machine Learning",
         "cs.LG",
         "2016-10-16 00:00:00",
         "2016-11-07 00:00:00",
         "['Cheng Tang', 'Claire Monteleoni']",
         "'Cheng Tang'",
         "We analyze online and mini-batch k-means variants. Both scale up the widely\nused Lloyd 's algorithm via stochastic approximation, and have become popular\nfor large-scale clustering and unsupervised feature learning. We show, for the\nfirst time, that they have global convergence towards local optima at\n$O(\\frac{1}{t})$ rate under general conditions. In addition, we show if the\ndataset is clusterable, with suitable initialization, mini-batch k-means\nconverges to an optimal k-means solution with $O(\\frac{1}{t})$ convergence rate\nwith high probability. The k-means objective is non-convex and\nnon-differentiable: we exploit ideas from non-convex gradient-based\noptimization by providing a novel characterization of the trajectory of k-means\nalgorithm on its solution space, and circumvent its non-differentiability via\ngeometric insights about k-means update.",
         "116"
        ],
        [
         "25745",
         "abs-1611.05132v1",
         "Convergence rate of stochastic k-means",
         "Machine Learning",
         "cs.LG",
         "2016-11-16 00:00:00",
         "2016-11-16 00:00:00",
         "['Cheng Tang', 'Claire Monteleoni']",
         "'Cheng Tang'",
         "We analyze online \\cite{BottouBengio} and mini-batch \\cite{Sculley} $k$-means\nvariants. Both scale up the widely used $k$-means algorithm via stochastic\napproximation, and have become popular for large-scale clustering and\nunsupervised feature learning. We show, for the first time, that starting with\nany initial solution, they converge to a \"local optimum\" at rate\n$O(\\frac{1}{t})$ (in terms of the $k$-means objective) under general\nconditions. In addition, we show if the dataset is clusterable, when\ninitialized with a simple and scalable seeding algorithm, mini-batch $k$-means\nconverges to an optimal $k$-means solution at rate $O(\\frac{1}{t})$ with high\nprobability. The $k$-means objective is non-convex and non-differentiable: we\nexploit ideas from recent work on stochastic gradient descent for non-convex\nproblems \\cite{ge:sgd_tensor, balsubramani13} by providing a novel\ncharacterization of the trajectory of $k$-means algorithm on its solution\nspace, and circumvent the non-differentiability problem via geometric insights\nabout $k$-means update.",
         "141"
        ],
        [
         "92952",
         "abs-2408.09583v1",
         "Convolutional Conditional Neural Processes",
         "Machine Learning (Statistics)",
         "stat.ML",
         "2024-08-18 00:00:00",
         "2024-08-18 00:00:00",
         "['Wessel P. Bruinsma']",
         "'Wessel P. Bruinsma'",
         "Neural processes are a family of models which use neural networks to directly\nparametrise a map from data sets to predictions. Directly parametrising this\nmap enables the use of expressive neural networks in small-data problems where\nneural networks would traditionally overfit. Neural processes can produce\nwell-calibrated uncertainties, effectively deal with missing data, and are\nsimple to train. These properties make this family of models appealing for a\nbreadth of applications areas, such as healthcare or environmental sciences.\n  This thesis advances neural processes in three ways.\n  First, we propose convolutional neural processes (ConvNPs). ConvNPs improve\ndata efficiency of neural processes by building in a symmetry called\ntranslation equivariance. ConvNPs rely on convolutional neural networks rather\nthan multi-layer perceptrons.\n  Second, we propose Gaussian neural processes (GNPs). GNPs directly\nparametrise dependencies in the predictions of a neural process. Current\napproaches to modelling dependencies in the predictions depend on a latent\nvariable, which consequently requires approximate inference, undermining the\nsimplicity of the approach.\n  Third, we propose autoregressive conditional neural processes (AR CNPs). AR\nCNPs train a neural process without any modifications to the model or training\nprocedure and, at test time, roll out the model in an autoregressive fashion.\nAR CNPs equip the neural process framework with a new knob where modelling\ncomplexity and computational expense at training time can be traded for\ncomputational expense at test time.\n  In addition to methodological advancements, this thesis also proposes a\nsoftware abstraction that enables a compositional approach to implementing\nneural processes. This approach allows the user to rapidly explore the space of\nneural process models by putting together elementary building blocks in\ndifferent ways.",
         "268"
        ],
        [
         "43973",
         "abs-1910.13556v5",
         "Convolutional Conditional Neural Processes",
         "Machine Learning (Statistics)",
         "stat.ML",
         "2019-10-29 00:00:00",
         "2020-06-25 00:00:00",
         "['Jonathan Gordon', 'Wessel P. Bruinsma', 'Andrew Y. K. Foong', 'James Requeima', 'Yann Dubois', 'Richard E. Turner']",
         "'Jonathan Gordon'",
         "We introduce the Convolutional Conditional Neural Process (ConvCNP), a new\nmember of the Neural Process family that models translation equivariance in the\ndata. Translation equivariance is an important inductive bias for many learning\nproblems including time series modelling, spatial data, and images. The model\nembeds data sets into an infinite-dimensional function space as opposed to a\nfinite-dimensional vector space. To formalize this notion, we extend the theory\nof neural representations of sets to include functional representations, and\ndemonstrate that any translation-equivariant embedding can be represented using\na convolutional deep set. We evaluate ConvCNPs in several settings,\ndemonstrating that they achieve state-of-the-art performance compared to\nexisting NPs. We demonstrate that building in translation equivariance enables\nzero-shot generalization to challenging, out-of-domain tasks.",
         "120"
        ],
        [
         "67",
         "cs-9609102v1",
         "Cue Phrase Classification Using Machine Learning",
         "Artificial Intelligence",
         "cs.AI",
         "1996-09-01 00:00:00",
         "1996-09-01 00:00:00",
         "['D. J. Litman']",
         "'D. J. Litman'",
         "Cue phrases may be used in a discourse sense to explicitly signal discourse\nstructure, but also in a sentential sense to convey semantic rather than\nstructural information. Correctly classifying cue phrases as discourse or\nsentential is critical in natural language processing systems that exploit\ndiscourse structure, e.g., for performing tasks such as anaphora resolution and\nplan recognition. This paper explores the use of machine learning for\nclassifying cue phrases as discourse or sentential. Two machine learning\nprograms (Cgrendel and C4.5) are used to induce classification models from sets\nof pre-classified cue phrases and their features in text and speech. Machine\nlearning is shown to be an effective technique for not only automating the\ngeneration of classification models, but also for improving upon previous\nresults. When compared to manually derived classification models already in the\nliterature, the learned models often perform with higher accuracy and contain\nnew linguistic insights into the data. In addition, the ability to\nautomatically construct classification models makes it easier to comparatively\nanalyze the utility of alternative feature representations of the data.\nFinally, the ease of retraining makes the learning approach more scalable and\nflexible than manual methods.",
         "191"
        ],
        [
         "130648",
         "cmp-lg-9609003v1",
         "Cue Phrase Classification Using Machine Learning",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1996-09-09 00:00:00",
         "1996-09-09 00:00:00",
         "['Diane J. Litman']",
         "'Diane J. Litman'",
         "Cue phrases may be used in a discourse sense to explicitly signal discourse\nstructure, but also in a sentential sense to convey semantic rather than\nstructural information. Correctly classifying cue phrases as discourse or\nsentential is critical in natural language processing systems that exploit\ndiscourse structure, e.g., for performing tasks such as anaphora resolution and\nplan recognition. This paper explores the use of machine learning for\nclassifying cue phrases as discourse or sentential. Two machine learning\nprograms (Cgrendel and C4.5) are used to induce classification models from sets\nof pre-classified cue phrases and their features in text and speech. Machine\nlearning is shown to be an effective technique for not only automating the\ngeneration of classification models, but also for improving upon previous\nresults. When compared to manually derived classification models already in the\nliterature, the learned models often perform with higher accuracy and contain\nnew linguistic insights into the data. In addition, the ability to\nautomatically construct classification models makes it easier to comparatively\nanalyze the utility of alternative feature representations of the data.\nFinally, the ease of retraining makes the learning approach more scalable and\nflexible than manual methods.",
         "191"
        ],
        [
         "84",
         "cs-9707103v1",
         "Defining Relative Likelihood in Partially-Ordered Preferential\n  Structures",
         "Artificial Intelligence",
         "cs.AI",
         "1997-07-01 00:00:00",
         "1997-07-01 00:00:00",
         "['J. Y. Halpern']",
         "'J. Y. Halpern'",
         "Starting with a likelihood or preference order on worlds, we extend it to a\nlikelihood ordering on sets of worlds in a natural way, and examine the\nresulting logic. Lewis earlier considered such a notion of relative likelihood\nin the context of studying counterfactuals, but he assumed a total preference\norder on worlds. Complications arise when examining partial orders that are not\npresent for total orders. There are subtleties involving the exact approach to\nlifting the order on worlds to an order on sets of worlds. In addition, the\naxiomatization of the logic of relative likelihood in the case of partial\norders gives insight into the connection between relative likelihood and\ndefault reasoning.",
         "113"
        ],
        [
         "2442",
         "abs-1407.7180v1",
         "Defining Relative Likelihood in Partially-Ordered Preferential\n  Structures",
         "Artificial Intelligence",
         "cs.AI",
         "2014-07-27 00:00:00",
         "2014-07-27 00:00:00",
         "['Joseph Y. Halpern']",
         "'Joseph Y. Halpern'",
         "Starting with a likelihood or preference order on worlds, we extend it to a\nlikelihood ordering on sets of worlds in a natural way, and examine the\nresulting logic. Lewis (1973) earlier considered such a notion of relative\nlikelihood in the context of studying counterfactuals, but he assumed a total\npreference order on worlds. Complications arise when examining partial orders\nthat are not present for total orders. There are subtleties involving the exact\napproach to lifting the order on worlds to an order on sets of worlds. In\naddition, the axiomatization of the logic of relative likelihood in the case of\npartial orders gives insight into the connection between relative likelihood\nand default reasoning.",
         "114"
        ],
        [
         "59101",
         "abs-1803.02188v2",
         "DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild",
         "Computer Vision and Pattern Recognition",
         "cs.CV",
         "2018-03-05 00:00:00",
         "2018-03-11 00:00:00",
         "['Riza Alp Guler', 'Yuxiang Zhou', 'George Trigeorgis', 'Epameinondas Antonakos', 'Patrick Snape', 'Stefanos Zafeiriou', 'Iasonas Kokkinos']",
         "'Riza Alp Guler'",
         "In this work we use deep learning to establish dense correspondences between\na 3D object model and an image \"in the wild\". We introduce \"DenseReg\", a\nfully-convolutional neural network (F-CNN) that densely regresses at every\nforeground pixel a pair of U-V template coordinates in a single feedforward\npass. To train DenseReg we construct a supervision signal by combining 3D\ndeformable model fitting and 2D landmark annotations. We define the regression\ntask in terms of the intrinsic, U-V coordinates of a 3D deformable model that\nis brought into correspondence with image instances at training time. A host of\nother object-related tasks (e.g. part segmentation, landmark localization) are\nshown to be by-products of this task, and to largely improve thanks to its\nintroduction. We obtain highly-accurate regression results by combining ideas\nfrom semantic segmentation with regression networks, yielding a 'quantized\nregression' architecture that first obtains a quantized estimate of position\nthrough classification, and refines it through regression of the residual. We\nshow that such networks can boost the performance of existing state-of-the-art\nsystems for pose estimation. Firstly, we show that our system can serve as an\ninitialization for Statistical Deformable Models, as well as an element of\ncascaded architectures that jointly localize landmarks and estimate dense\ncorrespondences. We also show that the obtained dense correspondence can act as\na source of 'privileged information' that complements and extends the pure\nlandmark-level annotations, accelerating and improving the training of pose\nestimation networks. We report state-of-the-art performance on the challenging\n300W benchmark for facial landmark localization and on the MPII and LSP\ndatasets for human pose estimation.",
         "262"
        ],
        [
         "54132",
         "abs-1612.01202v2",
         "DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild",
         "Computer Vision and Pattern Recognition",
         "cs.CV",
         "2016-12-04 00:00:00",
         "2017-06-19 00:00:00",
         "['RÄ±za Alp GÃ¼ler', 'George Trigeorgis', 'Epameinondas Antonakos', 'Patrick Snape', 'Stefanos Zafeiriou', 'Iasonas Kokkinos']",
         "'RÄ±za Alp GÃ¼ler'",
         "In this paper we propose to learn a mapping from image pixels into a dense\ntemplate grid through a fully convolutional network. We formulate this task as\na regression problem and train our network by leveraging upon manually\nannotated facial landmarks \"in-the-wild\". We use such landmarks to establish a\ndense correspondence field between a three-dimensional object template and the\ninput image, which then serves as the ground-truth for training our regression\nsystem. We show that we can combine ideas from semantic segmentation with\nregression networks, yielding a highly-accurate \"quantized regression\"\narchitecture.\n  Our system, called DenseReg, allows us to estimate dense image-to-template\ncorrespondences in a fully convolutional manner. As such our network can\nprovide useful correspondence information as a stand-alone system, while when\nused as an initialization for Statistical Deformable Models we obtain landmark\nlocalization results that largely outperform the current state-of-the-art on\nthe challenging 300W benchmark. We thoroughly evaluate our method on a host of\nfacial analysis tasks and also provide qualitative results for dense human body\ncorrespondence. We make our code available at http://alpguler.com/DenseReg.html\nalong with supplementary materials.",
         "179"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 168
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>category_code</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>first_author</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130124</th>\n",
       "      <td>cmp-lg-9407014v1</td>\n",
       "      <td>Abstract Machine for Typed Feature Structures</td>\n",
       "      <td>Computation and Language (Legacy category)</td>\n",
       "      <td>cmp-lg</td>\n",
       "      <td>1994-07-17</td>\n",
       "      <td>1994-07-17</td>\n",
       "      <td>['Shuly Wintner', 'Nissim Francez']</td>\n",
       "      <td>'Shuly Wintner'</td>\n",
       "      <td>This paper describes a first step towards the ...</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130324</th>\n",
       "      <td>cmp-lg-9504009v1</td>\n",
       "      <td>Abstract Machine for Typed Feature Structures</td>\n",
       "      <td>Computation and Language (Legacy category)</td>\n",
       "      <td>cmp-lg</td>\n",
       "      <td>1995-04-13</td>\n",
       "      <td>1995-04-13</td>\n",
       "      <td>['Shuly Wintner', 'Nissim Francez']</td>\n",
       "      <td>'Shuly Wintner'</td>\n",
       "      <td>This paper describes an abstract machine for l...</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>abs-1408.2056v1</td>\n",
       "      <td>Active Sensing as Bayes-Optimal Sequential Dec...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>2014-08-09</td>\n",
       "      <td>2014-08-09</td>\n",
       "      <td>['Sheeraz Ahmad', 'Angela Yu']</td>\n",
       "      <td>'Sheeraz Ahmad'</td>\n",
       "      <td>Sensory inference under conditions of uncertai...</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10523</th>\n",
       "      <td>abs-1305.6650v1</td>\n",
       "      <td>Active Sensing as Bayes-Optimal Sequential Dec...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>2013-05-28</td>\n",
       "      <td>2013-05-28</td>\n",
       "      <td>['Sheeraz Ahmad', 'Angela J. Yu']</td>\n",
       "      <td>'Sheeraz Ahmad'</td>\n",
       "      <td>Sensory inference under conditions of uncertai...</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>abs-1408.2034v1</td>\n",
       "      <td>Approximate inference on planar graphs using L...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>2014-08-09</td>\n",
       "      <td>2014-08-09</td>\n",
       "      <td>['Vicenc Gomez', 'Hilbert Kappen', 'Michael Ch...</td>\n",
       "      <td>'Vicenc Gomez'</td>\n",
       "      <td>We introduce novel results for approximate inf...</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>abs-1407.7188v1</td>\n",
       "      <td>When Ignorance is Bliss</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>2014-07-27</td>\n",
       "      <td>2014-07-27</td>\n",
       "      <td>['Peter D. Grunwald', 'Joseph Y. Halpern']</td>\n",
       "      <td>'Peter D. Grunwald'</td>\n",
       "      <td>It is commonly-accepted wisdom that more infor...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17549</th>\n",
       "      <td>abs-2206.07940v4</td>\n",
       "      <td>When Rigidity Hurts: Soft Consistency Regulari...</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>2022-06-16</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>['Harshavardhan Kamarthi', 'Lingkai Kong', 'Al...</td>\n",
       "      <td>'Harshavardhan Kamarthi'</td>\n",
       "      <td>Probabilistic hierarchical time-series forecas...</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21739</th>\n",
       "      <td>abs-2310.11569v2</td>\n",
       "      <td>When Rigidity Hurts: Soft Consistency Regulari...</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>2023-10-17</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>['Harshavardhan Kamarthi', 'Lingkai Kong', 'Al...</td>\n",
       "      <td>'Harshavardhan Kamarthi'</td>\n",
       "      <td>Probabilistic hierarchical time-series forecas...</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>abs-1408.1692v1</td>\n",
       "      <td>When do Numbers Really Matter?</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>2014-08-07</td>\n",
       "      <td>2014-08-07</td>\n",
       "      <td>['Hei Chan', 'Adnan Darwiche']</td>\n",
       "      <td>'Hei Chan'</td>\n",
       "      <td>Common wisdom has it that small distinctions i...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>abs-1106.1814v1</td>\n",
       "      <td>When do Numbers Really Matter?</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>2011-06-09</td>\n",
       "      <td>2011-06-09</td>\n",
       "      <td>['H. Chan', 'A. Darwiche']</td>\n",
       "      <td>'H. Chan'</td>\n",
       "      <td>Common wisdom has it that small distinctions i...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows Ã 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                              title  \\\n",
       "130124  cmp-lg-9407014v1      Abstract Machine for Typed Feature Structures   \n",
       "130324  cmp-lg-9504009v1      Abstract Machine for Typed Feature Structures   \n",
       "2472     abs-1408.2056v1  Active Sensing as Bayes-Optimal Sequential Dec...   \n",
       "10523    abs-1305.6650v1  Active Sensing as Bayes-Optimal Sequential Dec...   \n",
       "2467     abs-1408.2034v1  Approximate inference on planar graphs using L...   \n",
       "...                  ...                                                ...   \n",
       "2447     abs-1407.7188v1                            When Ignorance is Bliss   \n",
       "17549   abs-2206.07940v4  When Rigidity Hurts: Soft Consistency Regulari...   \n",
       "21739   abs-2310.11569v2  When Rigidity Hurts: Soft Consistency Regulari...   \n",
       "2463     abs-1408.1692v1                     When do Numbers Really Matter?   \n",
       "722      abs-1106.1814v1                     When do Numbers Really Matter?   \n",
       "\n",
       "                                          category category_code  \\\n",
       "130124  Computation and Language (Legacy category)        cmp-lg   \n",
       "130324  Computation and Language (Legacy category)        cmp-lg   \n",
       "2472                       Artificial Intelligence         cs.AI   \n",
       "10523                      Artificial Intelligence         cs.AI   \n",
       "2467                       Artificial Intelligence         cs.AI   \n",
       "...                                            ...           ...   \n",
       "2447                       Artificial Intelligence         cs.AI   \n",
       "17549                             Machine Learning         cs.LG   \n",
       "21739                             Machine Learning         cs.LG   \n",
       "2463                       Artificial Intelligence         cs.AI   \n",
       "722                        Artificial Intelligence         cs.AI   \n",
       "\n",
       "       published_date updated_date  \\\n",
       "130124     1994-07-17   1994-07-17   \n",
       "130324     1995-04-13   1995-04-13   \n",
       "2472       2014-08-09   2014-08-09   \n",
       "10523      2013-05-28   2013-05-28   \n",
       "2467       2014-08-09   2014-08-09   \n",
       "...               ...          ...   \n",
       "2447       2014-07-27   2014-07-27   \n",
       "17549      2022-06-16   2023-10-19   \n",
       "21739      2023-10-17   2023-10-19   \n",
       "2463       2014-08-07   2014-08-07   \n",
       "722        2011-06-09   2011-06-09   \n",
       "\n",
       "                                                  authors  \\\n",
       "130124                ['Shuly Wintner', 'Nissim Francez']   \n",
       "130324                ['Shuly Wintner', 'Nissim Francez']   \n",
       "2472                       ['Sheeraz Ahmad', 'Angela Yu']   \n",
       "10523                   ['Sheeraz Ahmad', 'Angela J. Yu']   \n",
       "2467    ['Vicenc Gomez', 'Hilbert Kappen', 'Michael Ch...   \n",
       "...                                                   ...   \n",
       "2447           ['Peter D. Grunwald', 'Joseph Y. Halpern']   \n",
       "17549   ['Harshavardhan Kamarthi', 'Lingkai Kong', 'Al...   \n",
       "21739   ['Harshavardhan Kamarthi', 'Lingkai Kong', 'Al...   \n",
       "2463                       ['Hei Chan', 'Adnan Darwiche']   \n",
       "722                            ['H. Chan', 'A. Darwiche']   \n",
       "\n",
       "                    first_author  \\\n",
       "130124           'Shuly Wintner'   \n",
       "130324           'Shuly Wintner'   \n",
       "2472             'Sheeraz Ahmad'   \n",
       "10523            'Sheeraz Ahmad'   \n",
       "2467              'Vicenc Gomez'   \n",
       "...                          ...   \n",
       "2447         'Peter D. Grunwald'   \n",
       "17549   'Harshavardhan Kamarthi'   \n",
       "21739   'Harshavardhan Kamarthi'   \n",
       "2463                  'Hei Chan'   \n",
       "722                    'H. Chan'   \n",
       "\n",
       "                                                  summary  summary_word_count  \n",
       "130124  This paper describes a first step towards the ...                 107  \n",
       "130324  This paper describes an abstract machine for l...                 116  \n",
       "2472    Sensory inference under conditions of uncertai...                 177  \n",
       "10523   Sensory inference under conditions of uncertai...                 177  \n",
       "2467    We introduce novel results for approximate inf...                 153  \n",
       "...                                                   ...                 ...  \n",
       "2447    It is commonly-accepted wisdom that more infor...                 120  \n",
       "17549   Probabilistic hierarchical time-series forecas...                 211  \n",
       "21739   Probabilistic hierarchical time-series forecas...                 211  \n",
       "2463    Common wisdom has it that small distinctions i...                 140  \n",
       "722     Common wisdom has it that small distinctions i...                 144  \n",
       "\n",
       "[168 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df[df['title'].isin(non_unique_titles.index.tolist())].sort_values(by='title'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these seem to be true duplicate papers, even though some of the extra columns are different. There are only 84 duplicate titles. \n",
    "\n",
    "There is at least one that is not a duplicate (same title, different authors), but many of the others have similar author lists. \n",
    "\n",
    "To Start:\n",
    "* Remove the older row where the title and first_auther are the same. I know this will miss some things (Joris M. Mooij vs. Joris Mooij), but it is a start.\n",
    "* Review the list again to see if there are other candidates for removal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
