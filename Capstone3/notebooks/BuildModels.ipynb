{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50ac72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from transformers import TrainerCallback\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952e950",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This is notebook one of three notebooks for this project. \n",
    "\n",
    "1. **Model building and testing using sciBert**\n",
    "2. Trend analysis\n",
    "3. Author network analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d06d2",
   "metadata": {},
   "source": [
    "# Warning\n",
    "\n",
    "**This notebook takes approximately 5 days to run (at least on a 64G MacBook Pro M1). Use caution when executing.**\n",
    "\n",
    "# Conclusions and comments\n",
    "\n",
    "The goal was to produce a model that could predict the paper categories with >90% accuracy. None of the models tested met this goal. In fact, given that it took a week to tune the model and tune the hyper parameters, the gain for effort was probably not a good tradeoff. \n",
    "\n",
    "Reasons for this likely include:\n",
    "\n",
    "* Class imbalance: model performance for the 3 largest classes was ~80%, 93% and 94%. For many of the smaller classes, it was quite poor. \n",
    "* Overlap between categories: based on the category description, it is likely there is some overlap between these categories. \n",
    "    + One approach could be to try to train on the categories rather than the category codes\n",
    "    + Another approach could be to lump some of the categories together. For example, is there really a distinction beteen cs.LG (Machine Learning) and stat.ML (Machine Learning (statistics))?\n",
    "* This data set only has the abstract and title available as data, yet sciBert was training on full text. It is likely that including the full text in the data set would improve the model performance.\n",
    "\n",
    "# Overview\n",
    "\n",
    "SciBert is a BERT-based model for analysing scientific text. A manuscript describing the model is here: arxiv.org/abs/1903.10676. And the model itself is available here: https://github.com/allenai/scibert?tab=readme-ov-file and on hugging face. \n",
    "\n",
    "For this project I:\n",
    "* Loaded the cleaned dataset from my EDA notebook: https://github.com/deannachurch/Springboard/blob/main/Capstone3/notebooks/EDA.ipynb\n",
    "* Filtered category_codes with less than 5 rows (perhaps I should have been more aggressive)\n",
    "* Split the data into train, test and validation sets\n",
    "* Tokenized the data using the SciBert tokenizer\n",
    "* Created a SciBert model using the Hugging Face library\n",
    "* Trained the model using the training set\n",
    "* Hyperparameter tuning using the training set\n",
    "* Evaluated the model using the test set\n",
    "* Evaluated all three models (original SciBert, tuned SciBert, tuned SciBert + tuned hyperparameters) using the validation set\n",
    "* Saved the model\n",
    "* Compared results of the three models to determine if the class imblance was an issue in model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe702ddc",
   "metadata": {},
   "source": [
    "# Model building and testing with sciBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee92a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category_code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "published_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "updated_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "first_author",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary_word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "author_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "author_count_boxcox",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "title_count_sqrt",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "published_year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "published_quarter",
         "rawType": "period[Q-DEC]",
         "type": "unknown"
        },
        {
         "name": "published_month",
         "rawType": "period[M]",
         "type": "unknown"
        },
        {
         "name": "updated_year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "updated_quarter",
         "rawType": "period[Q-DEC]",
         "type": "unknown"
        },
        {
         "name": "updated_month",
         "rawType": "period[M]",
         "type": "unknown"
        },
        {
         "name": "year_period",
         "rawType": "category",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "7ee2a632-b60c-4a50-bf2e-32c443c5223d",
       "rows": [
        [
         "0",
         "cs-9308101v1",
         "Dynamic Backtracking",
         "Artificial Intelligence",
         "cs.AI",
         "1993-08-01 00:00:00",
         "1993-08-01 00:00:00",
         "['M. L. Ginsberg']",
         "'M. L. Ginsberg'",
         "Because of their occasional need to return to shallow points in a search\ntree, existing backtracking methods can sometimes erase meaningful progress\ntoward solving a search problem. In this paper, we present a method by which\nbacktrack points can be moved deeper in the search space, thereby avoiding this\ndifficulty. The technique developed is a variant of dependency-directed\nbacktracking that uses only polynomial space while still providing useful\ncontrol information and retaining the completeness guarantees provided by\nearlier approaches.",
         "79",
         "1",
         "2",
         "0.0",
         "1.4142135623730951",
         "1993",
         "1993Q3",
         "1993-08",
         "1993",
         "1993Q3",
         "1993-08",
         "1990s"
        ],
        [
         "1",
         "cs-9308102v1",
         "A Market-Oriented Programming Environment and its Application to\n  Distributed Multicommodity Flow Problems",
         "Artificial Intelligence",
         "cs.AI",
         "1993-08-01 00:00:00",
         "1993-08-01 00:00:00",
         "['M. P. Wellman']",
         "'M. P. Wellman'",
         "Market price systems constitute a well-understood class of mechanisms that\nunder certain conditions provide effective decentralization of decision making\nwith minimal communication overhead. In a market-oriented programming approach\nto distributed problem solving, we derive the activities and resource\nallocations for a set of computational agents by computing the competitive\nequilibrium of an artificial economy. WALRAS provides basic constructs for\ndefining computational market structures, and protocols for deriving their\ncorresponding price equilibria. In a particular realization of this approach\nfor a form of multicommodity flow problem, we see that careful construction of\nthe decision process according to economic principles can lead to efficient\ndistributed resource allocation, and that the behavior of the system can be\nmeaningfully analyzed in economic terms.",
         "119",
         "1",
         "12",
         "0.0",
         "3.4641016151377544",
         "1993",
         "1993Q3",
         "1993-08",
         "1993",
         "1993Q3",
         "1993-08",
         "1990s"
        ],
        [
         "2",
         "cs-9309101v1",
         "An Empirical Analysis of Search in GSAT",
         "Artificial Intelligence",
         "cs.AI",
         "1993-09-01 00:00:00",
         "1993-09-01 00:00:00",
         "['I. P. Gent', 'T. Walsh']",
         "'I. P. Gent'",
         "We describe an extensive study of search in GSAT, an approximation procedure\nfor propositional satisfiability. GSAT performs greedy hill-climbing on the\nnumber of satisfied clauses in a truth assignment. Our experiments provide a\nmore complete picture of GSAT's search than previous accounts. We describe in\ndetail the two phases of search: rapid hill-climbing followed by a long plateau\nsearch. We demonstrate that when applied to randomly generated 3SAT problems,\nthere is a very simple scaling with problem size for both the mean number of\nsatisfied clauses and the mean branching rate. Our results allow us to make\ndetailed numerical conjectures about the length of the hill-climbing phase, the\naverage gradient of this phase, and to conjecture that both the average score\nand average branching rate decay exponentially during plateau search. We end by\nshowing how these results can be used to direct future theoretical analysis.\nThis work provides a case study of how computer experiments can be used to\nimprove understanding of the theoretical properties of algorithms.",
         "167",
         "2",
         "7",
         "0.715009822764921",
         "2.6457513110645907",
         "1993",
         "1993Q3",
         "1993-09",
         "1993",
         "1993Q3",
         "1993-09",
         "1990s"
        ],
        [
         "3",
         "cs-9311101v1",
         "The Difficulties of Learning Logic Programs with Cut",
         "Artificial Intelligence",
         "cs.AI",
         "1993-11-01 00:00:00",
         "1993-11-01 00:00:00",
         "['F. Bergadano', 'D. Gunetti', 'U. Trinchero']",
         "'F. Bergadano'",
         "As real logic programmers normally use cut (!), an effective learning\nprocedure for logic programs should be able to deal with it. Because the cut\npredicate has only a procedural meaning, clauses containing cut cannot be\nlearned using an extensional evaluation method, as is done in most learning\nsystems. On the other hand, searching a space of possible programs (instead of\na space of independent clauses) is unfeasible. An alternative solution is to\ngenerate first a candidate base program which covers the positive examples, and\nthen make it consistent by inserting cut where appropriate. The problem of\nlearning programs with cut has not been investigated before and this seems to\nbe a natural and reasonable approach. We generalize this scheme and investigate\nthe difficulties that arise. Some of the major shortcomings are actually\ncaused, in general, by the need for intensional evaluation. As a conclusion,\nthe analysis of this paper suggests, on precise and technical grounds, that\nlearning cut is difficult, and current induction techniques should probably be\nrestricted to purely declarative logic languages.",
         "174",
         "3",
         "8",
         "1.1542082347683233",
         "2.8284271247461903",
         "1993",
         "1993Q4",
         "1993-11",
         "1993",
         "1993Q4",
         "1993-11",
         "1990s"
        ],
        [
         "4",
         "cs-9311102v1",
         "Software Agents: Completing Patterns and Constructing User Interfaces",
         "Artificial Intelligence",
         "cs.AI",
         "1993-11-01 00:00:00",
         "1993-11-01 00:00:00",
         "['J. C. Schlimmer', 'L. A. Hermens']",
         "'J. C. Schlimmer'",
         "To support the goal of allowing users to record and retrieve information,\nthis paper describes an interactive note-taking system for pen-based computers\nwith two distinctive features. First, it actively predicts what the user is\ngoing to write. Second, it automatically constructs a custom, button-box user\ninterface on request. The system is an example of a learning-apprentice\nsoftware- agent. A machine learning component characterizes the syntax and\nsemantics of the user's information. A performance system uses this learned\ninformation to generate completion strings and construct a user interface.\nDescription of Online Appendix: People like to record information. Doing this\non paper is initially efficient, but lacks flexibility. Recording information\non a computer is less efficient but more powerful. In our new note taking\nsoftwre, the user records information directly on a computer. Behind the\ninterface, an agent acts for the user. To help, it provides defaults and\nconstructs a custom user interface. The demonstration is a QuickTime movie of\nthe note taking agent in action. The file is a binhexed self-extracting\narchive. Macintosh utilities for binhex are available from\nmac.archive.umich.edu. QuickTime is available from ftp.apple.com in the\ndts/mac/sys.soft/quicktime.",
         "187",
         "2",
         "8",
         "0.715009822764921",
         "2.8284271247461903",
         "1993",
         "1993Q4",
         "1993-11",
         "1993",
         "1993Q4",
         "1993-11",
         "1990s"
        ],
        [
         "5",
         "cs-9312101v1",
         "Decidable Reasoning in Terminological Knowledge Representation Systems",
         "Artificial Intelligence",
         "cs.AI",
         "1993-12-01 00:00:00",
         "1993-12-01 00:00:00",
         "['M. Buchheit', 'F. M. Donini', 'A. Schaerf']",
         "'M. Buchheit'",
         "Terminological knowledge representation systems (TKRSs) are tools for\ndesigning and using knowledge bases that make use of terminological languages\n(or concept languages). We analyze from a theoretical point of view a TKRS\nwhose capabilities go beyond the ones of presently available TKRSs. The new\nfeatures studied, often required in practical applications, can be summarized\nin three main points. First, we consider a highly expressive terminological\nlanguage, called ALCNR, including general complements of concepts, number\nrestrictions and role conjunction. Second, we allow to express inclusion\nstatements between general concepts, and terminological cycles as a particular\ncase. Third, we prove the decidability of a number of desirable TKRS-deduction\nservices (like satisfiability, subsumption and instance checking) through a\nsound, complete and terminating calculus for reasoning in ALCNR-knowledge\nbases. Our calculus extends the general technique of constraint systems. As a\nbyproduct of the proof, we get also the result that inclusion statements in\nALCNR can be simulated by terminological cycles, if descriptive semantics is\nadopted.",
         "161",
         "3",
         "7",
         "1.1542082347683233",
         "2.6457513110645907",
         "1993",
         "1993Q4",
         "1993-12",
         "1993",
         "1993Q4",
         "1993-12",
         "1990s"
        ],
        [
         "6",
         "cs-9401101v1",
         "Teleo-Reactive Programs for Agent Control",
         "Artificial Intelligence",
         "cs.AI",
         "1994-01-01 00:00:00",
         "1994-01-01 00:00:00",
         "['N. Nilsson']",
         "'N. Nilsson'",
         "A formalism is presented for computing and organizing actions for autonomous\nagents in dynamic environments. We introduce the notion of teleo-reactive (T-R)\nprograms whose execution entails the construction of circuitry for the\ncontinuous computation of the parameters and conditions on which agent action\nis based. In addition to continuous feedback, T-R programs support parameter\nbinding and recursion. A primary difference between T-R programs and many other\ncircuit-based systems is that the circuitry of T-R programs is more compact; it\nis constructed at run time and thus does not have to anticipate all the\ncontingencies that might arise over all possible runs. In addition, T-R\nprograms are intuitive and easy to write and are written in a form that is\ncompatible with automatic planning and learning methods. We briefly describe\nsome experimental applications of T-R programs in the control of simulated and\nactual mobile robots.",
         "144",
         "1",
         "5",
         "0.0",
         "2.23606797749979",
         "1994",
         "1994Q1",
         "1994-01",
         "1994",
         "1994Q1",
         "1994-01",
         "1990s"
        ],
        [
         "7",
         "cs-9402101v1",
         "Learning the Past Tense of English Verbs: The Symbolic Pattern\n  Associator vs. Connectionist Models",
         "Artificial Intelligence",
         "cs.AI",
         "1994-02-01 00:00:00",
         "1994-02-01 00:00:00",
         "['C. X. Ling']",
         "'C. X. Ling'",
         "Learning the past tense of English verbs - a seemingly minor aspect of\nlanguage acquisition - has generated heated debates since 1986, and has become\na landmark task for testing the adequacy of cognitive modeling. Several\nartificial neural networks (ANNs) have been implemented, and a challenge for\nbetter symbolic models has been posed. In this paper, we present a\ngeneral-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree\nlearning algorithm ID3. We conduct extensive head-to-head comparisons on the\ngeneralization ability between ANN models and the SPA under different\nrepresentations. We conclude that the SPA generalizes the past tense of unseen\nverbs better than ANN models by a wide margin, and we offer insights as to why\nthis should be the case. We also discuss a new default strategy for\ndecision-tree learning algorithms.",
         "132",
         "1",
         "14",
         "0.0",
         "3.7416573867739413",
         "1994",
         "1994Q1",
         "1994-02",
         "1994",
         "1994Q1",
         "1994-02",
         "1990s"
        ],
        [
         "8",
         "cs-9402102v1",
         "Substructure Discovery Using Minimum Description Length and Background\n  Knowledge",
         "Artificial Intelligence",
         "cs.AI",
         "1994-02-01 00:00:00",
         "1994-02-01 00:00:00",
         "['D. J. Cook', 'L. B. Holder']",
         "'D. J. Cook'",
         "The ability to identify interesting and repetitive substructures is an\nessential component to discovering knowledge in structural data. We describe a\nnew version of our SUBDUE substructure discovery system based on the minimum\ndescription length principle. The SUBDUE system discovers substructures that\ncompress the original data and represent structural concepts in the data. By\nreplacing previously-discovered substructures in the data, multiple passes of\nSUBDUE produce a hierarchical description of the structural regularities in the\ndata. SUBDUE uses a computationally-bounded inexact graph match that identifies\nsimilar, but not identical, instances of a substructure and finds an\napproximate measure of closeness of two substructures when under computational\nconstraints. In addition to the minimum description length principle, other\nbackground knowledge can be used by SUBDUE to guide the search towards more\nappropriate substructures. Experiments in a variety of domains demonstrate\nSUBDUE's ability to find substructures capable of compressing the original data\nand to discover structural concepts important to the domain. Description of\nOnline Appendix: This is a compressed tar file containing the SUBDUE discovery\nsystem, written in C. The program accepts as input databases represented in\ngraph form, and will output discovered substructures with their corresponding\nvalue.",
         "194",
         "2",
         "9",
         "0.715009822764921",
         "3.0",
         "1994",
         "1994Q1",
         "1994-02",
         "1994",
         "1994Q1",
         "1994-02",
         "1990s"
        ],
        [
         "9",
         "cs-9402103v1",
         "Bias-Driven Revision of Logical Domain Theories",
         "Artificial Intelligence",
         "cs.AI",
         "1994-02-01 00:00:00",
         "1994-02-01 00:00:00",
         "['M. Koppel', 'R. Feldman', 'A. M. Segre']",
         "'M. Koppel'",
         "The theory revision problem is the problem of how best to go about revising a\ndeficient domain theory using information contained in examples that expose\ninaccuracies. In this paper we present our approach to the theory revision\nproblem for propositional domain theories. The approach described here, called\nPTR, uses probabilities associated with domain theory elements to numerically\ntrack the ``flow'' of proof through the theory. This allows us to measure the\nprecise role of a clause or literal in allowing or preventing a (desired or\nundesired) derivation for a given example. This information is used to\nefficiently locate and repair flawed elements of the theory. PTR is proved to\nconverge to a theory which correctly classifies all examples, and shown\nexperimentally to be fast and accurate even for deep theories.",
         "130",
         "3",
         "6",
         "1.1542082347683233",
         "2.449489742783178",
         "1994",
         "1994Q1",
         "1994-02",
         "1994",
         "1994Q1",
         "1994-02",
         "1990s"
        ],
        [
         "10",
         "cs-9403101v1",
         "Exploring the Decision Forest: An Empirical Investigation of Occam's\n  Razor in Decision Tree Induction",
         "Artificial Intelligence",
         "cs.AI",
         "1994-03-01 00:00:00",
         "1994-03-01 00:00:00",
         "['P. M. Murphy', 'M. J. Pazzani']",
         "'P. M. Murphy'",
         "We report on a series of experiments in which all decision trees consistent\nwith the training data are constructed. These experiments were run to gain an\nunderstanding of the properties of the set of consistent decision trees and the\nfactors that affect the accuracy of individual trees. In particular, we\ninvestigated the relationship between the size of a decision tree consistent\nwith some training data and the accuracy of the tree on test data. The\nexperiments were performed on a massively parallel Maspar computer. The results\nof the experiments on several artificial and two real world problems indicate\nthat, for many of the problems investigated, smaller consistent decision trees\nare on average less accurate than the average accuracy of slightly larger\ntrees.",
         "122",
         "2",
         "14",
         "0.715009822764921",
         "3.7416573867739413",
         "1994",
         "1994Q1",
         "1994-03",
         "1994",
         "1994Q1",
         "1994-03",
         "1990s"
        ],
        [
         "11",
         "cs-9406101v1",
         "A Semantics and Complete Algorithm for Subsumption in the CLASSIC\n  Description Logic",
         "Artificial Intelligence",
         "cs.AI",
         "1994-06-01 00:00:00",
         "1994-06-01 00:00:00",
         "['A. Borgida', 'P. F. Patel-Schneider']",
         "'A. Borgida'",
         "This paper analyzes the correctness of the subsumption algorithm used in\nCLASSIC, a description logic-based knowledge representation system that is\nbeing used in practical applications. In order to deal efficiently with\nindividuals in CLASSIC descriptions, the developers have had to use an\nalgorithm that is incomplete with respect to the standard, model-theoretic\nsemantics for description logics. We provide a variant semantics for\ndescriptions with respect to which the current implementation is complete, and\nwhich can be independently motivated. The soundness and completeness of the\npolynomial-time subsumption algorithm is established using description graphs,\nwhich are an abstracted version of the implementation structures used in\nCLASSIC, and are of independent interest.",
         "109",
         "2",
         "12",
         "0.715009822764921",
         "3.4641016151377544",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "12",
         "cs-9406102v1",
         "Applying GSAT to Non-Clausal Formulas",
         "Artificial Intelligence",
         "cs.AI",
         "1994-06-01 00:00:00",
         "1994-06-01 00:00:00",
         "['R. Sebastiani']",
         "'R. Sebastiani'",
         "In this paper we describe how to modify GSAT so that it can be applied to\nnon-clausal formulas. The idea is to use a particular ``score'' function which\ngives the number of clauses of the CNF conversion of a formula which are false\nunder a given truth assignment. Its value is computed in linear time, without\nconstructing the CNF conversion itself. The proposed methodology applies to\nmost of the variants of GSAT proposed so far.",
         "75",
         "1",
         "5",
         "0.0",
         "2.23606797749979",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130087",
         "cmp-lg-9406017v1",
         "An Automatic Method of Finding Topic Boundaries",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-07 00:00:00",
         "1994-06-07 00:00:00",
         "['Jeffrey C. Reynar']",
         "'Jeffrey C. Reynar'",
         "This article outlines a new method of locating discourse boundaries based on\nlexical cohesion and a graphical technique called dotplotting. The application\nof dotplotting to discourse segmentation can be performed either manually, by\nexamining a graph, or automatically, using an optimization algorithm. The\nresults of two experiments involving automatically locating boundaries between\na series of concatenated documents are presented. Areas of application and\nfuture directions for this work are also outlined.",
         "71",
         "1",
         "7",
         "0.0",
         "2.6457513110645907",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130090",
         "cmp-lg-9406020v1",
         "DPOCL: A Principled Approach to Discourse Planning",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-10 00:00:00",
         "1994-06-10 00:00:00",
         "['R. Michael Young', 'Johanna D. Moore']",
         "'R. Michael Young'",
         "Research in discourse processing has identified two representational\nrequirements for discourse planning systems. First, discourse plans must\nadequately represent the intentional structure of the utterances they produce\nin order to enable a computational discourse agent to respond effectively to\ncommunicative failures \\cite{MooreParisCL}. Second, discourse plans must\nrepresent the informational structure of utterances. In addition to these\nrepresentational requirements, we argue that discourse planners should be\nformally characterizable in terms of soundness and completeness.",
         "73",
         "2",
         "7",
         "0.715009822764921",
         "2.6457513110645907",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130091",
         "cmp-lg-9406021v1",
         "A symbolic description of punning riddles and its computer\n  implementation",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-13 00:00:00",
         "1994-06-13 00:00:00",
         "['Kim Binsted', 'Graeme Ritchie']",
         "'Kim Binsted'",
         "Riddles based on simple puns can be classified according to the patterns of\nword, syllable or phrase similarity they depend upon. We have devised a formal\nmodel of the semantic and syntactic regularities underlying some of the simpler\ntypes of punning riddle. We have also implemented this preliminary theory in a\ncomputer program which can generate riddles from a lexicon containing general\ndata about words and phrases; that is, the lexicon content is not customised to\nproduce jokes. Informal evaluation of the program's results by a set of human\njudges suggest that the riddles produced by this program are of comparable\nquality to those in general circulation among school children.",
         "110",
         "2",
         "10",
         "0.715009822764921",
         "3.1622776601683795",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130092",
         "cmp-lg-9406022v1",
         "An implemented model of punning riddles",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-13 00:00:00",
         "1994-06-13 00:00:00",
         "['Kim Binsted', 'Graeme Ritchie']",
         "'Kim Binsted'",
         "In this paper, we discuss a model of simple question-answer punning,\nimplemented in a program, JAPE, which generates riddles from humour-independent\nlexical entries. The model uses two main types of structure: schemata, which\ndetermine the relationships between key words in a joke, and templates, which\nproduce the surface form of the joke. JAPE succeeds in generating pieces of\ntext that are recognizably jokes, but some of them are not very good jokes. We\nmention some potential improvements and extensions, including post-production\nheuristics for ordering the jokes according to quality.",
         "89",
         "2",
         "6",
         "0.715009822764921",
         "2.449489742783178",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130093",
         "cmp-lg-9406023v1",
         "A Spanish Tagset for the CRATER Project",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-14 00:00:00",
         "1994-06-14 00:00:00",
         "['Fernando Sánchez León']",
         "'Fernando Sánchez León'",
         "This working paper describes the Spanish tagset to be used in the context of\nCRATER, a CEC funded project aiming at the creation of a multilingual (English,\nFrench, Spanish) aligned corpus using the International Telecommunications\nUnion corpus. In this respect, each version of the corpus will be (or is\ncurrently) tagged. Xerox PARC tagger will be adapted to Spanish in order to\nperform the tagging of the Spanish version. This tagset has been devised as the\nideal one for Spanish, and has been posted to several lists in order to get\nfeedback to it.",
         "94",
         "1",
         "7",
         "0.0",
         "2.6457513110645907",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130088",
         "cmp-lg-9406018v3",
         "TDL--- A Type Description Language for Constraint-Based Grammars",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-08 00:00:00",
         "1994-06-15 00:00:00",
         "['Hans-Ulrich Krieger', 'Ulrich Schäfer']",
         "'Hans-Ulrich Krieger'",
         "This paper presents \\tdl, a typed feature-based representation language and\ninference system. Type definitions in \\tdl\\ consist of type and feature\nconstraints over the boolean connectives. \\tdl\\ supports open- and closed-world\nreasoning over types and allows for partitions and incompatible types. Working\nwith partially as well as with fully expanded types is possible. Efficient\nreasoning in \\tdl\\ is accomplished through specialized modules.",
         "62",
         "2",
         "8",
         "0.715009822764921",
         "2.8284271247461903",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130094",
         "cmp-lg-9406024v1",
         "Learning Fault-tolerant Speech Parsing with SCREEN",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-16 00:00:00",
         "1994-06-16 00:00:00",
         "['Stefan Wermter', 'Volker Weber']",
         "'Stefan Wermter'",
         "This paper describes a new approach and a system SCREEN for fault-tolerant\nspeech parsing. SCREEEN stands for Symbolic Connectionist Robust EnterprisE for\nNatural language. Speech parsing describes the syntactic and semantic analysis\nof spontaneous spoken language. The general approach is based on incremental\nimmediate flat analysis, learning of syntactic and semantic speech parsing,\nparallel integration of current hypotheses, and the consideration of various\nforms of speech related errors. The goal for this approach is to explore the\nparallel interactions between various knowledge sources for learning\nincremental fault-tolerant speech parsing. This approach is examined in a\nsystem SCREEN using various hybrid connectionist techniques. Hybrid\nconnectionist techniques are examined because of their promising properties of\ninherent fault tolerance, learning, gradedness and parallel constraint\nintegration. The input for SCREEN is hypotheses about recognized words of a\nspoken utterance potentially analyzed by a speech system, the output is\nhypotheses about the flat syntactic and semantic analysis of the utterance. In\nthis paper we focus on the general approach, the overall architecture, and\nexamples for learning flat syntactic speech parsing. Different from most other\nspeech language architectures SCREEN emphasizes an interactive rather than an\nautonomous position, learning rather than encoding, flat analysis rather than\nin-depth analysis, and fault-tolerant processing of phonetic, syntactic and\nsemantic knowledge.",
         "210",
         "2",
         "6",
         "0.715009822764921",
         "2.449489742783178",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130095",
         "cmp-lg-9406025v1",
         "Emergent Parsing and Generation with Generalized Chart",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-16 00:00:00",
         "1994-06-16 00:00:00",
         "['HASIDA Koiti']",
         "'HASIDA Koiti'",
         "A new, flexible inference method for Horn logic program is proposed, which is\na drastic generalization of chart parsing, partial instantiations of clauses in\na program roughly corresponding to arcs in a chart. Chart-like parsing and\nsemantic-head-driven generation emerge from this method. With a parsimonious\ninstantiation scheme for ambiguity packing, the parsing complexity reduces to\nthat of standard chart-based algorithms.",
         "60",
         "1",
         "7",
         "0.0",
         "2.6457513110645907",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130089",
         "cmp-lg-9406019v3",
         "A Complete and Recursive Feature Theory",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-10 00:00:00",
         "1994-06-17 00:00:00",
         "['Rolf Backofen', 'Gert Smolka']",
         "'Rolf Backofen'",
         "Various feature descriptions are being employed in logic programming\nlanguages and constrained-based grammar formalisms. The common notational\nprimitive of these descriptions are functional attributes called features. The\ndescriptions considered in this paper are the possibly quantified first-order\nformulae obtained from a signature of binary and unary predicates called\nfeatures and sorts, respectively. We establish a first-order theory FT by means\nof three axiom schemes, show its completeness, and construct three elementarily\nequivalent models. One of the models consists of so-called feature graphs, a\ndata structure common in computational linguistics. The other two models\nconsist of so-called feature trees, a record-like data structure generalizing\nthe trees corresponding to first-order terms. Our completeness proof exhibits a\nterminating simplification system deciding validity and satisfiability of\npossibly quantified feature descriptions.",
         "126",
         "2",
         "6",
         "0.715009822764921",
         "2.449489742783178",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130096",
         "cmp-lg-9406026v1",
         "The Very Idea of Dynamic Semantics",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-17 00:00:00",
         "1994-06-17 00:00:00",
         "['David Israel']",
         "'David Israel'",
         "\"Natural languages are programming languages for minds.\" Can we or should we\ntake this slogan seriously? If so, how? Can answers be found by looking at the\nvarious \"dynamic\" treatments of natural language developed over the last decade\nor so, mostly in response to problems associated with donkey anaphora? In\nDynamic Logic of Programs, the meaning of a program is a binary relation on the\nset of states of some abstract machine. This relation is meant to model aspects\nof the effects of the execution of the program, in particular its input-output\nbehavior. What, if anything, are the dynamic aspects of various proposed\ndynamic semantics for natural languages supposed to model? Is there anything\ndynamic to be modeled? If not, what is all the full about? We shall try to\nanswer some, at least, of these questions and provide materials for answers to\nothers.",
         "144",
         "1",
         "6",
         "0.0",
         "2.449489742783178",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130097",
         "cmp-lg-9406027v1",
         "Analyzing and Improving Statistical Language Models for Speech\n  Recognition",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-17 00:00:00",
         "1994-06-17 00:00:00",
         "['Joerg P. Ueberla']",
         "'Joerg P. Ueberla'",
         "In many current speech recognizers, a statistical language model is used to\nindicate how likely it is that a certain word will be spoken next, given the\nwords recognized so far. How can statistical language models be improved so\nthat more complex speech recognition tasks can be tackled? Since the knowledge\nof the weaknesses of any theory often makes improving the theory easier, the\ncentral idea of this thesis is to analyze the weaknesses of existing\nstatistical language models in order to subsequently improve them. To that end,\nwe formally define a weakness of a statistical language model in terms of the\nlogarithm of the total probability, LTP, a term closely related to the standard\nperplexity measure used to evaluate statistical language models. We apply our\ndefinition of a weakness to a frequently used statistical language model,\ncalled a bi-pos model. This results, for example, in a new modeling of unknown\nwords which improves the performance of the model by 14% to 21%. Moreover, one\nof the identified weaknesses has prompted the development of our generalized\nN-pos language model, which is also outlined in this thesis. It can incorporate\nlinguistic knowledge even if it extends over many words and this is not\nfeasible in a traditional N-pos model. This leads to a discussion of\nwhatknowledge should be added to statistical language models in general and we\ngive criteria for selecting potentially useful knowledge. These results show\nthe usefulness of both our definition of a weakness and of performing an\nanalysis of weaknesses of statistical language models in general.",
         "258",
         "1",
         "9",
         "0.0",
         "3.0",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130100",
         "cmp-lg-9406030v2",
         "The complexity of normal form rewrite sequences for Associativity",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-20 00:00:00",
         "1994-06-20 00:00:00",
         "['Michael Niv']",
         "'Michael Niv'",
         "The complexity of a particular term-rewrite system is considered: the rule of\nassociativity (x*y)*z --> x*(y*z). Algorithms and exact calculations are given\nfor the longest and shortest sequences of applications of --> that result in\nnormal form (NF). The shortest NF sequence for a term x is always n-drm(x),\nwhere n is the number of occurrences of * in x and drm(x) is the depth of the\nrightmost leaf of x. The longest NF sequence for any term is of length\nn(n-1)/2.",
         "82",
         "1",
         "9",
         "0.0",
         "3.0",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130098",
         "cmp-lg-9406028v2",
         "Resolution of Syntactic Ambiguity: the Case of New Subjects",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-20 00:00:00",
         "1994-06-20 00:00:00",
         "['Michael Niv']",
         "'Michael Niv'",
         "I review evidence for the claim that syntactic ambiguities are resolved on\nthe basis of the meaning of the competing analyses, not their structure. I\nidentify a collection of ambiguities that do not yet have a meaning-based\naccount and propose one which is based on the interaction of discourse and\ngrammatical function. I provide evidence for my proposal by examining\nstatistical properties of the Penn Treebank of syntactically annotated text.",
         "70",
         "1",
         "9",
         "0.0",
         "3.0",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130099",
         "cmp-lg-9406029v1",
         "A Computational Model of Syntactic Processing: Ambiguity Resolution from\n  Interpretation",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-20 00:00:00",
         "1994-06-20 00:00:00",
         "['Michael Niv']",
         "'Michael Niv'",
         "Syntactic ambiguity abounds in natural language, yet humans have no\ndifficulty coping with it. In fact, the process of ambiguity resolution is\nalmost always unconscious. But it is not infallible, however, as example 1\ndemonstrates.\n  1. The horse raced past the barn fell.\n  This sentence is perfectly grammatical, as is evident when it appears in the\nfollowing context:\n  2. Two horses were being shown off to a prospective buyer. One was raced past\na meadow. and the other was raced past a barn. ...\n  Grammatical yet unprocessable sentences such as 1 are called `garden-path\nsentences.' Their existence provides an opportunity to investigate the human\nsentence processing mechanism by studying how and when it fails. The aim of\nthis thesis is to construct a computational model of language understanding\nwhich can predict processing difficulty. The data to be modeled are known\nexamples of garden path and non-garden path sentences, and other results from\npsycholinguistics.\n  It is widely believed that there are two distinct loci of computation in\nsentence processing: syntactic parsing and semantic interpretation. One\nlongstanding controversy is which of these two modules bears responsibility for\nthe immediate resolution of ambiguity. My claim is that it is the latter, and\nthat the syntactic processing module is a very simple device which blindly and\nfaithfully constructs all possible analyses for the sentence up to the current\npoint of processing. The interpretive module serves as a filter, occasionally\ndiscarding certain of these analyses which it deems less appropriate for the\nongoing discourse than their competitors.\n  This document is divided into three parts. The first is introductory, and\nreviews a selection of proposals from the sentence processing literature. The\nsecond part explores a body of data which has been adduced in support of a\ntheory of structural preferences --- one that is inconsistent with the present\nclaim. I show how the current proposal can be specified to account for the\navailable data, and moreover to predict where structural preference theories\nwill go wrong. The third part is a theoretical investigation of how well the\nproposed architecture can be realized using current conceptions of linguistic\ncompetence. In it, I present a parsing algorithm and a meaning-based ambiguity\nresolution method.",
         "364",
         "1",
         "10",
         "0.0",
         "3.1622776601683795",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130101",
         "cmp-lg-9406031v1",
         "A Psycholinguistically Motivated Parser for CCG",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-20 00:00:00",
         "1994-06-20 00:00:00",
         "['Michael Niv']",
         "'Michael Niv'",
         "Considering the speed in which humans resolve syntactic ambiguity, and the\noverwhelming evidence that syntactic ambiguity is resolved through selection of\nthe analysis whose interpretation is the most `sensible', one comes to the\nconclusion that interpretation, hence parsing take place incrementally, just\nabout every word. Considerations of parsimony in the theory of the syntactic\nprocessor lead one to explore the simplest of parsers: one which represents\nonly analyses as defined by the grammar and no other information.\n  Toward this aim of a simple, incremental parser I explore the proposal that\nthe competence grammar is a Combinatory Categorial Grammar (CCG). I address the\nproblem of the proliferating analyses that stem from CCG's associativity of\nderivation. My solution involves maintaining only the maximally incremental\nanalysis and, when necessary, computing the maximally right-branching analysis.\nI use results from the study of rewrite systems to show that this computation\nis efficient.",
         "147",
         "1",
         "6",
         "0.0",
         "2.449489742783178",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130102",
         "cmp-lg-9406032v1",
         "Anytime Algorithms for Speech Parsing?",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-21 00:00:00",
         "1994-06-21 00:00:00",
         "['Guenther Goerz', 'Marcus Kesseler']",
         "'Guenther Goerz'",
         "This paper discusses to which extent the concept of ``anytime algorithms''\ncan be applied to parsing algorithms with feature unification. We first try to\ngive a more precise definition of what an anytime algorithm is. We arque that\nparsing algorithms have to be classified as contract algorithms as opposed to\n(truly) interruptible algorithms. With the restriction that the transaction\nbeing active at the time an interrupt is issued has to be completed before the\ninterrupt can be executed, it is possible to provide a parser with limited\nanytime behavior, which is in fact being realized in our research prototype.",
         "99",
         "2",
         "5",
         "0.715009822764921",
         "2.23606797749979",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130106",
         "cmp-lg-9406036v1",
         "Text Analysis Tools in Spoken Language Processing",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-23 00:00:00",
         "1994-06-23 00:00:00",
         "['Michael Riley', 'Richard Sproat']",
         "'Michael Riley'",
         "This submission contains the postscript of the final version of the slides\nused in our ACL-94 tutorial.",
         "17",
         "2",
         "7",
         "0.715009822764921",
         "2.6457513110645907",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130104",
         "cmp-lg-9406034v1",
         "Decision Lists for Lexical Ambiguity Resolution: Application to Accent\n  Restoration in Spanish and French",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-23 00:00:00",
         "1994-06-23 00:00:00",
         "['David Yarowsky']",
         "'David Yarowsky'",
         "This paper presents a statistical decision procedure for lexical ambiguity\nresolution. The algorithm exploits both local syntactic patterns and more\ndistant collocational evidence, generating an efficient, effective, and highly\nperspicuous recipe for resolving a given ambiguity. By identifying and\nutilizing only the single best disambiguating evidence in a target context, the\nalgorithm avoids the problematic complex modeling of statistical dependencies.\nAlthough directly applicable to a wide class of ambiguities, the algorithm is\ndescribed and evaluated in a realistic case study, the problem of restoring\nmissing accents in Spanish and French text.",
         "91",
         "1",
         "14",
         "0.0",
         "3.7416573867739413",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130107",
         "cmp-lg-9406037v1",
         "Multi-Paragraph Segmentation of Expository Text",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-23 00:00:00",
         "1994-06-23 00:00:00",
         "['Marti A. Hearst']",
         "'Marti A. Hearst'",
         "This paper describes TextTiling, an algorithm for partitioning expository\ntexts into coherent multi-paragraph discourse units which reflect the subtopic\nstructure of the texts. The algorithm uses domain-independent lexical frequency\nand distribution information to recognize the interactions of multiple\nsimultaneous themes. Two fully-implemented versions of the algorithm are\ndescribed and shown to produce segmentation that corresponds well to human\njudgments of the major subtopic boundaries of thirteen lengthy texts.",
         "68",
         "1",
         "5",
         "0.0",
         "2.23606797749979",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130103",
         "cmp-lg-9406033v3",
         "Verb Semantics and Lexical Selection",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-22 00:00:00",
         "1994-06-24 00:00:00",
         "['Zhibiao Wu', 'Martha Palmer']",
         "'Zhibiao Wu'",
         "This paper will focus on the semantic representation of verbs in computer\nsystems and its impact on lexical selection problems in machine translation\n(MT). Two groups of English and Chinese verbs are examined to show that lexical\nselection must be based on interpretation of the sentence as well as selection\nrestrictions placed on the verb arguments. A novel representation scheme is\nsuggested, and is compared to representations with selection restrictions used\nin transfer-based MT. We see our approach as closely aligned with\nknowledge-based MT approaches (KBMT), and as a separate component that could be\nincorporated into existing systems. Examples and experimental results will show\nthat, using this scheme, inexact matches can achieve correct lexical selection.",
         "115",
         "2",
         "5",
         "0.715009822764921",
         "2.23606797749979",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130109",
         "cmp-lg-9406039v1",
         "Three studies of grammar-based surface-syntactic parsing of unrestricted\n  English text. A summary and orientation",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-27 00:00:00",
         "1994-06-27 00:00:00",
         "['Atro Voutilainen']",
         "'Atro Voutilainen'",
         "The dissertation addresses the design of parsing grammars for automatic\nsurface-syntactic analysis of unconstrained English text. It consists of a\nsummary and three articles. {\\it Morphological disambiguation} documents a\ngrammar for morphological (or part-of-speech) disambiguation of English, done\nwithin the Constraint Grammar framework proposed by Fred Karlsson. The\ndisambiguator seeks to discard those of the alternative morphological analyses\nproposed by the lexical analyser that are contextually illegitimate. The 1,100\nconstraints express some 23 general, essentially syntactic statements as\nrestrictions on the linear order of morphological tags. The error rate of the\nmorphological disambiguator is about ten times smaller than that of another\nstate-of-the-art probabilistic disambiguator, given that both are allowed to\nleave some of the hardest ambiguities unresolved. This accuracy suggests the\nviability of the grammar-based approach to natural language parsing, thus also\ncontributing to the more general debate concerning the viability of\nprobabilistic vs.\\ linguistic techniques. {\\it Experiments with heuristics}\naddresses the question of how to resolve those ambiguities that survive the\nmorphological disambiguator. Two approaches are presented and empirically\nevaluated: (i) heuristic disambiguation constraints and (ii) techniques for\nlearning from the fully disambiguated part of the corpus and then applying this\ninformation to resolving remaining ambiguities.",
         "198",
         "1",
         "14",
         "0.0",
         "3.7416573867739413",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130108",
         "cmp-lg-9406038v1",
         "An Empirical Model of Acknowledgment for Spoken-Language Systems",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-27 00:00:00",
         "1994-06-27 00:00:00",
         "['David G. Novick', 'Stephen Sutton']",
         "'David G. Novick'",
         "We refine and extend prior views of the description, purposes, and\ncontexts-of-use of acknowledgment acts through empirical examination of the use\nof acknowledgments in task-based conversation. We distinguish three broad\nclasses of acknowledgments (other-->ackn, self-->other-->ackn, and self+ackn)\nand present a catalogue of 13 patterns within these classes that account for\nthe specific uses of acknowledgment in the corpus.",
         "58",
         "2",
         "8",
         "0.715009822764921",
         "2.8284271247461903",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130110",
         "cmp-lg-9406040v1",
         "Learning unification-based grammars using the Spoken English Corpus",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-28 00:00:00",
         "1994-06-28 00:00:00",
         "['Miles Osborne', 'Derek Bridge']",
         "'Miles Osborne'",
         "This paper describes a grammar learning system that combines model-based and\ndata-driven learning within a single framework. Our results from learning\ngrammars using the Spoken English Corpus (SEC) suggest that combined\nmodel-based and data-driven learning can produce a more plausible grammar than\nis the case when using either learning style isolation.",
         "51",
         "2",
         "8",
         "0.715009822764921",
         "2.8284271247461903",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130105",
         "cmp-lg-9406035v3",
         "DISCO---An HPSG-based NLP System and its Application for Appointment\n  Scheduling (Project Note)",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-06-23 00:00:00",
         "1994-06-30 00:00:00",
         "['Hans Uszkoreit', 'Rolf Backofen', 'Stephan Busemann', 'Abdel Kader Diagne', 'Elizabeth A. Hinkelman', 'Walter Kasper', 'Bernd Kiefer', 'Hans-Ulrich Krieger', 'Klaus Netter', 'Guenter Neumann', 'Stephan Oepen', 'Stephen P. Spackman']",
         "'Hans Uszkoreit'",
         "The natural language system DISCO is described. It combines o a powerful and\nflexible grammar development system; o linguistic competence for German\nincluding morphology, syntax and semantics; o new methods for linguistic\nperformance modelling on the basis of high-level competence grammars; o new\nmethods for modelling multi-agent dialogue competence; o an interesting sample\napplication for appointment scheduling and calendar management.",
         "60",
         "12",
         "12",
         "2.781625959610504",
         "3.4641016151377544",
         "1994",
         "1994Q2",
         "1994-06",
         "1994",
         "1994Q2",
         "1994-06",
         "1990s"
        ],
        [
         "130112",
         "cmp-lg-9407002v1",
         "Syntactic Analysis by Local Grammars Automata: an Efficient Algorithm",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-07-04 00:00:00",
         "1994-07-04 00:00:00",
         "['Mehryar Mohri']",
         "'Mehryar Mohri'",
         "Local grammars can be represented in a very convenient way by automata. This\npaper describes and illustrates an efficient algorithm for the application of\nlocal grammars put in this form to lemmatized texts.",
         "33",
         "1",
         "9",
         "0.0",
         "3.0",
         "1994",
         "1994Q3",
         "1994-07",
         "1994",
         "1994Q3",
         "1994-07",
         "1990s"
        ],
        [
         "130111",
         "cmp-lg-9407001v1",
         "Morphology with a Null-Interface",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-07-04 00:00:00",
         "1994-07-04 00:00:00",
         "['Harald Trost', 'Johannes Matiasek']",
         "'Harald Trost'",
         "We present an integrated architecture for word-level and sentence-level\nprocessing in a unification-based paradigm. The core of the system is a CLP\nimplementation of a unification engine for feature structures supporting\nrelational values. In this framework an HPSG-style grammar is implemented.\nWord-level processing uses X2MorF, a morphological component based on an\nextended version of two-level morphology. This component is tightly integrated\nwith the grammar as a relation. The advantage of this approach is that\nmorphology and syntax are kept logically autonomous while at the same time\nminimizing interface problems.",
         "89",
         "2",
         "4",
         "0.715009822764921",
         "2.0",
         "1994",
         "1994Q3",
         "1994-07",
         "1994",
         "1994Q3",
         "1994-07",
         "1990s"
        ],
        [
         "130113",
         "cmp-lg-9407003v1",
         "Compact Representations by Finite-State Transducers",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-07-04 00:00:00",
         "1994-07-04 00:00:00",
         "['Mehryar Mohri']",
         "'Mehryar Mohri'",
         "Finite-state transducers give efficient representations of many Natural\nLanguage phenomena. They allow to account for complex lexicon restrictions\nencountered, without involving the use of a large set of complex rules\ndifficult to analyze. We here show that these representations can be made very\ncompact, indicate how to perform the corresponding minimization, and point out\ninteresting linguistic side-effects of this operation.",
         "60",
         "1",
         "5",
         "0.0",
         "2.23606797749979",
         "1994",
         "1994Q3",
         "1994-07",
         "1994",
         "1994Q3",
         "1994-07",
         "1990s"
        ],
        [
         "130114",
         "cmp-lg-9407004v1",
         "Japanese word sense disambiguation based on examples of synonyms",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-07-05 00:00:00",
         "1994-07-05 00:00:00",
         "['Mitsutaka Matsumoto']",
         "'Mitsutaka Matsumoto'",
         "(This is not the abstract): The language is Japanese. If your printer does\nnot have fonts for Japases characters, the characters in figures will not be\nprinted out correctly. Dissertation for Bachelor's degree at Kyoto\nUniversity(Nagao lab.),March 1994.",
         "38",
         "1",
         "9",
         "0.0",
         "3.0",
         "1994",
         "1994Q3",
         "1994-07",
         "1994",
         "1994Q3",
         "1994-07",
         "1990s"
        ],
        [
         "130116",
         "cmp-lg-9407006v1",
         "Interleaving Syntax and Semantics in an Efficient Bottom-Up Parser",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-07-05 00:00:00",
         "1994-07-05 00:00:00",
         "['John Dowding', 'Robert Moore', 'Francois Andry', 'Douglas Moran']",
         "'John Dowding'",
         "We describe an efficient bottom-up parser that interleaves syntactic and\nsemantic structure building. Two techniques are presented for reducing search\nby reducing local ambiguity: Limited left-context constraints are used to\nreduce local syntactic ambiguity, and deferred sortal-constraint application is\nused to reduce local semantic ambiguity. We experimentally evaluate these\ntechniques, and show dramatic reductions in both number of chart-edges and\ntotal parsing time. The robust processing capabilities of the parser are\ndemonstrated in its use in improving the accuracy of a speech recognizer.",
         "83",
         "4",
         "9",
         "1.4755933758793587",
         "3.0",
         "1994",
         "1994Q3",
         "1994-07",
         "1994",
         "1994Q3",
         "1994-07",
         "1990s"
        ],
        [
         "130117",
         "cmp-lg-9407007v1",
         "GEMINI: A Natural Language System for Spoken-Language Understanding",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-07-05 00:00:00",
         "1994-07-05 00:00:00",
         "['John Dowding', 'Jean Mark Gawron', 'Doug Appelt', 'John Bear', 'Lynn Cherny', 'Robert Moore', 'Douglas Moran']",
         "'John Dowding'",
         "Gemini is a natural language understanding system developed for spoken\nlanguage applications. The paper describes the architecture of Gemini, paying\nparticular attention to resolving the tension between robustness and\novergeneration. Gemini features a broad-coverage unification-based grammar of\nEnglish, fully interleaved syntactic and semantic processing in an all-paths,\nbottom-up parser, and an utterance-level parser to find interpretations of\nsentences that might not be analyzable as complete sentences. Gemini also\nincludes novel components for recognizing and correcting grammatical\ndisfluencies, and for doing parse preferences. This paper presents a\ncomponent-by-component view of Gemini, providing detailed relevant measurements\nof size, efficiency, and performance.",
         "99",
         "7",
         "8",
         "2.124881541971062",
         "2.8284271247461903",
         "1994",
         "1994Q3",
         "1994-07",
         "1994",
         "1994Q3",
         "1994-07",
         "1990s"
        ],
        [
         "130115",
         "cmp-lg-9407005v1",
         "A Corrective Training Algorithm for Adaptive Learning in Bag Generation",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-07-06 00:00:00",
         "1994-07-06 00:00:00",
         "['Hsin-Hsi Chen', 'Yue-Shi Lee']",
         "'Hsin-Hsi Chen'",
         "The sampling problem in training corpus is one of the major sources of errors\nin corpus-based applications. This paper proposes a corrective training\nalgorithm to best-fit the run-time context domain in the application of bag\ngeneration. It shows which objects to be adjusted and how to adjust their\nprobabilities. The resulting techniques are greatly simplified and the\nexperimental results demonstrate the promising effects of the training\nalgorithm from generic domain to specific domain. In general, these techniques\ncan be easily extended to various language models and corpus-based\napplications.",
         "88",
         "2",
         "10",
         "0.715009822764921",
         "3.1622776601683795",
         "1994",
         "1994Q3",
         "1994-07",
         "1994",
         "1994Q3",
         "1994-07",
         "1990s"
        ],
        [
         "130118",
         "cmp-lg-9407008v1",
         "Tricolor DAGs for Machine Translation",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-07-06 00:00:00",
         "1994-07-06 00:00:00",
         "['Koichi Takeda']",
         "'Koichi Takeda'",
         "Machine translation (MT) has recently been formulated in terms of\nconstraint-based knowledge representation and unification theories, but it is\nbecoming more and more evident that it is not possible to design a practical MT\nsystem without an adequate method of handling mismatches between semantic\nrepresentations in the source and target languages. In this paper, we introduce\nthe idea of ``information-based'' MT, which is considerably more flexible than\ninterlingual MT or the conventional transfer-based MT.",
         "74",
         "1",
         "5",
         "0.0",
         "2.23606797749979",
         "1994",
         "1994Q3",
         "1994-07",
         "1994",
         "1994Q3",
         "1994-07",
         "1990s"
        ],
        [
         "130119",
         "cmp-lg-9407009v1",
         "Estimating Performance of Pipelined Spoken Language Translation Systems",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-07-12 00:00:00",
         "1994-07-12 00:00:00",
         "['Manny Rayner', 'David Carter', 'Patti Price', 'Bertil Lyberg']",
         "'Manny Rayner'",
         "Most spoken language translation systems developed to date rely on a\npipelined architecture, in which the main stages are speech recognition,\nlinguistic analysis, transfer, generation and speech synthesis. When making\nprojections of error rates for systems of this kind, it is natural to assume\nthat the error rates for the individual components are independent, making the\nsystem accuracy the product of the component accuracies.\n  The paper reports experiments carried out using the SRI-SICS-Telia Research\nSpoken Language Translator and a 1000-utterance sample of unseen data. The\nresults suggest that the naive performance model leads to serious overestimates\nof system error rates, since there are in fact strong dependencies between the\ncomponents. Predicting the system error rate on the independence assumption by\nsimple multiplication resulted in a 16\\% proportional overestimate for all\nutterances, and a 19\\% overestimate when only utterances of length 1-10 words\nwere considered.",
         "144",
         "4",
         "8",
         "1.4755933758793587",
         "2.8284271247461903",
         "1994",
         "1994Q3",
         "1994-07",
         "1994",
         "1994Q3",
         "1994-07",
         "1990s"
        ],
        [
         "130120",
         "cmp-lg-9407010v1",
         "Combining Knowledge Sources to Reorder N-Best Speech Hypothesis Lists",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-07-12 00:00:00",
         "1994-07-12 00:00:00",
         "['Manny Rayner', 'David Carter', 'Vassilios Digalakis', 'Patti Price']",
         "'Manny Rayner'",
         "A simple and general method is described that can combine different knowledge\nsources to reorder N-best lists of hypotheses produced by a speech recognizer.\nThe method is automatically trainable, acquiring information from both positive\nand negative examples. Experiments are described in which it was tested on a\n1000-utterance sample of unseen ATIS data.",
         "53",
         "4",
         "9",
         "1.4755933758793587",
         "3.0",
         "1994",
         "1994Q3",
         "1994-07",
         "1994",
         "1994Q3",
         "1994-07",
         "1990s"
        ],
        [
         "130121",
         "cmp-lg-9407011v1",
         "Discourse Obligations in Dialogue Processing",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-07-14 00:00:00",
         "1994-07-14 00:00:00",
         "['David R. Traum', 'James F. Allen']",
         "'David R. Traum'",
         "We show that in modeling social interaction, particularly dialogue, the\nattitude of obligation can be a useful adjunct to the popularly considered\nattitudes of belief, goal, and intention and their mutual and shared\ncounterparts. In particular, we show how discourse obligations can be used to\naccount in a natural manner for the connection between a question and its\nanswer in dialogue and how obligations can be used along with other parts of\nthe discourse context to extend the coverage of a dialogue system.",
         "83",
         "2",
         "5",
         "0.715009822764921",
         "2.23606797749979",
         "1994",
         "1994Q3",
         "1994-07",
         "1994",
         "1994Q3",
         "1994-07",
         "1990s"
        ],
        [
         "130122",
         "cmp-lg-9407012v1",
         "Phoneme Recognition Using Acoustic Events",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-07-15 00:00:00",
         "1994-07-15 00:00:00",
         "['Kai Huebener', 'Julie Carson-Berndsen']",
         "'Kai Huebener'",
         "This paper presents a new approach to phoneme recognition using nonsequential\nsub--phoneme units. These units are called acoustic events and are\nphonologically meaningful as well as recognizable from speech signals. Acoustic\nevents form a phonologically incomplete representation as compared to\ndistinctive features. This problem may partly be overcome by incorporating\nphonological constraints. Currently, 24 binary events describing manner and\nplace of articulation, vowel quality and voicing are used to recognize all\nGerman phonemes. Phoneme recognition in this paradigm consists of two steps:\nAfter the acoustic events have been determined from the speech signal, a\nphonological parser is used to generate syllable and phoneme hypotheses from\nthe event lattice. Results obtained on a speaker--dependent corpus are\npresented.",
         "116",
         "2",
         "5",
         "0.715009822764921",
         "2.23606797749979",
         "1994",
         "1994Q3",
         "1994-07",
         "1994",
         "1994Q3",
         "1994-07",
         "1990s"
        ],
        [
         "130123",
         "cmp-lg-9407013v1",
         "The Acquisition of a Lexicon from Paired Phoneme Sequences and Semantic\n  Representations",
         "Computation and Language (Legacy category)",
         "cmp-lg",
         "1994-07-15 00:00:00",
         "1994-07-15 00:00:00",
         "['Carl de Marcken']",
         "'Carl de Marcken'",
         "We present an algorithm that acquires words (pairings of phonological forms\nand semantic representations) from larger utterances of unsegmented phoneme\nsequences and semantic representations. The algorithm maintains from utterance\nto utterance only a single coherent dictionary, and learns in the presence of\nhomonymy, synonymy, and noise. Test results over a corpus of utterances\ngenerated from the Childes database of mother-child interactions are presented.",
         "63",
         "1",
         "12",
         "0.0",
         "3.4641016151377544",
         "1994",
         "1994Q3",
         "1994-07",
         "1994",
         "1994Q3",
         "1994-07",
         "1990s"
        ]
       ],
       "shape": {
        "columns": 21,
        "rows": 136160
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>category_code</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>first_author</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>title_count</th>\n",
       "      <th>author_count_boxcox</th>\n",
       "      <th>title_count_sqrt</th>\n",
       "      <th>published_year</th>\n",
       "      <th>published_quarter</th>\n",
       "      <th>published_month</th>\n",
       "      <th>updated_year</th>\n",
       "      <th>updated_quarter</th>\n",
       "      <th>updated_month</th>\n",
       "      <th>year_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cs-9308101v1</td>\n",
       "      <td>Dynamic Backtracking</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>1993-08-01</td>\n",
       "      <td>1993-08-01</td>\n",
       "      <td>['M. L. Ginsberg']</td>\n",
       "      <td>'M. L. Ginsberg'</td>\n",
       "      <td>Because of their occasional need to return to ...</td>\n",
       "      <td>79</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993Q3</td>\n",
       "      <td>1993-08</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993Q3</td>\n",
       "      <td>1993-08</td>\n",
       "      <td>1990s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cs-9308102v1</td>\n",
       "      <td>A Market-Oriented Programming Environment and ...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>1993-08-01</td>\n",
       "      <td>1993-08-01</td>\n",
       "      <td>['M. P. Wellman']</td>\n",
       "      <td>'M. P. Wellman'</td>\n",
       "      <td>Market price systems constitute a well-underst...</td>\n",
       "      <td>119</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993Q3</td>\n",
       "      <td>1993-08</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993Q3</td>\n",
       "      <td>1993-08</td>\n",
       "      <td>1990s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cs-9309101v1</td>\n",
       "      <td>An Empirical Analysis of Search in GSAT</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>1993-09-01</td>\n",
       "      <td>1993-09-01</td>\n",
       "      <td>['I. P. Gent', 'T. Walsh']</td>\n",
       "      <td>'I. P. Gent'</td>\n",
       "      <td>We describe an extensive study of search in GS...</td>\n",
       "      <td>167</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.715010</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993Q3</td>\n",
       "      <td>1993-09</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993Q3</td>\n",
       "      <td>1993-09</td>\n",
       "      <td>1990s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cs-9311101v1</td>\n",
       "      <td>The Difficulties of Learning Logic Programs wi...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>1993-11-01</td>\n",
       "      <td>1993-11-01</td>\n",
       "      <td>['F. Bergadano', 'D. Gunetti', 'U. Trinchero']</td>\n",
       "      <td>'F. Bergadano'</td>\n",
       "      <td>As real logic programmers normally use cut (!)...</td>\n",
       "      <td>174</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1.154208</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993Q4</td>\n",
       "      <td>1993-11</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993Q4</td>\n",
       "      <td>1993-11</td>\n",
       "      <td>1990s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cs-9311102v1</td>\n",
       "      <td>Software Agents: Completing Patterns and Const...</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>1993-11-01</td>\n",
       "      <td>1993-11-01</td>\n",
       "      <td>['J. C. Schlimmer', 'L. A. Hermens']</td>\n",
       "      <td>'J. C. Schlimmer'</td>\n",
       "      <td>To support the goal of allowing users to recor...</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.715010</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993Q4</td>\n",
       "      <td>1993-11</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993Q4</td>\n",
       "      <td>1993-11</td>\n",
       "      <td>1990s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112516</th>\n",
       "      <td>abs-2501.18184v1</td>\n",
       "      <td>Genetic Algorithm with Border Trades (GAB)</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>['Qingchuan Lyu']</td>\n",
       "      <td>'Qingchuan Lyu'</td>\n",
       "      <td>This paper introduces a novel approach to impr...</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025Q1</td>\n",
       "      <td>2025-01</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025Q1</td>\n",
       "      <td>2025-01</td>\n",
       "      <td>2020s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112517</th>\n",
       "      <td>abs-2501.18280v1</td>\n",
       "      <td>Jailbreaking LLMs' Safeguard with Universal Ma...</td>\n",
       "      <td>Computation and Language (Natural Language Pro...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>['Haoyu Liang', 'Youran Sun', 'Yunfeng Cai', '...</td>\n",
       "      <td>'Haoyu Liang'</td>\n",
       "      <td>The security issue of large language models (L...</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>1.730617</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025Q1</td>\n",
       "      <td>2025-01</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025Q1</td>\n",
       "      <td>2025-01</td>\n",
       "      <td>2020s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108722</th>\n",
       "      <td>abs-2405.20132v4</td>\n",
       "      <td>LLaMEA: A Large Language Model Evolutionary Al...</td>\n",
       "      <td>Neural and Evolutionary Computing</td>\n",
       "      <td>cs.NE</td>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>['Niki van Stein', 'Thomas Bäck']</td>\n",
       "      <td>'Niki van Stein'</td>\n",
       "      <td>Large Language Models (LLMs) such as GPT-4 hav...</td>\n",
       "      <td>177</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.715010</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024Q2</td>\n",
       "      <td>2024-05</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025Q1</td>\n",
       "      <td>2025-01</td>\n",
       "      <td>2020s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112519</th>\n",
       "      <td>abs-2501.18504v1</td>\n",
       "      <td>CLEAR: Cue Learning using Evolution for Accura...</td>\n",
       "      <td>Computer Vision and Pattern Recognition</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>['Peter J. Bentley', 'Soo Ling Lim', 'Fuyuki I...</td>\n",
       "      <td>'Peter J. Bentley'</td>\n",
       "      <td>Large Language Model (LLM) image recognition i...</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>1.154208</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025Q1</td>\n",
       "      <td>2025-01</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025Q1</td>\n",
       "      <td>2025-01</td>\n",
       "      <td>2020s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136132</th>\n",
       "      <td>abs-2407.13101v2</td>\n",
       "      <td>Retrieve, Summarize, Plan: Advancing Multi-hop...</td>\n",
       "      <td>Computation and Language (Natural Language Pro...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>2024-07-18</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>['Zhouyu Jiang', 'Mengshu Sun', 'Lei Liang', '...</td>\n",
       "      <td>'Zhouyu Jiang'</td>\n",
       "      <td>Multi-hop question answering is a challenging ...</td>\n",
       "      <td>148</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>1.475593</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024Q3</td>\n",
       "      <td>2024-07</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025Q1</td>\n",
       "      <td>2025-01</td>\n",
       "      <td>2020s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136160 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                              title  \\\n",
       "0           cs-9308101v1                               Dynamic Backtracking   \n",
       "1           cs-9308102v1  A Market-Oriented Programming Environment and ...   \n",
       "2           cs-9309101v1            An Empirical Analysis of Search in GSAT   \n",
       "3           cs-9311101v1  The Difficulties of Learning Logic Programs wi...   \n",
       "4           cs-9311102v1  Software Agents: Completing Patterns and Const...   \n",
       "...                  ...                                                ...   \n",
       "112516  abs-2501.18184v1         Genetic Algorithm with Border Trades (GAB)   \n",
       "112517  abs-2501.18280v1  Jailbreaking LLMs' Safeguard with Universal Ma...   \n",
       "108722  abs-2405.20132v4  LLaMEA: A Large Language Model Evolutionary Al...   \n",
       "112519  abs-2501.18504v1  CLEAR: Cue Learning using Evolution for Accura...   \n",
       "136132  abs-2407.13101v2  Retrieve, Summarize, Plan: Advancing Multi-hop...   \n",
       "\n",
       "                                                 category category_code  \\\n",
       "0                                 Artificial Intelligence         cs.AI   \n",
       "1                                 Artificial Intelligence         cs.AI   \n",
       "2                                 Artificial Intelligence         cs.AI   \n",
       "3                                 Artificial Intelligence         cs.AI   \n",
       "4                                 Artificial Intelligence         cs.AI   \n",
       "...                                                   ...           ...   \n",
       "112516                                   Machine Learning         cs.LG   \n",
       "112517  Computation and Language (Natural Language Pro...         cs.CL   \n",
       "108722                  Neural and Evolutionary Computing         cs.NE   \n",
       "112519            Computer Vision and Pattern Recognition         cs.CV   \n",
       "136132  Computation and Language (Natural Language Pro...         cs.CL   \n",
       "\n",
       "       published_date updated_date  \\\n",
       "0          1993-08-01   1993-08-01   \n",
       "1          1993-08-01   1993-08-01   \n",
       "2          1993-09-01   1993-09-01   \n",
       "3          1993-11-01   1993-11-01   \n",
       "4          1993-11-01   1993-11-01   \n",
       "...               ...          ...   \n",
       "112516     2025-01-30   2025-01-30   \n",
       "112517     2025-01-30   2025-01-30   \n",
       "108722     2024-05-30   2025-01-30   \n",
       "112519     2025-01-30   2025-01-30   \n",
       "136132     2024-07-18   2025-01-30   \n",
       "\n",
       "                                                  authors        first_author  \\\n",
       "0                                      ['M. L. Ginsberg']    'M. L. Ginsberg'   \n",
       "1                                       ['M. P. Wellman']     'M. P. Wellman'   \n",
       "2                              ['I. P. Gent', 'T. Walsh']        'I. P. Gent'   \n",
       "3          ['F. Bergadano', 'D. Gunetti', 'U. Trinchero']      'F. Bergadano'   \n",
       "4                    ['J. C. Schlimmer', 'L. A. Hermens']   'J. C. Schlimmer'   \n",
       "...                                                   ...                 ...   \n",
       "112516                                  ['Qingchuan Lyu']     'Qingchuan Lyu'   \n",
       "112517  ['Haoyu Liang', 'Youran Sun', 'Yunfeng Cai', '...       'Haoyu Liang'   \n",
       "108722                  ['Niki van Stein', 'Thomas Bäck']    'Niki van Stein'   \n",
       "112519  ['Peter J. Bentley', 'Soo Ling Lim', 'Fuyuki I...  'Peter J. Bentley'   \n",
       "136132  ['Zhouyu Jiang', 'Mengshu Sun', 'Lei Liang', '...      'Zhouyu Jiang'   \n",
       "\n",
       "                                                  summary  summary_word_count  \\\n",
       "0       Because of their occasional need to return to ...                  79   \n",
       "1       Market price systems constitute a well-underst...                 119   \n",
       "2       We describe an extensive study of search in GS...                 167   \n",
       "3       As real logic programmers normally use cut (!)...                 174   \n",
       "4       To support the goal of allowing users to recor...                 187   \n",
       "...                                                   ...                 ...   \n",
       "112516  This paper introduces a novel approach to impr...                  74   \n",
       "112517  The security issue of large language models (L...                 150   \n",
       "108722  Large Language Models (LLMs) such as GPT-4 hav...                 177   \n",
       "112519  Large Language Model (LLM) image recognition i...                 170   \n",
       "136132  Multi-hop question answering is a challenging ...                 148   \n",
       "\n",
       "        ...  title_count  author_count_boxcox  title_count_sqrt  \\\n",
       "0       ...            2             0.000000          1.414214   \n",
       "1       ...           12             0.000000          3.464102   \n",
       "2       ...            7             0.715010          2.645751   \n",
       "3       ...            8             1.154208          2.828427   \n",
       "4       ...            8             0.715010          2.828427   \n",
       "...     ...          ...                  ...               ...   \n",
       "112516  ...            6             0.000000          2.449490   \n",
       "112517  ...           11             1.730617          3.316625   \n",
       "108722  ...           11             0.715010          3.316625   \n",
       "112519  ...           13             1.154208          3.605551   \n",
       "136132  ...           11             1.475593          3.316625   \n",
       "\n",
       "        published_year  published_quarter published_month updated_year  \\\n",
       "0                 1993             1993Q3         1993-08         1993   \n",
       "1                 1993             1993Q3         1993-08         1993   \n",
       "2                 1993             1993Q3         1993-09         1993   \n",
       "3                 1993             1993Q4         1993-11         1993   \n",
       "4                 1993             1993Q4         1993-11         1993   \n",
       "...                ...                ...             ...          ...   \n",
       "112516            2025             2025Q1         2025-01         2025   \n",
       "112517            2025             2025Q1         2025-01         2025   \n",
       "108722            2024             2024Q2         2024-05         2025   \n",
       "112519            2025             2025Q1         2025-01         2025   \n",
       "136132            2024             2024Q3         2024-07         2025   \n",
       "\n",
       "        updated_quarter updated_month year_period  \n",
       "0                1993Q3       1993-08       1990s  \n",
       "1                1993Q3       1993-08       1990s  \n",
       "2                1993Q3       1993-09       1990s  \n",
       "3                1993Q4       1993-11       1990s  \n",
       "4                1993Q4       1993-11       1990s  \n",
       "...                 ...           ...         ...  \n",
       "112516           2025Q1       2025-01       2020s  \n",
       "112517           2025Q1       2025-01       2020s  \n",
       "108722           2025Q1       2025-01       2020s  \n",
       "112519           2025Q1       2025-01       2020s  \n",
       "136132           2025Q1       2025-01       2020s  \n",
       "\n",
       "[136160 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df= pd.read_parquet('../data/processed/arXiv_scientific_dataset_final.parquet')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "262a9cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category_code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "published_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "updated_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "first_author",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary_word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "author_count_boxcox",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "title_count_sqrt",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "625f8990-f183-4452-ba3f-6b68a45cb20f",
       "rows": [
        [
         "0",
         "cs-9308101v1",
         "Dynamic Backtracking",
         "cs.AI",
         "1993-08-01 00:00:00",
         "1993-08-01 00:00:00",
         "['M. L. Ginsberg']",
         "'M. L. Ginsberg'",
         "Because of their occasional need to return to shallow points in a search\ntree, existing backtracking methods can sometimes erase meaningful progress\ntoward solving a search problem. In this paper, we present a method by which\nbacktrack points can be moved deeper in the search space, thereby avoiding this\ndifficulty. The technique developed is a variant of dependency-directed\nbacktracking that uses only polynomial space while still providing useful\ncontrol information and retaining the completeness guarantees provided by\nearlier approaches.",
         "79",
         "0.0",
         "1.4142135623730951"
        ],
        [
         "1",
         "cs-9308102v1",
         "A Market-Oriented Programming Environment and its Application to\n  Distributed Multicommodity Flow Problems",
         "cs.AI",
         "1993-08-01 00:00:00",
         "1993-08-01 00:00:00",
         "['M. P. Wellman']",
         "'M. P. Wellman'",
         "Market price systems constitute a well-understood class of mechanisms that\nunder certain conditions provide effective decentralization of decision making\nwith minimal communication overhead. In a market-oriented programming approach\nto distributed problem solving, we derive the activities and resource\nallocations for a set of computational agents by computing the competitive\nequilibrium of an artificial economy. WALRAS provides basic constructs for\ndefining computational market structures, and protocols for deriving their\ncorresponding price equilibria. In a particular realization of this approach\nfor a form of multicommodity flow problem, we see that careful construction of\nthe decision process according to economic principles can lead to efficient\ndistributed resource allocation, and that the behavior of the system can be\nmeaningfully analyzed in economic terms.",
         "119",
         "0.0",
         "3.4641016151377544"
        ],
        [
         "2",
         "cs-9309101v1",
         "An Empirical Analysis of Search in GSAT",
         "cs.AI",
         "1993-09-01 00:00:00",
         "1993-09-01 00:00:00",
         "['I. P. Gent', 'T. Walsh']",
         "'I. P. Gent'",
         "We describe an extensive study of search in GSAT, an approximation procedure\nfor propositional satisfiability. GSAT performs greedy hill-climbing on the\nnumber of satisfied clauses in a truth assignment. Our experiments provide a\nmore complete picture of GSAT's search than previous accounts. We describe in\ndetail the two phases of search: rapid hill-climbing followed by a long plateau\nsearch. We demonstrate that when applied to randomly generated 3SAT problems,\nthere is a very simple scaling with problem size for both the mean number of\nsatisfied clauses and the mean branching rate. Our results allow us to make\ndetailed numerical conjectures about the length of the hill-climbing phase, the\naverage gradient of this phase, and to conjecture that both the average score\nand average branching rate decay exponentially during plateau search. We end by\nshowing how these results can be used to direct future theoretical analysis.\nThis work provides a case study of how computer experiments can be used to\nimprove understanding of the theoretical properties of algorithms.",
         "167",
         "0.715009822764921",
         "2.6457513110645907"
        ],
        [
         "3",
         "cs-9311101v1",
         "The Difficulties of Learning Logic Programs with Cut",
         "cs.AI",
         "1993-11-01 00:00:00",
         "1993-11-01 00:00:00",
         "['F. Bergadano', 'D. Gunetti', 'U. Trinchero']",
         "'F. Bergadano'",
         "As real logic programmers normally use cut (!), an effective learning\nprocedure for logic programs should be able to deal with it. Because the cut\npredicate has only a procedural meaning, clauses containing cut cannot be\nlearned using an extensional evaluation method, as is done in most learning\nsystems. On the other hand, searching a space of possible programs (instead of\na space of independent clauses) is unfeasible. An alternative solution is to\ngenerate first a candidate base program which covers the positive examples, and\nthen make it consistent by inserting cut where appropriate. The problem of\nlearning programs with cut has not been investigated before and this seems to\nbe a natural and reasonable approach. We generalize this scheme and investigate\nthe difficulties that arise. Some of the major shortcomings are actually\ncaused, in general, by the need for intensional evaluation. As a conclusion,\nthe analysis of this paper suggests, on precise and technical grounds, that\nlearning cut is difficult, and current induction techniques should probably be\nrestricted to purely declarative logic languages.",
         "174",
         "1.1542082347683233",
         "2.8284271247461903"
        ],
        [
         "4",
         "cs-9311102v1",
         "Software Agents: Completing Patterns and Constructing User Interfaces",
         "cs.AI",
         "1993-11-01 00:00:00",
         "1993-11-01 00:00:00",
         "['J. C. Schlimmer', 'L. A. Hermens']",
         "'J. C. Schlimmer'",
         "To support the goal of allowing users to record and retrieve information,\nthis paper describes an interactive note-taking system for pen-based computers\nwith two distinctive features. First, it actively predicts what the user is\ngoing to write. Second, it automatically constructs a custom, button-box user\ninterface on request. The system is an example of a learning-apprentice\nsoftware- agent. A machine learning component characterizes the syntax and\nsemantics of the user's information. A performance system uses this learned\ninformation to generate completion strings and construct a user interface.\nDescription of Online Appendix: People like to record information. Doing this\non paper is initially efficient, but lacks flexibility. Recording information\non a computer is less efficient but more powerful. In our new note taking\nsoftwre, the user records information directly on a computer. Behind the\ninterface, an agent acts for the user. To help, it provides defaults and\nconstructs a custom user interface. The demonstration is a QuickTime movie of\nthe note taking agent in action. The file is a binhexed self-extracting\narchive. Macintosh utilities for binhex are available from\nmac.archive.umich.edu. QuickTime is available from ftp.apple.com in the\ndts/mac/sys.soft/quicktime.",
         "187",
         "0.715009822764921",
         "2.8284271247461903"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>category_code</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>first_author</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_word_count</th>\n",
       "      <th>author_count_boxcox</th>\n",
       "      <th>title_count_sqrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cs-9308101v1</td>\n",
       "      <td>Dynamic Backtracking</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>1993-08-01</td>\n",
       "      <td>1993-08-01</td>\n",
       "      <td>['M. L. Ginsberg']</td>\n",
       "      <td>'M. L. Ginsberg'</td>\n",
       "      <td>Because of their occasional need to return to ...</td>\n",
       "      <td>79</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cs-9308102v1</td>\n",
       "      <td>A Market-Oriented Programming Environment and ...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>1993-08-01</td>\n",
       "      <td>1993-08-01</td>\n",
       "      <td>['M. P. Wellman']</td>\n",
       "      <td>'M. P. Wellman'</td>\n",
       "      <td>Market price systems constitute a well-underst...</td>\n",
       "      <td>119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.464102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cs-9309101v1</td>\n",
       "      <td>An Empirical Analysis of Search in GSAT</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>1993-09-01</td>\n",
       "      <td>1993-09-01</td>\n",
       "      <td>['I. P. Gent', 'T. Walsh']</td>\n",
       "      <td>'I. P. Gent'</td>\n",
       "      <td>We describe an extensive study of search in GS...</td>\n",
       "      <td>167</td>\n",
       "      <td>0.715010</td>\n",
       "      <td>2.645751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cs-9311101v1</td>\n",
       "      <td>The Difficulties of Learning Logic Programs wi...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>1993-11-01</td>\n",
       "      <td>1993-11-01</td>\n",
       "      <td>['F. Bergadano', 'D. Gunetti', 'U. Trinchero']</td>\n",
       "      <td>'F. Bergadano'</td>\n",
       "      <td>As real logic programmers normally use cut (!)...</td>\n",
       "      <td>174</td>\n",
       "      <td>1.154208</td>\n",
       "      <td>2.828427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cs-9311102v1</td>\n",
       "      <td>Software Agents: Completing Patterns and Const...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>1993-11-01</td>\n",
       "      <td>1993-11-01</td>\n",
       "      <td>['J. C. Schlimmer', 'L. A. Hermens']</td>\n",
       "      <td>'J. C. Schlimmer'</td>\n",
       "      <td>To support the goal of allowing users to recor...</td>\n",
       "      <td>187</td>\n",
       "      <td>0.715010</td>\n",
       "      <td>2.828427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title  \\\n",
       "0  cs-9308101v1                               Dynamic Backtracking   \n",
       "1  cs-9308102v1  A Market-Oriented Programming Environment and ...   \n",
       "2  cs-9309101v1            An Empirical Analysis of Search in GSAT   \n",
       "3  cs-9311101v1  The Difficulties of Learning Logic Programs wi...   \n",
       "4  cs-9311102v1  Software Agents: Completing Patterns and Const...   \n",
       "\n",
       "  category_code published_date updated_date  \\\n",
       "0         cs.AI     1993-08-01   1993-08-01   \n",
       "1         cs.AI     1993-08-01   1993-08-01   \n",
       "2         cs.AI     1993-09-01   1993-09-01   \n",
       "3         cs.AI     1993-11-01   1993-11-01   \n",
       "4         cs.AI     1993-11-01   1993-11-01   \n",
       "\n",
       "                                          authors       first_author  \\\n",
       "0                              ['M. L. Ginsberg']   'M. L. Ginsberg'   \n",
       "1                               ['M. P. Wellman']    'M. P. Wellman'   \n",
       "2                      ['I. P. Gent', 'T. Walsh']       'I. P. Gent'   \n",
       "3  ['F. Bergadano', 'D. Gunetti', 'U. Trinchero']     'F. Bergadano'   \n",
       "4            ['J. C. Schlimmer', 'L. A. Hermens']  'J. C. Schlimmer'   \n",
       "\n",
       "                                             summary  summary_word_count  \\\n",
       "0  Because of their occasional need to return to ...                  79   \n",
       "1  Market price systems constitute a well-underst...                 119   \n",
       "2  We describe an extensive study of search in GS...                 167   \n",
       "3  As real logic programmers normally use cut (!)...                 174   \n",
       "4  To support the goal of allowing users to recor...                 187   \n",
       "\n",
       "   author_count_boxcox  title_count_sqrt  \n",
       "0             0.000000          1.414214  \n",
       "1             0.000000          3.464102  \n",
       "2             0.715010          2.645751  \n",
       "3             1.154208          2.828427  \n",
       "4             0.715010          2.828427  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we only need a subset of columns for training\n",
    "# specifically removing category- we will use category code as the target, but category is highly correlated with category_code\n",
    "\n",
    "cols_to_keep=['id', 'title', 'category_code', 'published_date', 'updated_date', 'authors', 'first_author', 'summary', 'summary_word_count', 'author_count_boxcox', 'title_count_sqrt']\n",
    "df_model=df[cols_to_keep]\n",
    "display(df_model.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc32604",
   "metadata": {},
   "source": [
    "# Need to remove some rows\n",
    "\n",
    "When I tried to do the test train split, I got an error because some category codes are seen too few times. I need to go through and remove these before I can do the split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa36d04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 with category counts less than 5\n",
      "Categories with very few samples:\n",
      "category_code\n",
      "nucl-th              4\n",
      "physics.acc-ph       4\n",
      "cs.SC                4\n",
      "cond-mat.str-el      4\n",
      "math.AG              4\n",
      "cs.GL                4\n",
      "astro-ph.EP          3\n",
      "astro-ph.GA          3\n",
      "physics.ed-ph        3\n",
      "cs.OS                3\n",
      "math.GT              3\n",
      "astro-ph.HE          3\n",
      "econ.GN              3\n",
      "astro-ph             3\n",
      "physics.space-ph     3\n",
      "math.CT              3\n",
      "math.MG              3\n",
      "physics.gen-ph       3\n",
      "q-bio.OT             2\n",
      "q-bio.CB             2\n",
      "math-ph              2\n",
      "math.GR              2\n",
      "math.RT              2\n",
      "q-fin.EC             2\n",
      "math.SP              2\n",
      "nlin.PS              1\n",
      "physics.hist-ph      1\n",
      "math.GM              1\n",
      "math.NT              1\n",
      "nucl-ex              1\n",
      "cond-mat.supr-con    1\n",
      "q-bio.TO             1\n",
      "physics.class-ph     1\n",
      "math.CV              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count samples per category\n",
    "category_counts = df_model['category_code'].value_counts()\n",
    "print(f\"{len(category_counts[category_counts < 5])} with category counts less than 5\")\n",
    "print(\"Categories with very few samples:\")\n",
    "print(category_counts[category_counts < 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0242befa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape before filtering: (136160, 11)\n",
      "Dataframe shape after filtering: (136077, 11)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataframe shape before filtering: {df_model.shape}\")\n",
    "rare_categories = category_counts[category_counts < 5].index\n",
    "df_model_filtered = df_model[~df_model['category_code'].isin(rare_categories)]\n",
    "print(f\"Dataframe shape after filtering: {df_model_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a245694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 with category counts less than 5\n",
      "Categories with very few samples:\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "category_counts = df_model_filtered['category_code'].value_counts()\n",
    "print(f\"{len(category_counts[category_counts < 5])} with category counts less than 5\")\n",
    "print(\"Categories with very few samples:\")\n",
    "print(category_counts[category_counts < 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a50657c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing dataframes (80/20 split)\n",
    "train_df, test_df = train_test_split(\n",
    "    df_model_filtered, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df_model_filtered['category_code']  # Ensure balanced distribution of categories\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae6a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standard variables\n",
    "y_train = train_df['category_code']\n",
    "y_test = test_df['category_code']\n",
    "X_train = train_df.drop(columns=['category_code'], axis=1)\n",
    "X_test = test_df.drop(columns=['category_code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c69ef87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (108861, 10)\n",
      "y_train shape: (108861,)\n",
      "X_test shape: (27216, 10)\n",
      "y_test shape: (27216,)\n"
     ]
    }
   ],
   "source": [
    "#Check shapes\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bebb92",
   "metadata": {},
   "source": [
    "# Start training and tuning hyperparameters with sciBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f409cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and summary for richer context\n",
    "train_df['text'] = train_df['title'] + \" \" + train_df['summary'].fillna(\"\")\n",
    "test_df['text'] = test_df['title'] + \" \" + test_df['summary'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e04b4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sciBert tokenizer from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc44233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3043efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define metrics for evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce84d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Datasets format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da73e100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f64dcbd1ddb45d7926dba5061a2b84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/108861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb038d20d46d4c2f98797fe6dbfe53d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply tokenization\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23afaed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique categories and create a label mapping\n",
    "unique_categories = train_df['category_code'].unique()\n",
    "label_to_id = {label: i for i, label in enumerate(unique_categories)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a1014dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f4485b506040e2ae5b339787cfd58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/108861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b93e43712c47fabf765a7531da8d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map category names to IDs\n",
    "tokenized_train = tokenized_train.map(lambda x: {'label': label_to_id[x['category_code']]})\n",
    "tokenized_test = tokenized_test.map(lambda x: {'label': label_to_id[x['category_code']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66243c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments (these hyperparameters can be tuned)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64dd805b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea707e2cc28458692ddef446bdce804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90b01e87eda46d28f137c876188cccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "num_labels = len(unique_categories)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\", \n",
    "    num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68530dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c4c7c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40824' max='40824' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40824/40824 10:24:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.841800</td>\n",
       "      <td>0.830313</td>\n",
       "      <td>0.748457</td>\n",
       "      <td>0.725991</td>\n",
       "      <td>0.720663</td>\n",
       "      <td>0.748457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.680700</td>\n",
       "      <td>0.831189</td>\n",
       "      <td>0.754666</td>\n",
       "      <td>0.732556</td>\n",
       "      <td>0.729094</td>\n",
       "      <td>0.754666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.565600</td>\n",
       "      <td>0.903743</td>\n",
       "      <td>0.754630</td>\n",
       "      <td>0.746296</td>\n",
       "      <td>0.741130</td>\n",
       "      <td>0.754630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40824, training_loss=0.7526785128815792, metrics={'train_runtime': 37465.4341, 'train_samples_per_second': 8.717, 'train_steps_per_second': 1.09, 'total_flos': 8.600706324317491e+16, 'train_loss': 0.7526785128815792, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ea8ec9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1701' max='1701' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1701/1701 13:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.9037431478500366, 'eval_accuracy': 0.7546296296296297, 'eval_f1': 0.7462958656061841, 'eval_precision': 0.7411295148116699, 'eval_recall': 0.7546296296296297, 'eval_runtime': 836.2316, 'eval_samples_per_second': 32.546, 'eval_steps_per_second': 2.034, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6fcb971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully saved to ../models/first_train_saved_scibert_model\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer to a directory\n",
    "output_dir = \"../models/first_train_saved_scibert_model\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Optionally, save the training arguments\n",
    "with open(f\"{output_dir}/training_args.json\", 'w') as f:\n",
    "    json.dump(trainer.args.to_dict(), f)\n",
    "\n",
    "print(f\"Model successfully saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36e3633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nc/8xd1vn2n62dgdsxxm4vzqjtm0000gn/T/ipykernel_65856/2801410635.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  hp_trainer = Trainer(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-04-06 16:57:16,989] A new study created in memory with name: no-name-a07ac6e0-49e8-415f-905a-098cbd0b2d8d\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54432' max='54432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54432/54432 13:34:11, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.836300</td>\n",
       "      <td>0.794130</td>\n",
       "      <td>0.754078</td>\n",
       "      <td>0.736869</td>\n",
       "      <td>0.734700</td>\n",
       "      <td>0.754078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.621200</td>\n",
       "      <td>0.806271</td>\n",
       "      <td>0.757569</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.734911</td>\n",
       "      <td>0.757569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.539100</td>\n",
       "      <td>0.865173</td>\n",
       "      <td>0.756687</td>\n",
       "      <td>0.749345</td>\n",
       "      <td>0.746015</td>\n",
       "      <td>0.756687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.453700</td>\n",
       "      <td>1.115623</td>\n",
       "      <td>0.751984</td>\n",
       "      <td>0.745286</td>\n",
       "      <td>0.741010</td>\n",
       "      <td>0.751984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.7941299676895142, 'eval_accuracy': 0.7540784832451499, 'eval_f1': 0.736869050750117, 'eval_precision': 0.7347002448356736, 'eval_recall': 0.7540784832451499, 'eval_runtime': 789.3692, 'eval_samples_per_second': 34.478, 'eval_steps_per_second': 2.155, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.8062713742256165, 'eval_accuracy': 0.7575690770135215, 'eval_f1': 0.7345128492493839, 'eval_precision': 0.7349105125058728, 'eval_recall': 0.7575690770135215, 'eval_runtime': 811.768, 'eval_samples_per_second': 33.527, 'eval_steps_per_second': 2.095, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.8651725053787231, 'eval_accuracy': 0.7566872427983539, 'eval_f1': 0.7493454107045417, 'eval_precision': 0.7460149432419484, 'eval_recall': 0.7566872427983539, 'eval_runtime': 814.3765, 'eval_samples_per_second': 33.419, 'eval_steps_per_second': 2.089, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 1.1156232357025146, 'eval_accuracy': 0.751984126984127, 'eval_f1': 0.7452861248046245, 'eval_precision': 0.7410101849091518, 'eval_recall': 0.751984126984127, 'eval_runtime': 784.2863, 'eval_samples_per_second': 34.702, 'eval_steps_per_second': 2.169, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-04-07 06:31:29,955] Trial 0 finished with value: 2.99026456368203 and parameters: {'learning_rate': 2.9436025690262455e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 4, 'weight_decay': 0.0115531419876304}. Best is trial 0 with value: 2.99026456368203.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27216' max='27216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27216/27216 6:51:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.736900</td>\n",
       "      <td>0.778948</td>\n",
       "      <td>0.761574</td>\n",
       "      <td>0.745257</td>\n",
       "      <td>0.736715</td>\n",
       "      <td>0.761574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.538900</td>\n",
       "      <td>0.757642</td>\n",
       "      <td>0.771899</td>\n",
       "      <td>0.758612</td>\n",
       "      <td>0.753063</td>\n",
       "      <td>0.771899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.7789477705955505, 'eval_accuracy': 0.7615740740740741, 'eval_f1': 0.7452567060670364, 'eval_precision': 0.7367148960585395, 'eval_recall': 0.7615740740740741, 'eval_runtime': 1043.012, 'eval_samples_per_second': 26.094, 'eval_steps_per_second': 1.631, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.7576424479484558, 'eval_accuracy': 0.7718988830099941, 'eval_f1': 0.7586121839584354, 'eval_precision': 0.7530633566918324, 'eval_recall': 0.7718988830099941, 'eval_runtime': 807.7761, 'eval_samples_per_second': 33.693, 'eval_steps_per_second': 2.106, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-04-07 13:22:42,486] Trial 1 finished with value: 3.0554733066702564 and parameters: {'learning_rate': 1.542075482849585e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 2, 'weight_decay': 0.021889860653894203}. Best is trial 1 with value: 3.0554733066702564.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54432' max='54432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54432/54432 13:53:09, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.824600</td>\n",
       "      <td>0.802475</td>\n",
       "      <td>0.751066</td>\n",
       "      <td>0.731251</td>\n",
       "      <td>0.727315</td>\n",
       "      <td>0.751066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.649600</td>\n",
       "      <td>0.811515</td>\n",
       "      <td>0.757716</td>\n",
       "      <td>0.734622</td>\n",
       "      <td>0.733394</td>\n",
       "      <td>0.757716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.551700</td>\n",
       "      <td>0.871810</td>\n",
       "      <td>0.756504</td>\n",
       "      <td>0.748673</td>\n",
       "      <td>0.746880</td>\n",
       "      <td>0.756504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>1.106066</td>\n",
       "      <td>0.750992</td>\n",
       "      <td>0.744227</td>\n",
       "      <td>0.740424</td>\n",
       "      <td>0.750992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.8024747371673584, 'eval_accuracy': 0.7510655496766608, 'eval_f1': 0.7312506535537816, 'eval_precision': 0.7273153159677715, 'eval_recall': 0.7510655496766608, 'eval_runtime': 1098.3513, 'eval_samples_per_second': 24.779, 'eval_steps_per_second': 1.549, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.8115150928497314, 'eval_accuracy': 0.7577160493827161, 'eval_f1': 0.7346222724184841, 'eval_precision': 0.7333942342937588, 'eval_recall': 0.7577160493827161, 'eval_runtime': 810.1876, 'eval_samples_per_second': 33.592, 'eval_steps_per_second': 2.1, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.8718098998069763, 'eval_accuracy': 0.7565035273368607, 'eval_f1': 0.7486726387370474, 'eval_precision': 0.7468795007115948, 'eval_recall': 0.7565035273368607, 'eval_runtime': 790.4627, 'eval_samples_per_second': 34.43, 'eval_steps_per_second': 2.152, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 1.1060656309127808, 'eval_accuracy': 0.7509920634920635, 'eval_f1': 0.7442272671055937, 'eval_precision': 0.7404237301907711, 'eval_recall': 0.7509920634920635, 'eval_runtime': 790.5939, 'eval_samples_per_second': 34.425, 'eval_steps_per_second': 2.152, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-04-08 03:15:53,393] Trial 2 finished with value: 2.9866351242804923 and parameters: {'learning_rate': 2.9431471855670324e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 4, 'weight_decay': 0.04554799408161034}. Best is trial 1 with value: 3.0554733066702564.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108864' max='108864' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108864/108864 16:52:06, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.123800</td>\n",
       "      <td>0.971706</td>\n",
       "      <td>0.729314</td>\n",
       "      <td>0.700931</td>\n",
       "      <td>0.699719</td>\n",
       "      <td>0.729314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.191500</td>\n",
       "      <td>1.077697</td>\n",
       "      <td>0.738463</td>\n",
       "      <td>0.717561</td>\n",
       "      <td>0.710984</td>\n",
       "      <td>0.738463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.700400</td>\n",
       "      <td>1.208030</td>\n",
       "      <td>0.740447</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.731122</td>\n",
       "      <td>0.740447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.628200</td>\n",
       "      <td>1.457963</td>\n",
       "      <td>0.737875</td>\n",
       "      <td>0.729940</td>\n",
       "      <td>0.724783</td>\n",
       "      <td>0.737875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.9717056751251221, 'eval_accuracy': 0.7293136390358612, 'eval_f1': 0.700931251394988, 'eval_precision': 0.6997186580715208, 'eval_recall': 0.7293136390358612, 'eval_runtime': 790.0155, 'eval_samples_per_second': 34.45, 'eval_steps_per_second': 2.153, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 1.0776968002319336, 'eval_accuracy': 0.7384626690182245, 'eval_f1': 0.7175609417712006, 'eval_precision': 0.7109835813211023, 'eval_recall': 0.7384626690182245, 'eval_runtime': 806.427, 'eval_samples_per_second': 33.749, 'eval_steps_per_second': 2.109, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 1.2080297470092773, 'eval_accuracy': 0.7404467960023515, 'eval_f1': 0.7333331690069669, 'eval_precision': 0.7311219086875841, 'eval_recall': 0.7404467960023515, 'eval_runtime': 843.0027, 'eval_samples_per_second': 32.285, 'eval_steps_per_second': 2.018, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 1.4579625129699707, 'eval_accuracy': 0.7378747795414462, 'eval_f1': 0.7299399077266049, 'eval_precision': 0.7247828005185604, 'eval_recall': 0.7378747795414462, 'eval_runtime': 860.8631, 'eval_samples_per_second': 31.615, 'eval_steps_per_second': 1.976, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-04-08 20:08:02,282] Trial 3 finished with value: 2.9304722673280574 and parameters: {'learning_rate': 4.6304175973525386e-05, 'per_device_train_batch_size': 4, 'num_train_epochs': 4, 'weight_decay': 0.02753754920538231}. Best is trial 1 with value: 3.0554733066702564.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40824' max='40824' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40824/40824 10:08:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.761000</td>\n",
       "      <td>0.845557</td>\n",
       "      <td>0.748383</td>\n",
       "      <td>0.726982</td>\n",
       "      <td>0.722209</td>\n",
       "      <td>0.748383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.698300</td>\n",
       "      <td>0.816681</td>\n",
       "      <td>0.759223</td>\n",
       "      <td>0.737433</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.759223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.462700</td>\n",
       "      <td>0.892972</td>\n",
       "      <td>0.754152</td>\n",
       "      <td>0.745994</td>\n",
       "      <td>0.741016</td>\n",
       "      <td>0.754152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.8455565571784973, 'eval_accuracy': 0.7483833039388595, 'eval_f1': 0.7269817669708576, 'eval_precision': 0.7222088491473485, 'eval_recall': 0.7483833039388595, 'eval_runtime': 789.3687, 'eval_samples_per_second': 34.478, 'eval_steps_per_second': 2.155, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.8166805505752563, 'eval_accuracy': 0.7592225161669606, 'eval_f1': 0.7374326891889951, 'eval_precision': 0.7329613745934893, 'eval_recall': 0.7592225161669606, 'eval_runtime': 789.1812, 'eval_samples_per_second': 34.486, 'eval_steps_per_second': 2.155, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.8929721117019653, 'eval_accuracy': 0.7541519694297472, 'eval_f1': 0.7459944927327967, 'eval_precision': 0.7410161547646051, 'eval_recall': 0.7541519694297472, 'eval_runtime': 791.8495, 'eval_samples_per_second': 34.37, 'eval_steps_per_second': 2.148, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-04-09 06:16:45,166] Trial 4 finished with value: 2.995314586356896 and parameters: {'learning_rate': 4.954751741566973e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 3, 'weight_decay': 0.06483969562767664}. Best is trial 1 with value: 3.0554733066702564.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40824' max='40824' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40824/40824 10:28:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.813900</td>\n",
       "      <td>0.791130</td>\n",
       "      <td>0.757055</td>\n",
       "      <td>0.739398</td>\n",
       "      <td>0.734341</td>\n",
       "      <td>0.757055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.601800</td>\n",
       "      <td>0.775994</td>\n",
       "      <td>0.766865</td>\n",
       "      <td>0.748047</td>\n",
       "      <td>0.745362</td>\n",
       "      <td>0.766865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.548000</td>\n",
       "      <td>0.842794</td>\n",
       "      <td>0.764293</td>\n",
       "      <td>0.756580</td>\n",
       "      <td>0.751561</td>\n",
       "      <td>0.764293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.7911298274993896, 'eval_accuracy': 0.7570546737213404, 'eval_f1': 0.7393980773002204, 'eval_precision': 0.7343410448904039, 'eval_recall': 0.7570546737213404, 'eval_runtime': 794.9418, 'eval_samples_per_second': 34.236, 'eval_steps_per_second': 2.14, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.7759935855865479, 'eval_accuracy': 0.7668650793650794, 'eval_f1': 0.7480474999480715, 'eval_precision': 0.7453624482244374, 'eval_recall': 0.7668650793650794, 'eval_runtime': 847.2125, 'eval_samples_per_second': 32.124, 'eval_steps_per_second': 2.008, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.8427939414978027, 'eval_accuracy': 0.764293062904174, 'eval_f1': 0.7565804274710398, 'eval_precision': 0.7515608893806157, 'eval_recall': 0.764293062904174, 'eval_runtime': 829.5346, 'eval_samples_per_second': 32.809, 'eval_steps_per_second': 2.051, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-04-09 16:45:07,168] Trial 5 finished with value: 3.0367274426600037 and parameters: {'learning_rate': 2.2325150424130563e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 3, 'weight_decay': 0.04643672138786191}. Best is trial 1 with value: 3.0554733066702564.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27216' max='27216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27216/27216 6:49:25, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.759900</td>\n",
       "      <td>0.784489</td>\n",
       "      <td>0.760949</td>\n",
       "      <td>0.744398</td>\n",
       "      <td>0.736477</td>\n",
       "      <td>0.760949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.574200</td>\n",
       "      <td>0.758341</td>\n",
       "      <td>0.771384</td>\n",
       "      <td>0.757401</td>\n",
       "      <td>0.751178</td>\n",
       "      <td>0.771384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.7844890356063843, 'eval_accuracy': 0.7609494415049971, 'eval_f1': 0.7443978013167994, 'eval_precision': 0.7364765347376072, 'eval_recall': 0.7609494415049971, 'eval_runtime': 799.3559, 'eval_samples_per_second': 34.047, 'eval_steps_per_second': 2.128, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.7583407759666443, 'eval_accuracy': 0.7713844797178131, 'eval_f1': 0.7574008134035475, 'eval_precision': 0.7511777522956509, 'eval_recall': 0.7713844797178131, 'eval_runtime': 798.6488, 'eval_samples_per_second': 34.078, 'eval_steps_per_second': 2.13, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-04-09 23:34:34,645] Trial 6 finished with value: 3.0513475251348243 and parameters: {'learning_rate': 1.2280142065595493e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 2, 'weight_decay': 0.02250965129934576}. Best is trial 1 with value: 3.0554733066702564.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13608' max='40824' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13608/40824 3:23:16 < 6:46:36, 1.12 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.811768</td>\n",
       "      <td>0.750882</td>\n",
       "      <td>0.731518</td>\n",
       "      <td>0.728104</td>\n",
       "      <td>0.750882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.8117676973342896, 'eval_accuracy': 0.7508818342151675, 'eval_f1': 0.7315178111941577, 'eval_precision': 0.7281040253776916, 'eval_recall': 0.7508818342151675, 'eval_runtime': 798.2984, 'eval_samples_per_second': 34.093, 'eval_steps_per_second': 2.131, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-04-10 02:57:52,726] Trial 7 pruned. \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27216' max='27216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27216/27216 6:54:56, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.752100</td>\n",
       "      <td>0.789716</td>\n",
       "      <td>0.757716</td>\n",
       "      <td>0.738150</td>\n",
       "      <td>0.736250</td>\n",
       "      <td>0.757716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.561800</td>\n",
       "      <td>0.764351</td>\n",
       "      <td>0.768849</td>\n",
       "      <td>0.756613</td>\n",
       "      <td>0.751465</td>\n",
       "      <td>0.768849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.7897156476974487, 'eval_accuracy': 0.7577160493827161, 'eval_f1': 0.7381497464503939, 'eval_precision': 0.7362496240023345, 'eval_recall': 0.7577160493827161, 'eval_runtime': 797.7648, 'eval_samples_per_second': 34.115, 'eval_steps_per_second': 2.132, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.7643511891365051, 'eval_accuracy': 0.7688492063492064, 'eval_f1': 0.7566126083262934, 'eval_precision': 0.751465096022372, 'eval_recall': 0.7688492063492064, 'eval_runtime': 794.6541, 'eval_samples_per_second': 34.249, 'eval_steps_per_second': 2.141, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-04-10 09:52:51,458] Trial 8 finished with value: 3.0457761170470783 and parameters: {'learning_rate': 3.695497949539519e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 2, 'weight_decay': 0.010931181691060046}. Best is trial 1 with value: 3.0554733066702564.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13608' max='40824' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13608/40824 3:27:46 < 6:55:35, 1.09 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.823500</td>\n",
       "      <td>0.818901</td>\n",
       "      <td>0.746105</td>\n",
       "      <td>0.728277</td>\n",
       "      <td>0.723108</td>\n",
       "      <td>0.746105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.8189014792442322, 'eval_accuracy': 0.7461052322163433, 'eval_f1': 0.7282771559063943, 'eval_precision': 0.7231075498302315, 'eval_recall': 0.7461052322163433, 'eval_runtime': 826.4159, 'eval_samples_per_second': 32.933, 'eval_steps_per_second': 2.058, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2025-04-10 13:20:40,508] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 1.542075482849585e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 2, 'weight_decay': 0.021889860653894203}\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameters\n",
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is not None:\n",
    "            print(f\"Validation metrics: {metrics}\")\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"allenai/scibert_scivocab_uncased\", \n",
    "        num_labels=len(unique_categories)\n",
    "    )\n",
    "\n",
    "# Define hyperparameter search space\n",
    "hyperparameter_space = {\n",
    "    \"learning_rate\": [1e-5, 3e-5, 5e-5],\n",
    "    \"per_device_train_batch_size\": [4, 8],\n",
    "    \"num_train_epochs\": [2, 3, 4],\n",
    "    \"weight_decay\": [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Create trainer for hyperparameter search\n",
    "hp_trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[PrinterCallback(), EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Run hyperparameter search\n",
    "best_run = hp_trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    n_trials=10,\n",
    "    hp_space=lambda trial: {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 4),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.01, 0.1, log=True),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Best hyperparameters: {best_run.hyperparameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a8026",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "This took 3.8 days!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40607897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 1.542075482849585e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 2, 'weight_decay': 0.021889860653894203}\n"
     ]
    }
   ],
   "source": [
    "# If you used hp_trainer.hyperparameter_search()\n",
    "best_hyperparameters = best_run.hyperparameters\n",
    "print(\"Best hyperparameters:\", best_hyperparameters)\n",
    "\n",
    "# Create training arguments with the best hyperparameters\n",
    "best_training_args = TrainingArguments(\n",
    "    output_dir='./results_final',\n",
    "    **best_hyperparameters,\n",
    "    # Add any other arguments not covered in hyperparameter search\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40f6763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/nc/8xd1vn2n62dgdsxxm4vzqjtm0000gn/T/ipykernel_65856/3233613421.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  final_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27216' max='27216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27216/27216 7:03:33, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.811900</td>\n",
       "      <td>0.777088</td>\n",
       "      <td>0.761868</td>\n",
       "      <td>0.745437</td>\n",
       "      <td>0.738134</td>\n",
       "      <td>0.761868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.659600</td>\n",
       "      <td>0.757403</td>\n",
       "      <td>0.770025</td>\n",
       "      <td>0.756649</td>\n",
       "      <td>0.750831</td>\n",
       "      <td>0.770025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=27216, training_loss=0.8034150927014942, metrics={'train_runtime': 25415.1735, 'train_samples_per_second': 8.567, 'train_steps_per_second': 1.071, 'total_flos': 5.733804216211661e+16, 'train_loss': 0.8034150927014942, 'epoch': 2.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model with best hyperparameters\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\", \n",
    "    num_labels=len(unique_categories)\n",
    ")\n",
    "\n",
    "# Create the final trainer\n",
    "final_trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=best_training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the final model\n",
    "final_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9c92ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model successfully saved to ../models/final_scibert_model\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "final_output_dir = \"../models/final_scibert_model\"\n",
    "final_trainer.save_model(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n",
    "\n",
    "# Save the training arguments\n",
    "import json\n",
    "with open(f\"{final_output_dir}/training_args.json\", 'w') as f:\n",
    "    json.dump(final_trainer.args.to_dict(), f)\n",
    "\n",
    "print(f\"Final model successfully saved to {final_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83dfd1f",
   "metadata": {},
   "source": [
    "# Now test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0afcb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(text, model, tokenizer, id_to_label=None):\n",
    "    \"\"\"Predict category for a scientific paper using any BERT model\"\"\"\n",
    "    # Prepare input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    device = model.device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get probabilities and predicted class\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)[0]\n",
    "    predicted_class_id = outputs.logits.argmax(-1).item()\n",
    "    \n",
    "    # If id_to_label mapping is provided, convert ID to category name\n",
    "    if id_to_label:\n",
    "        predicted_category = id_to_label[predicted_class_id]\n",
    "        return predicted_category, probs[predicted_class_id].item()\n",
    "    else:\n",
    "        return predicted_class_id, probs[predicted_class_id].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4256c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_df, tokenizer, id_to_label):\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    confidences = []\n",
    "    \n",
    "    for i in range(len(test_df)):\n",
    "        example = test_df.iloc[i]\n",
    "        text = example['title'] + \" \" + (example['summary'] if not pd.isna(example['summary']) else \"\")\n",
    "        true_category = example['category_code']\n",
    "        \n",
    "        predicted_category, confidence = predict_category(\n",
    "            text, \n",
    "            model, \n",
    "            tokenizer, \n",
    "            id_to_label\n",
    "        )\n",
    "        \n",
    "        predictions.append(predicted_category)\n",
    "        true_labels.append(true_category)\n",
    "        confidences.append(confidence)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(true_labels, predictions, output_dict=True)\n",
    "    \n",
    "    # Average confidence\n",
    "    avg_confidence = sum(confidences) / len(confidences)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': report,\n",
    "        'avg_confidence': avg_confidence,\n",
    "        'predictions': predictions,\n",
    "        'confidences': confidences\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d4c2407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model is original scibert model\n",
    "tuned_model = AutoModelForSequenceClassification.from_pretrained(\"../models/first_train_saved_scibert_model\")\n",
    "hypertuned_model = AutoModelForSequenceClassification.from_pretrained(\"../models/final_scibert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "156e5011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/deannachurch/opt/anaconda3/envs/arxiv-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all three models\n",
    "base_results = evaluate_model(model, test_df, tokenizer, id_to_label)\n",
    "tuned_results = evaluate_model(tuned_model, test_df, tokenizer, id_to_label)\n",
    "hypertuned_results = evaluate_model(hypertuned_model, test_df, tokenizer, id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35a24178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base SciBERT accuracy: 0.7546296296296297\n",
      "Tuned SciBERT accuracy: 0.7546296296296297\n",
      "Hyperparameter-tuned SciBERT accuracy: 0.7700249853027631\n",
      "\n",
      "Base SciBERT average confidence: 0.8765491222927696\n",
      "Tuned SciBERT average confidence: 0.8765491356807369\n",
      "Hyperparameter-tuned SciBERT average confidence: 0.8336762553880288\n",
      "\n",
      "F1 scores for each model:\n",
      "Base model - Macro F1: 0.2274, Weighted F1: 0.7463\n",
      "Tuned model - Macro F1: 0.2274, Weighted F1: 0.7463\n",
      "Hypertuned model - Macro F1: 0.1809, Weighted F1: 0.7566\n"
     ]
    }
   ],
   "source": [
    "# Compare results\n",
    "print(\"Base SciBERT accuracy:\", base_results['accuracy'])\n",
    "print(\"Tuned SciBERT accuracy:\", tuned_results['accuracy'])\n",
    "print(\"Hyperparameter-tuned SciBERT accuracy:\", hypertuned_results['accuracy'])\n",
    "\n",
    "print(\"\\nBase SciBERT average confidence:\", base_results['avg_confidence'])\n",
    "print(\"Tuned SciBERT average confidence:\", tuned_results['avg_confidence'])\n",
    "print(\"Hyperparameter-tuned SciBERT average confidence:\", hypertuned_results['avg_confidence'])\n",
    "\n",
    "# You can also compare detailed metrics like F1 score per class\n",
    "print(\"\\nF1 scores for each model:\")\n",
    "for model_name, results in [(\"Base\", base_results), (\"Tuned\", tuned_results), (\"Hypertuned\", hypertuned_results)]:\n",
    "    macro_f1 = results['classification_report']['macro avg']['f1-score']\n",
    "    weighted_f1 = results['classification_report']['weighted avg']['f1-score']\n",
    "    print(f\"{model_name} model - Macro F1: {macro_f1:.4f}, Weighted F1: {weighted_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28fd7f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category_code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "category_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "base_prediction",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tuned_prediction",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "hypertuned_prediction",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "884e9298-8c47-4f08-980e-7f5990477106",
       "rows": [
        [
         "101869",
         "A working likelihood approach to support vector regression with a\n  data-driven insensitivity parameter",
         "cs.LG",
         "31973",
         "stat.ML",
         "stat.ML",
         "stat.ML"
        ],
        [
         "7237",
         "A Surprisingly Simple Continuous-Action POMDP Solver: Lazy Cross-Entropy\n  Search Over Policy Trees",
         "cs.AI",
         "10354",
         "cs.AI",
         "cs.AI",
         "cs.AI"
        ],
        [
         "108484",
         "Memory-Efficient Reversible Spiking Neural Networks",
         "cs.CV",
         "23240",
         "cs.NE",
         "cs.NE",
         "cs.NE"
        ],
        [
         "124544",
         "Some Languages are More Equal than Others: Probing Deeper into the\n  Linguistic Disparity in the NLP World",
         "cs.CL",
         "20158",
         "cs.CL",
         "cs.CL",
         "cs.CL"
        ],
        [
         "51639",
         "Lie Algebrized Gaussians for Image Representation",
         "cs.CV",
         "23240",
         "cs.CV",
         "cs.CV",
         "cs.CV"
        ],
        [
         "76005",
         "Doubly Stochastic Variational Inference for Deep Gaussian Processes",
         "stat.ML",
         "8345",
         "stat.ML",
         "stat.ML",
         "stat.ML"
        ],
        [
         "93916",
         "Inference with Discriminative Posterior",
         "stat.ML",
         "8345",
         "cs.LG",
         "cs.LG",
         "stat.ML"
        ],
        [
         "81504",
         "Weighted Distributed Differential Privacy ERM: Convex and Non-convex",
         "cs.LG",
         "31973",
         "cs.LG",
         "cs.LG",
         "cs.LG"
        ],
        [
         "116818",
         "Detecting Machine-Translated Text using Back Translation",
         "cs.CL",
         "20158",
         "cs.CL",
         "cs.CL",
         "cs.CL"
        ],
        [
         "18236",
         "Disentangled Contrastive Learning for Social Recommendation",
         "cs.IR",
         "721",
         "cs.IR",
         "cs.IR",
         "cs.IR"
        ],
        [
         "121780",
         "Using Adversarial Attacks to Reveal the Statistical Bias in Machine\n  Reading Comprehension Models",
         "cs.CL",
         "20158",
         "cs.CL",
         "cs.CL",
         "cs.CL"
        ],
        [
         "131748",
         "I Know What You Asked: Graph Path Learning using AMR for Commonsense\n  Reasoning",
         "cs.CL",
         "20158",
         "cs.CL",
         "cs.CL",
         "cs.CL"
        ],
        [
         "15271",
         "On Lottery Tickets and Minimal Task Representations in Deep\n  Reinforcement Learning",
         "cs.LG",
         "31973",
         "cs.LG",
         "cs.LG",
         "cs.LG"
        ],
        [
         "113440",
         "Probabilistic Frame Induction",
         "cs.CL",
         "20158",
         "cs.CL",
         "cs.CL",
         "cs.CL"
        ],
        [
         "83400",
         "Defending Regression Learners Against Poisoning Attacks",
         "cs.LG",
         "31973",
         "cs.LG",
         "cs.LG",
         "cs.LG"
        ],
        [
         "90179",
         "Deep Kernel Learning of Dynamical Models from High-Dimensional Noisy\n  Data",
         "cs.LG",
         "31973",
         "cs.LG",
         "cs.LG",
         "cs.LG"
        ],
        [
         "45307",
         "Asymptotic Singular Value Distribution of Linear Convolutional Layers",
         "cs.LG",
         "31973",
         "stat.ML",
         "stat.ML",
         "stat.ML"
        ],
        [
         "12699",
         "Assessing State-of-the-Art Sentiment Models on State-of-the-Art\n  Sentiment Datasets",
         "cs.CL",
         "20158",
         "cs.CL",
         "cs.CL",
         "cs.CL"
        ],
        [
         "121601",
         "Representation Learning for Weakly Supervised Relation Extraction",
         "cs.CL",
         "20158",
         "cs.CL",
         "cs.CL",
         "cs.CL"
        ],
        [
         "54373",
         "An Analysis of 1-to-First Matching in Iris Recognition",
         "cs.CV",
         "23240",
         "cs.CV",
         "cs.CV",
         "cs.CV"
        ],
        [
         "62274",
         "Tracking The Untrackable: Learning To Track Multiple Cues with Long-Term\n  Dependencies",
         "cs.CV",
         "23240",
         "cs.CV",
         "cs.CV",
         "cs.CV"
        ],
        [
         "14222",
         "New Hoopoe Heuristic Optimization",
         "cs.NE",
         "4405",
         "cs.NE",
         "cs.NE",
         "cs.NE"
        ],
        [
         "66744",
         "Learning to Measure Change: Fully Convolutional Siamese Metric Networks\n  for Scene Change Detection",
         "cs.CV",
         "23240",
         "cs.CV",
         "cs.CV",
         "cs.CV"
        ],
        [
         "132795",
         "On the long-term learning ability of LSTM LMs",
         "cs.CL",
         "20158",
         "cs.CL",
         "cs.CL",
         "cs.CL"
        ],
        [
         "61231",
         "Fast Bayesian Restoration of Poisson Corrupted Images with INLA",
         "cs.CV",
         "23240",
         "stat.ML",
         "stat.ML",
         "cs.CV"
        ],
        [
         "64674",
         "Beyond Joint Demosaicking and Denoising: An Image Processing Pipeline\n  for a Pixel-bin Image Sensor",
         "cs.CV",
         "23240",
         "cs.CV",
         "cs.CV",
         "cs.CV"
        ],
        [
         "100508",
         "One Embedding To Do Them All",
         "cs.LG",
         "31973",
         "cs.LG",
         "cs.LG",
         "cs.LG"
        ],
        [
         "13873",
         "Motion Informed Object Detection of Small Insects in Time-lapse Camera\n  Recordings",
         "cs.CV",
         "23240",
         "cs.CV",
         "cs.CV",
         "cs.CV"
        ],
        [
         "52573",
         "Globally Tuned Cascade Pose Regression via Back Propagation with\n  Application in 2D Face Pose Estimation and Heart Segmentation in 3D CT Images",
         "cs.CV",
         "23240",
         "cs.CV",
         "cs.CV",
         "cs.CV"
        ],
        [
         "24087",
         "IrokoBench: A New Benchmark for African Languages in the Age of Large\n  Language Models",
         "cs.CL",
         "20158",
         "cs.CL",
         "cs.CL",
         "cs.CL"
        ],
        [
         "124535",
         "AraLegal-BERT: A pretrained language model for Arabic Legal text",
         "cs.CL",
         "20158",
         "cs.CL",
         "cs.CL",
         "cs.CL"
        ],
        [
         "5874",
         "Optimizing Graph Transformer Networks with Graph-based Techniques",
         "cs.AI",
         "10354",
         "cs.LG",
         "cs.LG",
         "cs.LG"
        ],
        [
         "3158",
         "Hierarchical and Interpretable Skill Acquisition in Multi-task\n  Reinforcement Learning",
         "cs.AI",
         "10354",
         "cs.LG",
         "cs.LG",
         "cs.LG"
        ],
        [
         "46556",
         "On the Fairness of Generative Adversarial Networks (GANs)",
         "cs.LG",
         "31973",
         "cs.LG",
         "cs.LG",
         "cs.LG"
        ],
        [
         "39320",
         "Node Classification With Integrated Reject Option",
         "cs.LG",
         "31973",
         "cs.LG",
         "cs.LG",
         "cs.LG"
        ],
        [
         "22656",
         "Standardizing Your Training Process for Human Activity Recognition\n  Models: A Comprehensive Review in the Tunable Factors",
         "cs.LG",
         "31973",
         "cs.LG",
         "cs.LG",
         "cs.LG"
        ],
        [
         "116662",
         "Pun Generation with Surprise",
         "cs.CL",
         "20158",
         "cs.CL",
         "cs.CL",
         "cs.CL"
        ],
        [
         "4223",
         "A Pareto Optimal D* Search Algorithm for Multiobjective Path Planning",
         "cs.AI",
         "10354",
         "cs.RO",
         "cs.RO",
         "cs.RO"
        ],
        [
         "69231",
         "Moving SLAM: Fully Unsupervised Deep Learning in Non-Rigid Scenes",
         "cs.CV",
         "23240",
         "cs.CV",
         "cs.CV",
         "cs.CV"
        ],
        [
         "16598",
         "iRoPro: An interactive Robot Programming Framework",
         "cs.RO",
         "722",
         "cs.RO",
         "cs.RO",
         "cs.RO"
        ],
        [
         "97758",
         "Meta-Learning Deep Energy-Based Memory Models",
         "stat.ML",
         "8345",
         "cs.NE",
         "cs.NE",
         "cs.LG"
        ],
        [
         "33851",
         "Graph Scattering beyond Wavelet Shackles",
         "cs.LG",
         "31973",
         "cs.LG",
         "cs.LG",
         "cs.LG"
        ],
        [
         "43440",
         "Generative Adversarial Networks Synthesize Realistic OCT Images of the\n  Retina",
         "cs.CV",
         "23240",
         "cs.CV",
         "cs.CV",
         "cs.CV"
        ],
        [
         "6393",
         "HousE: Knowledge Graph Embedding with Householder Parameterization",
         "cs.AI",
         "10354",
         "cs.CL",
         "cs.CL",
         "cs.CL"
        ],
        [
         "67972",
         "Context Prior for Scene Segmentation",
         "cs.CV",
         "23240",
         "cs.CV",
         "cs.CV",
         "cs.CV"
        ],
        [
         "43278",
         "Privacy Preserving Off-Policy Evaluation",
         "cs.LG",
         "31973",
         "cs.LG",
         "cs.LG",
         "cs.LG"
        ],
        [
         "29729",
         "Scaling transition from momentum stochastic gradient descent to plain\n  stochastic gradient descent",
         "cs.LG",
         "31973",
         "cs.LG",
         "cs.LG",
         "cs.LG"
        ],
        [
         "47912",
         "SolarDK: A high-resolution urban solar panel image classification and\n  localization dataset",
         "cs.CV",
         "23240",
         "cs.CV",
         "cs.CV",
         "cs.CV"
        ],
        [
         "67430",
         "V-PROM: A Benchmark for Visual Reasoning Using Visual Progressive\n  Matrices",
         "cs.CV",
         "23240",
         "cs.CV",
         "cs.CV",
         "cs.CV"
        ],
        [
         "13847",
         "E-Commerce Dispute Resolution Prediction",
         "cs.CL",
         "20158",
         "cs.CL",
         "cs.CL",
         "cs.CL"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 50
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category_code</th>\n",
       "      <th>category_size</th>\n",
       "      <th>base_prediction</th>\n",
       "      <th>tuned_prediction</th>\n",
       "      <th>hypertuned_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101869</th>\n",
       "      <td>A working likelihood approach to support vecto...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>31973</td>\n",
       "      <td>stat.ML</td>\n",
       "      <td>stat.ML</td>\n",
       "      <td>stat.ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7237</th>\n",
       "      <td>A Surprisingly Simple Continuous-Action POMDP ...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>10354</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108484</th>\n",
       "      <td>Memory-Efficient Reversible Spiking Neural Net...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>23240</td>\n",
       "      <td>cs.NE</td>\n",
       "      <td>cs.NE</td>\n",
       "      <td>cs.NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124544</th>\n",
       "      <td>Some Languages are More Equal than Others: Pro...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20158</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51639</th>\n",
       "      <td>Lie Algebrized Gaussians for Image Representation</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>23240</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76005</th>\n",
       "      <td>Doubly Stochastic Variational Inference for De...</td>\n",
       "      <td>stat.ML</td>\n",
       "      <td>8345</td>\n",
       "      <td>stat.ML</td>\n",
       "      <td>stat.ML</td>\n",
       "      <td>stat.ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93916</th>\n",
       "      <td>Inference with Discriminative Posterior</td>\n",
       "      <td>stat.ML</td>\n",
       "      <td>8345</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>stat.ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81504</th>\n",
       "      <td>Weighted Distributed Differential Privacy ERM:...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>31973</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116818</th>\n",
       "      <td>Detecting Machine-Translated Text using Back T...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20158</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18236</th>\n",
       "      <td>Disentangled Contrastive Learning for Social R...</td>\n",
       "      <td>cs.IR</td>\n",
       "      <td>721</td>\n",
       "      <td>cs.IR</td>\n",
       "      <td>cs.IR</td>\n",
       "      <td>cs.IR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121780</th>\n",
       "      <td>Using Adversarial Attacks to Reveal the Statis...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20158</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131748</th>\n",
       "      <td>I Know What You Asked: Graph Path Learning usi...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20158</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15271</th>\n",
       "      <td>On Lottery Tickets and Minimal Task Representa...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>31973</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113440</th>\n",
       "      <td>Probabilistic Frame Induction</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20158</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83400</th>\n",
       "      <td>Defending Regression Learners Against Poisonin...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>31973</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90179</th>\n",
       "      <td>Deep Kernel Learning of Dynamical Models from ...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>31973</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45307</th>\n",
       "      <td>Asymptotic Singular Value Distribution of Line...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>31973</td>\n",
       "      <td>stat.ML</td>\n",
       "      <td>stat.ML</td>\n",
       "      <td>stat.ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12699</th>\n",
       "      <td>Assessing State-of-the-Art Sentiment Models on...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20158</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121601</th>\n",
       "      <td>Representation Learning for Weakly Supervised ...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20158</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54373</th>\n",
       "      <td>An Analysis of 1-to-First Matching in Iris Rec...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>23240</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62274</th>\n",
       "      <td>Tracking The Untrackable: Learning To Track Mu...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>23240</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14222</th>\n",
       "      <td>New Hoopoe Heuristic Optimization</td>\n",
       "      <td>cs.NE</td>\n",
       "      <td>4405</td>\n",
       "      <td>cs.NE</td>\n",
       "      <td>cs.NE</td>\n",
       "      <td>cs.NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66744</th>\n",
       "      <td>Learning to Measure Change: Fully Convolutiona...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>23240</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132795</th>\n",
       "      <td>On the long-term learning ability of LSTM LMs</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20158</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61231</th>\n",
       "      <td>Fast Bayesian Restoration of Poisson Corrupted...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>23240</td>\n",
       "      <td>stat.ML</td>\n",
       "      <td>stat.ML</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64674</th>\n",
       "      <td>Beyond Joint Demosaicking and Denoising: An Im...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>23240</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100508</th>\n",
       "      <td>One Embedding To Do Them All</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>31973</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13873</th>\n",
       "      <td>Motion Informed Object Detection of Small Inse...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>23240</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52573</th>\n",
       "      <td>Globally Tuned Cascade Pose Regression via Bac...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>23240</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24087</th>\n",
       "      <td>IrokoBench: A New Benchmark for African Langua...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20158</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124535</th>\n",
       "      <td>AraLegal-BERT: A pretrained language model for...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20158</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>Optimizing Graph Transformer Networks with Gra...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>10354</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3158</th>\n",
       "      <td>Hierarchical and Interpretable Skill Acquisiti...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>10354</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46556</th>\n",
       "      <td>On the Fairness of Generative Adversarial Netw...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>31973</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39320</th>\n",
       "      <td>Node Classification With Integrated Reject Option</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>31973</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22656</th>\n",
       "      <td>Standardizing Your Training Process for Human ...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>31973</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116662</th>\n",
       "      <td>Pun Generation with Surprise</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20158</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>A Pareto Optimal D* Search Algorithm for Multi...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>10354</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>cs.RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69231</th>\n",
       "      <td>Moving SLAM: Fully Unsupervised Deep Learning ...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>23240</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16598</th>\n",
       "      <td>iRoPro: An interactive Robot Programming Frame...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>722</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>cs.RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97758</th>\n",
       "      <td>Meta-Learning Deep Energy-Based Memory Models</td>\n",
       "      <td>stat.ML</td>\n",
       "      <td>8345</td>\n",
       "      <td>cs.NE</td>\n",
       "      <td>cs.NE</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33851</th>\n",
       "      <td>Graph Scattering beyond Wavelet Shackles</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>31973</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43440</th>\n",
       "      <td>Generative Adversarial Networks Synthesize Rea...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>23240</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6393</th>\n",
       "      <td>HousE: Knowledge Graph Embedding with Househol...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>10354</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67972</th>\n",
       "      <td>Context Prior for Scene Segmentation</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>23240</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43278</th>\n",
       "      <td>Privacy Preserving Off-Policy Evaluation</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>31973</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29729</th>\n",
       "      <td>Scaling transition from momentum stochastic gr...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>31973</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47912</th>\n",
       "      <td>SolarDK: A high-resolution urban solar panel i...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>23240</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67430</th>\n",
       "      <td>V-PROM: A Benchmark for Visual Reasoning Using...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>23240</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13847</th>\n",
       "      <td>E-Commerce Dispute Resolution Prediction</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20158</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title category_code  \\\n",
       "101869  A working likelihood approach to support vecto...         cs.LG   \n",
       "7237    A Surprisingly Simple Continuous-Action POMDP ...         cs.AI   \n",
       "108484  Memory-Efficient Reversible Spiking Neural Net...         cs.CV   \n",
       "124544  Some Languages are More Equal than Others: Pro...         cs.CL   \n",
       "51639   Lie Algebrized Gaussians for Image Representation         cs.CV   \n",
       "76005   Doubly Stochastic Variational Inference for De...       stat.ML   \n",
       "93916             Inference with Discriminative Posterior       stat.ML   \n",
       "81504   Weighted Distributed Differential Privacy ERM:...         cs.LG   \n",
       "116818  Detecting Machine-Translated Text using Back T...         cs.CL   \n",
       "18236   Disentangled Contrastive Learning for Social R...         cs.IR   \n",
       "121780  Using Adversarial Attacks to Reveal the Statis...         cs.CL   \n",
       "131748  I Know What You Asked: Graph Path Learning usi...         cs.CL   \n",
       "15271   On Lottery Tickets and Minimal Task Representa...         cs.LG   \n",
       "113440                      Probabilistic Frame Induction         cs.CL   \n",
       "83400   Defending Regression Learners Against Poisonin...         cs.LG   \n",
       "90179   Deep Kernel Learning of Dynamical Models from ...         cs.LG   \n",
       "45307   Asymptotic Singular Value Distribution of Line...         cs.LG   \n",
       "12699   Assessing State-of-the-Art Sentiment Models on...         cs.CL   \n",
       "121601  Representation Learning for Weakly Supervised ...         cs.CL   \n",
       "54373   An Analysis of 1-to-First Matching in Iris Rec...         cs.CV   \n",
       "62274   Tracking The Untrackable: Learning To Track Mu...         cs.CV   \n",
       "14222                   New Hoopoe Heuristic Optimization         cs.NE   \n",
       "66744   Learning to Measure Change: Fully Convolutiona...         cs.CV   \n",
       "132795      On the long-term learning ability of LSTM LMs         cs.CL   \n",
       "61231   Fast Bayesian Restoration of Poisson Corrupted...         cs.CV   \n",
       "64674   Beyond Joint Demosaicking and Denoising: An Im...         cs.CV   \n",
       "100508                       One Embedding To Do Them All         cs.LG   \n",
       "13873   Motion Informed Object Detection of Small Inse...         cs.CV   \n",
       "52573   Globally Tuned Cascade Pose Regression via Bac...         cs.CV   \n",
       "24087   IrokoBench: A New Benchmark for African Langua...         cs.CL   \n",
       "124535  AraLegal-BERT: A pretrained language model for...         cs.CL   \n",
       "5874    Optimizing Graph Transformer Networks with Gra...         cs.AI   \n",
       "3158    Hierarchical and Interpretable Skill Acquisiti...         cs.AI   \n",
       "46556   On the Fairness of Generative Adversarial Netw...         cs.LG   \n",
       "39320   Node Classification With Integrated Reject Option         cs.LG   \n",
       "22656   Standardizing Your Training Process for Human ...         cs.LG   \n",
       "116662                       Pun Generation with Surprise         cs.CL   \n",
       "4223    A Pareto Optimal D* Search Algorithm for Multi...         cs.AI   \n",
       "69231   Moving SLAM: Fully Unsupervised Deep Learning ...         cs.CV   \n",
       "16598   iRoPro: An interactive Robot Programming Frame...         cs.RO   \n",
       "97758       Meta-Learning Deep Energy-Based Memory Models       stat.ML   \n",
       "33851            Graph Scattering beyond Wavelet Shackles         cs.LG   \n",
       "43440   Generative Adversarial Networks Synthesize Rea...         cs.CV   \n",
       "6393    HousE: Knowledge Graph Embedding with Househol...         cs.AI   \n",
       "67972                Context Prior for Scene Segmentation         cs.CV   \n",
       "43278            Privacy Preserving Off-Policy Evaluation         cs.LG   \n",
       "29729   Scaling transition from momentum stochastic gr...         cs.LG   \n",
       "47912   SolarDK: A high-resolution urban solar panel i...         cs.CV   \n",
       "67430   V-PROM: A Benchmark for Visual Reasoning Using...         cs.CV   \n",
       "13847            E-Commerce Dispute Resolution Prediction         cs.CL   \n",
       "\n",
       "        category_size base_prediction tuned_prediction hypertuned_prediction  \n",
       "101869          31973         stat.ML          stat.ML               stat.ML  \n",
       "7237            10354           cs.AI            cs.AI                 cs.AI  \n",
       "108484          23240           cs.NE            cs.NE                 cs.NE  \n",
       "124544          20158           cs.CL            cs.CL                 cs.CL  \n",
       "51639           23240           cs.CV            cs.CV                 cs.CV  \n",
       "76005            8345         stat.ML          stat.ML               stat.ML  \n",
       "93916            8345           cs.LG            cs.LG               stat.ML  \n",
       "81504           31973           cs.LG            cs.LG                 cs.LG  \n",
       "116818          20158           cs.CL            cs.CL                 cs.CL  \n",
       "18236             721           cs.IR            cs.IR                 cs.IR  \n",
       "121780          20158           cs.CL            cs.CL                 cs.CL  \n",
       "131748          20158           cs.CL            cs.CL                 cs.CL  \n",
       "15271           31973           cs.LG            cs.LG                 cs.LG  \n",
       "113440          20158           cs.CL            cs.CL                 cs.CL  \n",
       "83400           31973           cs.LG            cs.LG                 cs.LG  \n",
       "90179           31973           cs.LG            cs.LG                 cs.LG  \n",
       "45307           31973         stat.ML          stat.ML               stat.ML  \n",
       "12699           20158           cs.CL            cs.CL                 cs.CL  \n",
       "121601          20158           cs.CL            cs.CL                 cs.CL  \n",
       "54373           23240           cs.CV            cs.CV                 cs.CV  \n",
       "62274           23240           cs.CV            cs.CV                 cs.CV  \n",
       "14222            4405           cs.NE            cs.NE                 cs.NE  \n",
       "66744           23240           cs.CV            cs.CV                 cs.CV  \n",
       "132795          20158           cs.CL            cs.CL                 cs.CL  \n",
       "61231           23240         stat.ML          stat.ML                 cs.CV  \n",
       "64674           23240           cs.CV            cs.CV                 cs.CV  \n",
       "100508          31973           cs.LG            cs.LG                 cs.LG  \n",
       "13873           23240           cs.CV            cs.CV                 cs.CV  \n",
       "52573           23240           cs.CV            cs.CV                 cs.CV  \n",
       "24087           20158           cs.CL            cs.CL                 cs.CL  \n",
       "124535          20158           cs.CL            cs.CL                 cs.CL  \n",
       "5874            10354           cs.LG            cs.LG                 cs.LG  \n",
       "3158            10354           cs.LG            cs.LG                 cs.LG  \n",
       "46556           31973           cs.LG            cs.LG                 cs.LG  \n",
       "39320           31973           cs.LG            cs.LG                 cs.LG  \n",
       "22656           31973           cs.LG            cs.LG                 cs.LG  \n",
       "116662          20158           cs.CL            cs.CL                 cs.CL  \n",
       "4223            10354           cs.RO            cs.RO                 cs.RO  \n",
       "69231           23240           cs.CV            cs.CV                 cs.CV  \n",
       "16598             722           cs.RO            cs.RO                 cs.RO  \n",
       "97758            8345           cs.NE            cs.NE                 cs.LG  \n",
       "33851           31973           cs.LG            cs.LG                 cs.LG  \n",
       "43440           23240           cs.CV            cs.CV                 cs.CV  \n",
       "6393            10354           cs.CL            cs.CL                 cs.CL  \n",
       "67972           23240           cs.CV            cs.CV                 cs.CV  \n",
       "43278           31973           cs.LG            cs.LG                 cs.LG  \n",
       "29729           31973           cs.LG            cs.LG                 cs.LG  \n",
       "47912           23240           cs.CV            cs.CV                 cs.CV  \n",
       "67430           23240           cs.CV            cs.CV                 cs.CV  \n",
       "13847           20158           cs.CL            cs.CL                 cs.CL  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy by category size:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nc/8xd1vn2n62dgdsxxm4vzqjtm0000gn/T/ipykernel_65856/2941544422.py:41: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  size_analysis = results_df.groupby('size_bin').agg({\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "size_bin",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "base_correct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tuned_correct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hypertuned_correct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8a0eb178-252f-4807-9ced-247ab2a2f8d5",
       "rows": [
        [
         "Very Small (1-10)",
         "0.04878048780487805",
         "0.04878048780487805",
         "0.0",
         "41"
        ],
        [
         "Small (11-100)",
         "0.18159203980099503",
         "0.18159203980099503",
         "0.10199004975124377",
         "402"
        ],
        [
         "Medium (101-1000)",
         "0.41376104137610414",
         "0.41376104137610414",
         "0.41143654114365413",
         "2151"
        ],
        [
         "Large (1001-10000)",
         "0.5189714644089056",
         "0.5189714644089056",
         "0.5412354970210097",
         "3189"
        ],
        [
         "Very Large (10000+)",
         "0.8360005598842906",
         "0.8360005598842906",
         "0.8540568282554939",
         "21433"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_correct</th>\n",
       "      <th>tuned_correct</th>\n",
       "      <th>hypertuned_correct</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size_bin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Very Small (1-10)</th>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Small (11-100)</th>\n",
       "      <td>0.181592</td>\n",
       "      <td>0.181592</td>\n",
       "      <td>0.101990</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium (101-1000)</th>\n",
       "      <td>0.413761</td>\n",
       "      <td>0.413761</td>\n",
       "      <td>0.411437</td>\n",
       "      <td>2151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Large (1001-10000)</th>\n",
       "      <td>0.518971</td>\n",
       "      <td>0.518971</td>\n",
       "      <td>0.541235</td>\n",
       "      <td>3189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Very Large (10000+)</th>\n",
       "      <td>0.836001</td>\n",
       "      <td>0.836001</td>\n",
       "      <td>0.854057</td>\n",
       "      <td>21433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     base_correct  tuned_correct  hypertuned_correct  count\n",
       "size_bin                                                                   \n",
       "Very Small (1-10)        0.048780       0.048780            0.000000     41\n",
       "Small (11-100)           0.181592       0.181592            0.101990    402\n",
       "Medium (101-1000)        0.413761       0.413761            0.411437   2151\n",
       "Large (1001-10000)       0.518971       0.518971            0.541235   3189\n",
       "Very Large (10000+)      0.836001       0.836001            0.854057  21433"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy by category (sorted by size):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "category_code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "base_correct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tuned_correct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hypertuned_correct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "category_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "test_count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a0ed3d53-3db2-474e-8fd8-b5283a0b7aea",
       "rows": [
        [
         "hep-lat",
         "0.0",
         "0.0",
         "0.0",
         "4",
         "1"
        ],
        [
         "math.AT",
         "0.0",
         "0.0",
         "0.0",
         "5",
         "1"
        ],
        [
         "nlin.CD",
         "0.0",
         "0.0",
         "0.0",
         "5",
         "1"
        ],
        [
         "cond-mat.soft",
         "0.0",
         "0.0",
         "0.0",
         "5",
         "1"
        ],
        [
         "physics.ins-det",
         "0.0",
         "0.0",
         "0.0",
         "5",
         "1"
        ],
        [
         "stat.OT",
         "0.0",
         "0.0",
         "0.0",
         "6",
         "2"
        ],
        [
         "hep-th",
         "0.5",
         "0.5",
         "0.0",
         "6",
         "2"
        ],
        [
         "math.DG",
         "0.0",
         "0.0",
         "0.0",
         "6",
         "1"
        ],
        [
         "math.HO",
         "0.0",
         "0.0",
         "0.0",
         "6",
         "1"
        ],
        [
         "nlin.CG",
         "0.0",
         "0.0",
         "0.0",
         "6",
         "2"
        ],
        [
         "physics.plasm-ph",
         "0.0",
         "0.0",
         "0.0",
         "6",
         "2"
        ],
        [
         "cond-mat.mes-hall",
         "0.0",
         "0.0",
         "0.0",
         "6",
         "1"
        ],
        [
         "q-fin.MF",
         "0.0",
         "0.0",
         "0.0",
         "6",
         "1"
        ],
        [
         "q-fin.PR",
         "0.0",
         "0.0",
         "0.0",
         "6",
         "1"
        ],
        [
         "hep-ex",
         "0.0",
         "0.0",
         "0.0",
         "6",
         "2"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 15
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_correct</th>\n",
       "      <th>tuned_correct</th>\n",
       "      <th>hypertuned_correct</th>\n",
       "      <th>category_size</th>\n",
       "      <th>test_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hep-lat</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>math.AT</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlin.CD</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cond-mat.soft</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physics.ins-det</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stat.OT</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hep-th</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>math.DG</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>math.HO</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlin.CG</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physics.plasm-ph</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cond-mat.mes-hall</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q-fin.MF</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q-fin.PR</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hep-ex</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   base_correct  tuned_correct  hypertuned_correct  \\\n",
       "category_code                                                        \n",
       "hep-lat                     0.0            0.0                 0.0   \n",
       "math.AT                     0.0            0.0                 0.0   \n",
       "nlin.CD                     0.0            0.0                 0.0   \n",
       "cond-mat.soft               0.0            0.0                 0.0   \n",
       "physics.ins-det             0.0            0.0                 0.0   \n",
       "stat.OT                     0.0            0.0                 0.0   \n",
       "hep-th                      0.5            0.5                 0.0   \n",
       "math.DG                     0.0            0.0                 0.0   \n",
       "math.HO                     0.0            0.0                 0.0   \n",
       "nlin.CG                     0.0            0.0                 0.0   \n",
       "physics.plasm-ph            0.0            0.0                 0.0   \n",
       "cond-mat.mes-hall           0.0            0.0                 0.0   \n",
       "q-fin.MF                    0.0            0.0                 0.0   \n",
       "q-fin.PR                    0.0            0.0                 0.0   \n",
       "hep-ex                      0.0            0.0                 0.0   \n",
       "\n",
       "                   category_size  test_count  \n",
       "category_code                                 \n",
       "hep-lat                        4           1  \n",
       "math.AT                        5           1  \n",
       "nlin.CD                        5           1  \n",
       "cond-mat.soft                  5           1  \n",
       "physics.ins-det                5           1  \n",
       "stat.OT                        6           2  \n",
       "hep-th                         6           2  \n",
       "math.DG                        6           1  \n",
       "math.HO                        6           1  \n",
       "nlin.CG                        6           2  \n",
       "physics.plasm-ph               6           2  \n",
       "cond-mat.mes-hall              6           1  \n",
       "q-fin.MF                       6           1  \n",
       "q-fin.PR                       6           1  \n",
       "hep-ex                         6           2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for largest categories:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "category_code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "base_correct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tuned_correct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hypertuned_correct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "category_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "test_count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "9c627acf-056b-45ab-927b-608c48ef1720",
       "rows": [
        [
         "cs.LG",
         "0.7728296222166625",
         "0.7728296222166625",
         "0.8031023267450588",
         "31973",
         "7994"
        ],
        [
         "cs.CV",
         "0.9197934595524957",
         "0.9197934595524957",
         "0.9314974182444062",
         "23240",
         "5810"
        ],
        [
         "cs.CL",
         "0.939484126984127",
         "0.939484126984127",
         "0.9434523809523809",
         "20158",
         "5040"
        ],
        [
         "cs.AI",
         "0.6415604480494399",
         "0.6415604480494399",
         "0.6635766705291618",
         "10354",
         "2589"
        ],
        [
         "stat.ML",
         "0.43938667944417825",
         "0.43938667944417825",
         "0.44801149976042165",
         "8345",
         "2087"
        ],
        [
         "cs.NE",
         "0.6696914700544465",
         "0.6696914700544465",
         "0.7177858439201452",
         "4405",
         "1102"
        ],
        [
         "cs.RO",
         "0.7403314917127072",
         "0.7403314917127072",
         "0.7292817679558011",
         "722",
         "181"
        ],
        [
         "cs.IR",
         "0.46111111111111114",
         "0.46111111111111114",
         "0.5111111111111111",
         "721",
         "180"
        ],
        [
         "stat.ME",
         "0.3163841807909605",
         "0.3163841807909605",
         "0.3672316384180791",
         "705",
         "177"
        ],
        [
         "math.OC",
         "0.37333333333333335",
         "0.37333333333333335",
         "0.3933333333333333",
         "599",
         "150"
        ],
        [
         "cmp-lg",
         "0.6174496644295302",
         "0.6174496644295302",
         "0.7114093959731543",
         "597",
         "149"
        ],
        [
         "cs.CR",
         "0.6090225563909775",
         "0.6090225563909775",
         "0.556390977443609",
         "532",
         "133"
        ],
        [
         "cs.CY",
         "0.2815533980582524",
         "0.2815533980582524",
         "0.2621359223300971",
         "412",
         "103"
        ],
        [
         "cs.SI",
         "0.38095238095238093",
         "0.38095238095238093",
         "0.39285714285714285",
         "333",
         "84"
        ],
        [
         "cs.HC",
         "0.4024390243902439",
         "0.4024390243902439",
         "0.4024390243902439",
         "329",
         "82"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 15
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_correct</th>\n",
       "      <th>tuned_correct</th>\n",
       "      <th>hypertuned_correct</th>\n",
       "      <th>category_size</th>\n",
       "      <th>test_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cs.LG</th>\n",
       "      <td>0.772830</td>\n",
       "      <td>0.772830</td>\n",
       "      <td>0.803102</td>\n",
       "      <td>31973</td>\n",
       "      <td>7994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs.CV</th>\n",
       "      <td>0.919793</td>\n",
       "      <td>0.919793</td>\n",
       "      <td>0.931497</td>\n",
       "      <td>23240</td>\n",
       "      <td>5810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs.CL</th>\n",
       "      <td>0.939484</td>\n",
       "      <td>0.939484</td>\n",
       "      <td>0.943452</td>\n",
       "      <td>20158</td>\n",
       "      <td>5040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs.AI</th>\n",
       "      <td>0.641560</td>\n",
       "      <td>0.641560</td>\n",
       "      <td>0.663577</td>\n",
       "      <td>10354</td>\n",
       "      <td>2589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stat.ML</th>\n",
       "      <td>0.439387</td>\n",
       "      <td>0.439387</td>\n",
       "      <td>0.448011</td>\n",
       "      <td>8345</td>\n",
       "      <td>2087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs.NE</th>\n",
       "      <td>0.669691</td>\n",
       "      <td>0.669691</td>\n",
       "      <td>0.717786</td>\n",
       "      <td>4405</td>\n",
       "      <td>1102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs.RO</th>\n",
       "      <td>0.740331</td>\n",
       "      <td>0.740331</td>\n",
       "      <td>0.729282</td>\n",
       "      <td>722</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs.IR</th>\n",
       "      <td>0.461111</td>\n",
       "      <td>0.461111</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>721</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stat.ME</th>\n",
       "      <td>0.316384</td>\n",
       "      <td>0.316384</td>\n",
       "      <td>0.367232</td>\n",
       "      <td>705</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>math.OC</th>\n",
       "      <td>0.373333</td>\n",
       "      <td>0.373333</td>\n",
       "      <td>0.393333</td>\n",
       "      <td>599</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cmp-lg</th>\n",
       "      <td>0.617450</td>\n",
       "      <td>0.617450</td>\n",
       "      <td>0.711409</td>\n",
       "      <td>597</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs.CR</th>\n",
       "      <td>0.609023</td>\n",
       "      <td>0.609023</td>\n",
       "      <td>0.556391</td>\n",
       "      <td>532</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs.CY</th>\n",
       "      <td>0.281553</td>\n",
       "      <td>0.281553</td>\n",
       "      <td>0.262136</td>\n",
       "      <td>412</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs.SI</th>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>333</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs.HC</th>\n",
       "      <td>0.402439</td>\n",
       "      <td>0.402439</td>\n",
       "      <td>0.402439</td>\n",
       "      <td>329</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               base_correct  tuned_correct  hypertuned_correct  category_size  \\\n",
       "category_code                                                                   \n",
       "cs.LG              0.772830       0.772830            0.803102          31973   \n",
       "cs.CV              0.919793       0.919793            0.931497          23240   \n",
       "cs.CL              0.939484       0.939484            0.943452          20158   \n",
       "cs.AI              0.641560       0.641560            0.663577          10354   \n",
       "stat.ML            0.439387       0.439387            0.448011           8345   \n",
       "cs.NE              0.669691       0.669691            0.717786           4405   \n",
       "cs.RO              0.740331       0.740331            0.729282            722   \n",
       "cs.IR              0.461111       0.461111            0.511111            721   \n",
       "stat.ME            0.316384       0.316384            0.367232            705   \n",
       "math.OC            0.373333       0.373333            0.393333            599   \n",
       "cmp-lg             0.617450       0.617450            0.711409            597   \n",
       "cs.CR              0.609023       0.609023            0.556391            532   \n",
       "cs.CY              0.281553       0.281553            0.262136            412   \n",
       "cs.SI              0.380952       0.380952            0.392857            333   \n",
       "cs.HC              0.402439       0.402439            0.402439            329   \n",
       "\n",
       "               test_count  \n",
       "category_code              \n",
       "cs.LG                7994  \n",
       "cs.CV                5810  \n",
       "cs.CL                5040  \n",
       "cs.AI                2589  \n",
       "stat.ML              2087  \n",
       "cs.NE                1102  \n",
       "cs.RO                 181  \n",
       "cs.IR                 180  \n",
       "stat.ME               177  \n",
       "math.OC               150  \n",
       "cmp-lg                149  \n",
       "cs.CR                 133  \n",
       "cs.CY                 103  \n",
       "cs.SI                  84  \n",
       "cs.HC                  82  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categories with largest performance differences between models:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "category_code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "base_correct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tuned_correct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hypertuned_correct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "category_size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "test_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "max_diff",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "dd4e6d1c-7eec-4253-8337-d26649c4036f",
       "rows": [
        [
         "astro-ph.CO",
         "1.0",
         "1.0",
         "0.0",
         "18",
         "4",
         "1.0"
        ],
        [
         "astro-ph.SR",
         "0.5",
         "0.5",
         "0.0",
         "8",
         "2",
         "0.5"
        ],
        [
         "hep-th",
         "0.5",
         "0.5",
         "0.0",
         "6",
         "2",
         "0.5"
        ],
        [
         "q-bio.GN",
         "0.5",
         "0.5",
         "0.0",
         "33",
         "8",
         "0.5"
        ],
        [
         "physics.optics",
         "0.4",
         "0.4",
         "0.0",
         "21",
         "5",
         "0.4"
        ],
        [
         "cond-mat.mtrl-sci",
         "0.5",
         "0.5",
         "0.1",
         "42",
         "10",
         "0.4"
        ],
        [
         "physics.comp-ph",
         "0.3333333333333333",
         "0.3333333333333333",
         "0.06666666666666667",
         "61",
         "15",
         "0.26666666666666666"
        ],
        [
         "cs.SY",
         "0.2222222222222222",
         "0.2222222222222222",
         "0.0",
         "71",
         "18",
         "0.2222222222222222"
        ],
        [
         "cs.MA",
         "0.3142857142857143",
         "0.3142857142857143",
         "0.11428571428571428",
         "142",
         "35",
         "0.2"
        ],
        [
         "q-bio.PE",
         "0.3",
         "0.3",
         "0.1",
         "38",
         "10",
         "0.19999999999999998"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_correct</th>\n",
       "      <th>tuned_correct</th>\n",
       "      <th>hypertuned_correct</th>\n",
       "      <th>category_size</th>\n",
       "      <th>test_count</th>\n",
       "      <th>max_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>astro-ph.CO</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>astro-ph.SR</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hep-th</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q-bio.GN</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physics.optics</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cond-mat.mtrl-sci</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physics.comp-ph</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>61</td>\n",
       "      <td>15</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs.SY</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71</td>\n",
       "      <td>18</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs.MA</th>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>142</td>\n",
       "      <td>35</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q-bio.PE</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   base_correct  tuned_correct  hypertuned_correct  \\\n",
       "category_code                                                        \n",
       "astro-ph.CO            1.000000       1.000000            0.000000   \n",
       "astro-ph.SR            0.500000       0.500000            0.000000   \n",
       "hep-th                 0.500000       0.500000            0.000000   \n",
       "q-bio.GN               0.500000       0.500000            0.000000   \n",
       "physics.optics         0.400000       0.400000            0.000000   \n",
       "cond-mat.mtrl-sci      0.500000       0.500000            0.100000   \n",
       "physics.comp-ph        0.333333       0.333333            0.066667   \n",
       "cs.SY                  0.222222       0.222222            0.000000   \n",
       "cs.MA                  0.314286       0.314286            0.114286   \n",
       "q-bio.PE               0.300000       0.300000            0.100000   \n",
       "\n",
       "                   category_size  test_count  max_diff  \n",
       "category_code                                           \n",
       "astro-ph.CO                   18           4  1.000000  \n",
       "astro-ph.SR                    8           2  0.500000  \n",
       "hep-th                         6           2  0.500000  \n",
       "q-bio.GN                      33           8  0.500000  \n",
       "physics.optics                21           5  0.400000  \n",
       "cond-mat.mtrl-sci             42          10  0.400000  \n",
       "physics.comp-ph               61          15  0.266667  \n",
       "cs.SY                         71          18  0.222222  \n",
       "cs.MA                        142          35  0.200000  \n",
       "q-bio.PE                      38          10  0.200000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full results saved to CSV files\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the test dataframe to avoid modifying the original\n",
    "results_df = test_df.copy()\n",
    "\n",
    "# Add predictions from each model\n",
    "results_df['base_prediction'] = base_results['predictions']\n",
    "results_df['base_confidence'] = base_results['confidences']\n",
    "\n",
    "results_df['tuned_prediction'] = tuned_results['predictions']\n",
    "results_df['tuned_confidence'] = tuned_results['confidences']\n",
    "\n",
    "results_df['hypertuned_prediction'] = hypertuned_results['predictions']\n",
    "results_df['hypertuned_confidence'] = hypertuned_results['confidences']\n",
    "\n",
    "# Add a column to show if predictions match the true label\n",
    "results_df['base_correct'] = results_df['base_prediction'] == results_df['category_code']\n",
    "results_df['tuned_correct'] = results_df['tuned_prediction'] == results_df['category_code']\n",
    "results_df['hypertuned_correct'] = results_df['hypertuned_prediction'] == results_df['category_code']\n",
    "\n",
    "# Calculate category frequencies in the original training dataset\n",
    "category_counts = train_df['category_code'].value_counts().to_dict()\n",
    "\n",
    "# Create a mapping function to add category size\n",
    "def get_category_size(category):\n",
    "    return category_counts.get(category, 0)\n",
    "\n",
    "# Add category size to the results dataframe\n",
    "results_df['category_size'] = results_df['category_code'].apply(get_category_size)\n",
    "\n",
    "# Print a sample of the results with category size\n",
    "display(results_df[['title', 'category_code', 'category_size', 'base_prediction', 'tuned_prediction', 'hypertuned_prediction']].head(50))\n",
    "\n",
    "# Analyze performance by category size\n",
    "# Let's create bins of category sizes\n",
    "results_df['size_bin'] = pd.cut(\n",
    "    results_df['category_size'], \n",
    "    bins=[0, 10, 100, 1000, 10000, float('inf')],\n",
    "    labels=['Very Small (1-10)', 'Small (11-100)', 'Medium (101-1000)', 'Large (1001-10000)', 'Very Large (10000+)']\n",
    ")\n",
    "\n",
    "# Analyze accuracy by category size\n",
    "size_analysis = results_df.groupby('size_bin').agg({\n",
    "    'base_correct': 'mean',\n",
    "    'tuned_correct': 'mean',\n",
    "    'hypertuned_correct': 'mean',\n",
    "    'category_code': 'count'\n",
    "}).rename(columns={'category_code': 'count'})\n",
    "\n",
    "print(\"\\nAccuracy by category size:\")\n",
    "display(size_analysis)\n",
    "\n",
    "# Detailed analysis by category\n",
    "category_analysis = results_df.groupby('category_code').agg({\n",
    "    'base_correct': 'mean',\n",
    "    'tuned_correct': 'mean',\n",
    "    'hypertuned_correct': 'mean',\n",
    "    'category_size': 'first',  # Grab the size for each category\n",
    "    'category_code': 'count'\n",
    "}).rename(columns={'category_code': 'test_count'})\n",
    "\n",
    "# Sort by category size to see performance on small vs. large categories\n",
    "print(\"\\nAccuracy by category (sorted by size):\")\n",
    "display(category_analysis.sort_values('category_size').head(15))\n",
    "\n",
    "# Also look at the largest categories\n",
    "print(\"\\nAccuracy for largest categories:\")\n",
    "display(category_analysis.sort_values('category_size', ascending=False).head(15))\n",
    "\n",
    "# Find categories where performance varies significantly between models\n",
    "category_analysis['max_diff'] = category_analysis[['base_correct', 'tuned_correct', 'hypertuned_correct']].max(axis=1) - \\\n",
    "                              category_analysis[['base_correct', 'tuned_correct', 'hypertuned_correct']].min(axis=1)\n",
    "\n",
    "print(\"\\nCategories with largest performance differences between models:\")\n",
    "display(category_analysis.sort_values('max_diff', ascending=False).head(10))\n",
    "\n",
    "# Save the results to CSV for further analysis\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)\n",
    "category_analysis.to_csv('category_performance_analysis.csv')\n",
    "print(\"\\nFull results saved to CSV files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
